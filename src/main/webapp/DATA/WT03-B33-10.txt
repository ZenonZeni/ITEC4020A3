
<DOC>
<DOCNO>WT03-B33-10</DOCNO>
<DOCOLDNO>IA059-000323-B026-6</DOCOLDNO>
<DOCHDR>
http://newsnet.com:80/libiss/ec109.html 205.156.212.5 19970114223038 text/html 152940
HTTP/1.0 200 OK
Server: Netscape-Commerce/1.12
Date: Tuesday, 14-Jan-97 22:28:26 GMT
Last-modified: Tuesday, 29-Oct-96 21:05:20 GMT
Content-length: 152753
Content-type: text/html
</DOCHDR>
<HTML>
<HEAD>
<TITLE>/data/webdev/libiss/ec109.html Sample Issue</TITLE>
</HEAD>
<BODY BGCOLOR="FFFFFF">
<FONT SIZE = 3>
<IMG SRC="/pubgifs/ec109.gif"><BR><BR>
<A NAME=HeadList"></A>
Copyright <BR>
OEM MAGAZINE via NewsNet <BR>
February 1, 1996  Vol. 4 No. <BR>
SAMPLE ISSUE HEADLINES<BR><BR><BR>
<BR>
<H3>COLUMNS</H3>
<UL>
<A HREF = "#1"><LI>Alpha Site: Tool time for pc makers</A>&nbsp&nbsp&nbsp<NOBR>(493 words)</NOBR></LI>
</UL>
<BR>
<H3>LETTERS</H3>
<UL>
<A HREF = "#2"><LI>Kudos from Compaq</A>&nbsp&nbsp&nbsp<NOBR>(31 words)</NOBR></LI>
<A HREF = "#3"><LI>The Net is Key</A>&nbsp&nbsp&nbsp<NOBR>(97 words)</NOBR></LI>
<A HREF = "#4"><LI>Accolades from Apple</A>&nbsp&nbsp&nbsp<NOBR>(195 words)</NOBR></LI>
<A HREF = "#5"><LI>World PC inventor</A>&nbsp&nbsp&nbsp<NOBR>(59 words)</NOBR></LI>
<A HREF = "#6"><LI>Nuts about Acorn</A>&nbsp&nbsp&nbsp<NOBR>(65 words)</NOBR></LI>
<A HREF = "#7"><LI>World PC Won't Sell</A>&nbsp&nbsp&nbsp<NOBR>(304 words)</NOBR></LI>
<A HREF = "#8"><LI>Power Prose</A>&nbsp&nbsp&nbsp<NOBR>(167 words)</NOBR></LI>
<A HREF = "#9"><LI>Fighting Bloatware</A>&nbsp&nbsp&nbsp<NOBR>(275 words)</NOBR></LI>
<A HREF = "#10"><LI>Data Blaster</A>&nbsp&nbsp&nbsp<NOBR>(102 words)</NOBR></LI>
<A HREF = "#11"><LI>Krill or Cetacean?</A>&nbsp&nbsp&nbsp<NOBR>(895 words)</NOBR></LI>
<A HREF = "#12"><LI>A Battery No-Go</A>&nbsp&nbsp&nbsp<NOBR>(189 words)</NOBR></LI>
<A HREF = "#13"><LI>Skinny or FUD?</A>&nbsp&nbsp&nbsp<NOBR>(178 words)</NOBR></LI>
<A HREF = "#14"><LI>CORRECTION</A>&nbsp&nbsp&nbsp<NOBR>(45 words)</NOBR></LI>
</UL>
<BR>
<H3>MANAGING TECHNOLOGY</H3>
<UL>
<A HREF = "#15"><A HREF = "#15"><A HREF = "#15"><LI>Servers in training -- Video servers are cutting their teeth&nbsp;on training-on-demand applications, while waiting for the big&nbsp;opportunities in consumer video-on-demand to appear</A>&nbsp&nbsp&nbsp<NOBR>(1301 words)</NOBR></LI>
</UL>
<BR>
<H3>COLUMNS</H3>
<UL>
<A HREF = "#16"><LI>Tech law: Loose Lips Can Sink Standards</A>&nbsp&nbsp&nbsp<NOBR>(496 words)</NOBR></LI>
<A HREF = "#17"><LI>The big picture: Jigsaw-puzzle integration</A>&nbsp&nbsp&nbsp<NOBR>(432 words)</NOBR></LI>
</UL>
<BR>
<H3>FEATURES</H3>
<UL>
<A HREF = "#18"><LI>The PRET-A-PORTER PC -- (Ready-to-Wear Computing)</A>&nbsp&nbsp&nbsp<NOBR>(2603 words)</NOBR></LI>
<A HREF = "#19"><LI>Weight watchers for wearables</A>&nbsp&nbsp&nbsp<NOBR>(361 words)</NOBR></LI>
<A HREF = "#20"><LI>A wearable skids off the runway</A>&nbsp&nbsp&nbsp<NOBR>(260 words)</NOBR></LI>
<A HREF = "#21"><LI>The Interview: Carl Yankowski</A>&nbsp&nbsp&nbsp<NOBR>(2735 words)</NOBR></LI>
<A HREF = "#22"><A HREF = "#22"><A HREF = "#22"><LI>PCs step into 3D -- The journey into the world of three-dimensional&nbsp;computing will be a difficult one for PC makers, according&nbsp;to the publisher of the PC Graphics Report</A>&nbsp&nbsp&nbsp<NOBR>(2791 words)</NOBR></LI>
<A HREF = "#23"><LI>DVD -- Driving the Vidoe Disk</A>&nbsp&nbsp&nbsp<NOBR>(2645 words)</NOBR></LI>
<A HREF = "#24"><LI>Will Hollywood back DVD as a censor?</A>&nbsp&nbsp&nbsp<NOBR>(415 words)</NOBR></LI>
<A HREF = "#25"><LI>Wireless LANs -- Not of this World</A>&nbsp&nbsp&nbsp<NOBR>(2624 words)</NOBR></LI>
<A HREF = "#26"><LI>Infrared aims for the corporate LAN</A>&nbsp&nbsp&nbsp<NOBR>(210 words)</NOBR></LI>
</UL>
<BR>
<H3>OEM ENABLERS</H3>
<UL>
<A HREF = "#27"><LI>Multiplexing modem</A>&nbsp&nbsp&nbsp<NOBR>(67 words)</NOBR></LI>
<A HREF = "#28"><LI>Power designer</A>&nbsp&nbsp&nbsp<NOBR>(66 words)</NOBR></LI>
<A HREF = "#29"><LI>Soft sound synthesis</A>&nbsp&nbsp&nbsp<NOBR>(66 words)</NOBR></LI>
<A HREF = "#30"><LI>Fast Mac graphics</A>&nbsp&nbsp&nbsp<NOBR>(64 words)</NOBR></LI>
<A HREF = "#31"><LI>No smoke, just mirrors</A>&nbsp&nbsp&nbsp<NOBR>(58 words)</NOBR></LI>
<A HREF = "#32"><LI>Sizing up JPEG</A>&nbsp&nbsp&nbsp<NOBR>(70 words)</NOBR></LI>
</UL>
<BR>
<H3>COLUMNS</H3>
<UL>
<A HREF = "#33"><LI>Dream this: The Mainframe's Last Stand</A>&nbsp&nbsp&nbsp<NOBR>(475 words)</NOBR></LI>
</UL>
<BR>
<H3>COMPUTING 2000</H3>
<UL>
<A HREF = "#34"><A HREF = "#34"><A HREF = "#34"><LI>Rainbow room -- A company spun out of the Massachusetts Institute&nbsp;of Technology has devised a unique way to generate multiple&nbsp;colors in miniature flat-panel displays</A>&nbsp&nbsp&nbsp<NOBR>(1309 words)</NOBR></LI>
</UL>
<BR>
<H3>COLUMNS</H3>
<UL>
<A HREF = "#35"><LI>Fault line: Fishing on the Internet</A>&nbsp&nbsp&nbsp<NOBR>(446 words)</NOBR></LI>
<A HREF = "#36"><LI>Back panel: Digital Cellular Takes Its Toll</A>&nbsp&nbsp&nbsp<NOBR>(439 words)</NOBR></LI>
</UL>
</FONT>
<BR><BR>
<HR>
<PRE>
<A NAME = "1"></A><IMG SRC="/pubgifs/ec109.gif"><BR><BR>
Copyright
OEM MAGAZINE via NewsNet
February 1, 1996  Vol. 4 No. 

Alpha Site: Tool time for pc makers

By:
Rick Boyd-Merritt

I'm not much of a handyman around the house. I avoid preventive
maintenance of any sort and only operate in a reactive mode,
trying to fix things that have broken. I toss the odd collection
of tools I have higgledy-piggledy into a cupboard above our
washing machine. God help me when I need something specific
like a penny nail and have to fish around through the dregs
of that cupboard! The point of this little domestic confession
is that I believe the PC industry operates in the same sorry
way. OEMs don't think much about preventive maintenance.

They let Intel or Microsoft or, sometimes, Compaq make their
regular service calls on the PC architecture, assuming that's
enough to keep things humming. Every once in a while, a pipe
bursts, and they go digging around for a tool to deal with
the emergency. As often as not, the rusty, ill-kept tool box
takes the form of an occasional meeting place known as the
Video Electronics Standards Association.

VESA's been around since 1989 and lays claim to some 30-odd
standards. But the impact of the group has been a mixed bag.
It's produced helpful bench-level engineering specifications,
such as monitor timings. But its work on issues of broad impact
have often been met with push back from companies such as Intel
and Microsoft, who often sit imperiously at the periphery of
the group.

Thus, the VESA Local (VL) Bus made a significant contribution
to 486 systems but was snuffed out by the steamroller of the
PCI bus from Intel. Similarly, the Unified Memory Architecture
spec drafted by VESA now sits under a cloud generated by Microsoft's
lack of support for it and a competing approach being implemented
by Intel. A VESA set-top group was folded primarily because
its work was not timely or relevant enough for the emerging
market it hoped to serve; a new cable-modem group could face
a similar fate.

The problem lies in how the PC industry takes care of its tools.
There are 250 members in VESA, but organizers say it is a stellar
event when 30 turn out for a meeting on any single issue. Other
issues struggle along with a fraction of that support, often
from engineers who carry all too little clout with their companies,
resulting in meetings sometimes defined by the agenda of a
small group.

Worse yet, VESA doesn't even hold together well. Vendors try
to drive those issues in which they have a vested interest
and ignore others. The group hit rock-bottom recently when
member Dell Computer tried to collect royalties for a patent
relating to the VL Bus.

The computer industry desperately needs a strong open forum
that can shepherd the PC architecture through the changes it
faces over the next few years. VESA could be it-if we learn
how to take care of our tools.

You can reach me at rbmerrit@eet.cmp.com.




<A HREF ="#HeadList">Back to Headline List</A><BR>
<HR>
<A NAME = "2"></A><IMG SRC="/pubgifs/ec109.gif"><BR><BR>
Copyright
OEM MAGAZINE via NewsNet
February 1, 1996  Vol. 4 No. 

Kudos from Compaq

By:
Bob Stearns, Vice President, Technology and Corporate Development,

Your November issue was simply outstanding. I read it cover
to cover. OEM has become a really great publication. Congratulations!




<A HREF ="#HeadList">Back to Headline List</A><BR>
<HR>
<A NAME = "3"></A><IMG SRC="/pubgifs/ec109.gif"><BR><BR>
Copyright
OEM MAGAZINE via NewsNet
February 1, 1996  Vol. 4 No. 

The Net is Key

By:
Anonymous, via the Internet

The idea of a WorldPC/NetworkPC [November, page 28] is a very
welcome one. A mass-market, $500 to $800 computer is feasible,
and it will transform the PC industry if industry leaders pursue
the idea wholeheartedly.

Every person should own such an information appliance. The crucial
part is the network-a reliable, affordable, Internet-connected
network with at least ISDN speed. Currently, that essential
part is mostly missing. So I don't expect the World PC idea
to succeed in the United States within the next two years.
After that, things will change.




<A HREF ="#HeadList">Back to Headline List</A><BR>
<HR>
<A NAME = "4"></A><IMG SRC="/pubgifs/ec109.gif"><BR><BR>
Copyright
OEM MAGAZINE via NewsNet
February 1, 1996  Vol. 4 No. 

Accolades from Apple

By:
Warren D. Raisch, Manager, Apple Pacific, Business Development,

"The World PC" [November, page 28] was fantastic. Your ideas
and research were right on the money. Congratulations: You
articulated the opportunity perfectly.

I am a senior business development executive at Apple Computer
in Cupertino, Calif., but I am leaving Apple shortly to start
a venture. Your article reads almost exactly like my business
plan.

I have gathered $20 million in initial seed capital and am considering
partnering with Apple on certain technology areas. But I believe
that it will not be the PC vendors that will bring the World
PC to market. I believe that it will be driven by a cooperative
of consumer-electronics companies and large network-service
providers. Your article correctly points out the significant
opportunity in working with developing nations on this vision.
I have spent much of the past 16 years working with developing
nations' governments, the World Bank, the UN and private-sector
partners to build an IT infrastructure that could easily leapfrog
what we have in the United States. At this point, the barriers
are more psychological than technological.

Once again, congratulations on an outstanding article.




<A HREF ="#HeadList">Back to Headline List</A><BR>
<HR>
<A NAME = "5"></A><IMG SRC="/pubgifs/ec109.gif"><BR><BR>
Copyright
OEM MAGAZINE via NewsNet
February 1, 1996  Vol. 4 No. 

World PC inventor

By:
Alex Blok, President, Tecnation Digital World, Palo Alto, Calif.,

Regarding "The World PC," [November, page 28], I wanted to let
you know that I conceived the idea for a network computer over
10 years ago, and my company is working on it right now. Larry
Ellison, Sun and others are not the originators of the idea.




<A HREF ="#HeadList">Back to Headline List</A><BR>
<HR>
<A NAME = "6"></A><IMG SRC="/pubgifs/ec109.gif"><BR><BR>
Copyright
OEM MAGAZINE via NewsNet
February 1, 1996  Vol. 4 No. 

Nuts about Acorn

By:
Frank Towler, Strategic Unix Networks Corp., Victoria, B.C.,

I enjoyed your article on Acorn in OEM Magazine ["The World
PC," November, page 28] and would be interested in knowing
more about Acorn's products. Do you have a contact name and
e-mail address you could pass on?

Ed: You may contact Acorn through its public relations firm,
Cain Communications, at (408) 291-2585.




<A HREF ="#HeadList">Back to Headline List</A><BR>
<HR>
<A NAME = "7"></A><IMG SRC="/pubgifs/ec109.gif"><BR><BR>
Copyright
OEM MAGAZINE via NewsNet
February 1, 1996  Vol. 4 No. 

World PC Won't Sell

By:
Dave Ray, via the Internet

I recently read your article on the World PC and felt compelled
to comment. I don't know how long you've been around MIS or
DP, but I have been in DP for almost 28 years and have watched
the centralization/decentralization issue as well as the upsize
downsize paradigm. It never fails to amaze me that people actually
believe these have provided real benefit. The World PC is just
another stab at something that won't sell.

I used to punch my own programs and dreamed of the day when
I would just have a terminal. The ability to have a fully functional
PC that can do most of what a mainframe does (and lots of what
it can't do) is something the World PC would take away. How
can a machine that only has 16 meg of memory and no hard drive
do anything? Get real: If that was all that was needed, the
386 with 16 meg of memory would have been just the ticket-but
you know how fast it failed. Without storage, it's just a dumb
terminal.

Consider the cost reduction in terms of computing power. Why
would anyone want a $1,000 dumb terminal, when a Pentium with
8 meg and an 800-meg hard drive can be gotten for $2,000? Future
technologies will reduce the cost even more while providing
more power.

Have you seen any of the surveys regarding the public's desire
for this type of machine? Not one has found favor with the
idea. One of the few that touts this philosophy is Oracle-and
Oracle is one of the few companies that has not hit the mark
on any hardware or application it has created or promoted.
True, it has a great DBMS, but other than that, it fails miserably.




<A HREF ="#HeadList">Back to Headline List</A><BR>
<HR>
<A NAME = "8"></A><IMG SRC="/pubgifs/ec109.gif"><BR><BR>
Copyright
OEM MAGAZINE via NewsNet
February 1, 1996  Vol. 4 No. 

Power Prose

By:
John Ockerman, via the Internet

Thanks for your article in the November issue ("Batteries Aweigh,"page
72). It is an excellent review of the portable power market.
The arrival of the laptop catalyzed battery and charger development
like nothing else I can recall during 30-plus years in the
battery industry.

In the 1980s, while at Exide, I helped the Department of Energy
spend $160 million on EV battery improvements. For their money,
they got maybe 50 Wh/kg lead-acid, and nothing much better
is available today. It is tough to compete with gasoline, at
34,000 Wh/gallon.

The work at Integrated Circuit Systems Inc., is one of the best
approaches to fast charging that I know about. With Dave Whitmer's
help, I've used its chips in the lab and at home. I hope ICS
succeeds in taming lithium-ion charging, but I wouldn't be
surprised if another, safer lithium system comes along soon.
An unsatisfied market for power at $200/pound and up is a terrific
incentive.




<A HREF ="#HeadList">Back to Headline List</A><BR>
<HR>
<A NAME = "9"></A><IMG SRC="/pubgifs/ec109.gif"><BR><BR>
Copyright
OEM MAGAZINE via NewsNet
February 1, 1996  Vol. 4 No. 

Fighting Bloatware

By:
S. Y. Wong, via the Internet

"The World PC" [November, page 28] was very timely. I have been
visiting China since 1989 and find that the PC situation has
changed dramatically. My nephew's son acquired an entry-level
486SX machine upon his graduation from college. The machine
was about $1,000. The operating system, UCDOS, is DOS with
the character type modified to be 8 bits, with the eighth bit
encoding 1-byte ASCII or 2-byte Chinese GB-2312. The UCDOS
has an extender and interesting graphics but is only somewhat
larger than DOS, if the character-font library is not counted.

One of the biggest resource burners in PCs are the "modern"
operating systems, which are all patterned after the obsolete
concept of "multi-programming," invented for CPU time-sharing
back when CPU costs were prohibitive. Indeed, all computer
chips, CISC or RISC, are patterned after the CPU concept, which
dates back to the mainframe. The combination of obsolete processor
operating-system design approaches has combined to keep costs
high and memory requirements large.

What the industry needs is a design approach that weighs processor
and OS design as an integrated whole. The United States lacks
technical leadership of the caliber needed to pull off such
a bold strategy. In China, where bloatware does not quite suit
the economy, there is more opportunity for an entrepreneur
to implement such a grand vision. But luckily for the United
States, China tends to blindly follow America's lead.

Still, you might sound the warning that the United States could
lose its computer leadership, just as it lost earlier leadership
positions in such industries as clothing, footwear and TV sets.




<A HREF ="#HeadList">Back to Headline List</A><BR>
<HR>
<A NAME = "10"></A><IMG SRC="/pubgifs/ec109.gif"><BR><BR>
Copyright
OEM MAGAZINE via NewsNet
February 1, 1996  Vol. 4 No. 

Data Blaster

By:
Jay Knutson, niwot@csn.net, via the Internet

"The Broadcast PC is on the air" [November, page 64] was an
interesting article. However, Niwot Networks, using its file
transfer kits (hardware and software), can move data over dial
up telephones lines for one-fifth the price listed by Direct
Satellite System/DirecTv. Our customers-the most modern printing,
pre-press and magazine publishers-can deliver Mbytes on demand
for 11 to 20 cents/

Mbyte. That is better than Syquest disks and Federal Express
($28 to go, $28 to retrieve-meaning $56 for a 100-Mbyte disk).
And it arrives in minutes, not the next day at 10:30.




<A HREF ="#HeadList">Back to Headline List</A><BR>
<HR>
<A NAME = "11"></A><IMG SRC="/pubgifs/ec109.gif"><BR><BR>
Copyright
OEM MAGAZINE via NewsNet
February 1, 1996  Vol. 4 No. 

Krill or Cetacean?

By:
John Amenyo, via the Internet

"The World PC" [November, page 28] was an important and timely
look at the future of mainstream computing in the age of the
network. But several of the industry experts whom you relied
on for your conclusions demonstrated glaring examples of poor
strategic thinking. One could ascribe the lack of success of
the videotex to such confusion among the supposed visionaries
of the computing industry.

You wrote of an "awakening hunger for information, entertainment
and connectivity." I hope that by "connectivity" you mean the
need and urge to participate in electronic communities. If
so, I agree.

But you then veer off in the wrong direction by advancing the
opinion that "the potential of a machine that could satisfy
that hunger at a price tag of a few hundred dollars is enormous."
The leap to that conclusion is unwarranted. Total usage costs,
not merely the cost of the access unit, are the real barrier
to the Internet and Infobahn. The access-unit cost is easily
amortized. But the overall monthly costs of (remote) content
usage must truly fit people's budgets. That holds even more
true for the developing countries.

Another of your assertions that is not very useful is that "there
is disposable income, but it does not come in packages of $3,000
per household. It comes-dearly-in chunks of a few hundred dollars
at a time." True enough, but the implications here are misleading.
An assumed amortization schedule of $83/month is within the
range of a typical household phone bill (including long-distance
calls). Further, for several generations now, marketers have
perfected ways of handling the "bulk vs. chunk" income problem
 financing, credit arrangements and lease/rent-to-buy options.

It is a strategic delusion for a PC vendor selling into the
Internet/Web market to rely on the following kinds of assertions

 "In [the] server-centric view of computing, a network of servers
provides the data: from bank accounts to document archives
to the latest movies and games. The servers also run the heavy
parts of the applications. All the home computer does is run
'applets,' which are fragments of software that do just enough
local computing to minimize the system's communications-bandwidth
needs."

"One of the most important aspects of the $500 home computer
is that it is inherently a networked architecture, not a standalone
machine. For this reason, the rules of the road are entirely
different from a standalone's. For starters, local storage
 . . . is an unwarranted expense. The network can store and
deliver any necessary data. It can execute all but the tiny
applets of all the applications. And it can do this for only
a service charge."

That makes sense only if one assumes that in both the standalone
(application/desktop-centric) and networked (server-centric)
cases, the non-computing costs (i.e., the communication and
information costs) are zero or insignificant. Not on Terra.

Consider the matter from the consumer's viewpoint. Who will
own the PC? Who will own the network/connectivity and access
services? Who will own the content? How much will the "service
charge" be-the "toll" that will have to be paid every time
the consumer wishes to do anything that is even mildly interesting?
It is a fantasy on the part of minimalist World PC advocates
that consumers will be satisfied forever with a "look-but-don't
touch" approach to information consumption.

In the same vein, music-on-demand, video-on-demand and interactive
TV will all fail if all the user gets to do is treat the Internet
Web like a giant jukebox in a honky-tonk bar. What if a user
wishes to embellish and save the multimedia experience karaoke
style? A minimalist World PC would prevent that. People want
to pay to download and store copies of multimedia objects.

One thing is clear: The consumer will have to pay something
to somebody. The first question is how much. An even more important
question is to whom. Would information consumers find it more
palatable to pay the PC/software vendors, or would they rather
most of the money go to the networking/service providers? That's
the crux of the issue.

In your current daily life, which do you prefer? Do you like
to rent or lease things and pay for usage, or do you prefer
to buy and own them? Automobiles are very expensive, houses
even more so. But do most consumers prefers to own such items,
or to lease or rent them?

In my opinion, the strategic action needed to jump-start a social
revolution rivaling Guttenberg's is clear: Use the on-rushing
advances in technology to drive down the total usage costs
of travel on the worldwide information superhighway. That will
require very low cost but full-featured access stations. I
would strongly advise that the industry not proceed on the
model of the "PCjr," with crippled features and no frills.
Industry experience has been that it has never worked, and
nothing has happened to suggest that it now could be a successful
strategy.

I strongly believe the future of the PC for the Infobahn belongs
to the "beast" you blithely dismissed: a "Pentium-powered home
multimedia PC, supplied with a huge disk and at least 16 Mbytes
of DRAM, armed with specialized hardware for graphics, 3-D,
video and even telephone calls."

Who, after all, is the king of the ocean: the krill or the cetacean?




<A HREF ="#HeadList">Back to Headline List</A><BR>
<HR>
<A NAME = "12"></A><IMG SRC="/pubgifs/ec109.gif"><BR><BR>
Copyright
OEM MAGAZINE via NewsNet
February 1, 1996  Vol. 4 No. 

A Battery No-Go

By:
Greg Smith, Ultralife Batteries Inc., Montvale, N.J.

I have several comments on the "Batteries Aweigh" article [November,
page 72], which was generally very good: The observation that
cycle life and environmental issues will be paramount considerations
for OEMs is right on the mark. And the observation that lithium
ion is likely to supplant nickel-based batteries, particularly
as its costs come down over time, is also accurate. But lithium
polymer, meaning rechargeable batteries using lithium in its
metallic form, should not be occupying so many column inches
at this stage of the game.

Though lithium-metal is a battery maker's dream, it is not suitable
for rechargeable batteries. A lithium-metal battery's cycle
life is too limited (150 cycles), and the potential for catastrophic
failure is too severe. A particular problem is buildup of dendritic
structures on the lithium electrode, which eventually can pierce
the separator and cause an internal short-circuit.

The one-time leading proponent of lithium-polymer technology
was a company called Valence Technology. But Valence no longer
promotes the technology vigorously and has, in fact, made an
effort to switch to lithium-ion technology similar to Ultralife's.




<A HREF ="#HeadList">Back to Headline List</A><BR>
<HR>
<A NAME = "13"></A><IMG SRC="/pubgifs/ec109.gif"><BR><BR>
Copyright
OEM MAGAZINE via NewsNet
February 1, 1996  Vol. 4 No. 

Skinny or FUD?

By:
David Hunter, PC Product Manager, Spectron Microsystems, via

Well, the good news is that your "inside skinny" note ("The
inside skinny on native signal processing," October, page 58)
apparently is well read-at least by the newsgroup contributors.
The bad news is that your anonymous PC maker is either badly
informed or specifically intent on misleading your readers.

Spox is definitely a major component of NSP. In fact, all of
the Windows 3.1 interfaces provided by IA-Spox are virtually
the same under Windows 95. The only public changes that have
been made are those that were absolutely necessary to support
full 32-bit application programming interfaces.

If you want more details, I'd be glad to forward a copy of a
PowerPoint presentation I wrote last month. And if you really
want to go into it, I'd be glad to send you a CD copy of our
latest beta. Want to develop your own real-time signal-processing
code on the Pentium?

So, do you think that anonymous fax could be a part of somebody's
FUD campaign?




<A HREF ="#HeadList">Back to Headline List</A><BR>
<HR>
<A NAME = "14"></A><IMG SRC="/pubgifs/ec109.gif"><BR><BR>
Copyright
OEM MAGAZINE via NewsNet
February 1, 1996  Vol. 4 No. 

CORRECTION


Because of an editing error, a story in the October issue of
OEM Magazine ("Superdrives," page 62) incorrectly stated that
Data General supplies RAID arrays to Digital Equipment Corp.
In fact, Digital makes and markets its own RAID arrays. We
regret the error.




<A HREF ="#HeadList">Back to Headline List</A><BR>
<HR>
<A NAME = "15"></A><IMG SRC="/pubgifs/ec109.gif"><BR><BR>
Copyright
OEM MAGAZINE via NewsNet
February 1, 1996  Vol. 4 No. 

Servers in training -- Video servers are cutting their teeth
on training-on-demand applications, while waiting for the big
opportunities in consumer video-on-demand to appear

By:
Tam Harbert

After years of investing in video-server technology for elusive
consumer TV applications, computer OEMs are pinning their hopes
on the corporate LAN as the place to get some return on that
investment. The hot spot for servers today is in corporate
training-on-demand applications, but so far, vendors haven't
been able to gain much of a toehold.

Many OEMs are simply bundling their general-purpose servers
with video-networking software from third parties, in some
cases as a place holder until they've completed development
of new server software of their own. Some are developing hardware
as well. Redesigns are required, says Carl Lehmann, senior
research analyst at the Meta Group in Stamford, Conn., because
of the rigorous requirements of tomorrow's high-speed, multimedia
networks for isochronous data delivery in real-time.

"In my opinion, the big OEMs are groping," says Ed Delaney,
vice president of sales and marketing at Sea Change Technologies,
a small vendor of niche video servers based in Concord, Mass.
"They have invested a lot of money to develop these servers
for video-on-demand, and they are sitting there, trying to
figure out how to sell them." Based on proprietary chips and
buses, the servers are too expensive for the corporate market,
many observers believe.

Software bundling has emerged as an alternative to what Jim
Long, president of Starlight Networks Inc., calls "brute force
by hardware." Based in Mountain View, Calif., Starlight is
the market leader in video-server software. The company focuses
solely on the corporate enterprise market, producing client
server software and networking middleware that allow video
to be sent over local-area networks. Hewlett-Packard Co., IBM
Corp. and Sun Microsystems Inc. have all teamed up with Starlight
to deliver video on the LAN.

Long says the top applications for Starlight's products are
training and education, with training-on-demand a particularly
sweet spot. "Training is a line item in corporations today,"
says Long. "They have to find innovative ways of keeping people
up to date."

Indeed, U.S. corporations spent $50.6 billion in 1994 on training
for 47.3 million employees, roughly $1,070 per employee, according
to a survey conducted by Training Magazine. Training-on-demand
shaves those costs by eliminating the time and money spent
to send employees to off-site courses and to hire instructors.

Training-on-demand is interactive: a user can pull a video off
the network when he wants it, rather than having to watch it
when it's made available to him. Employees don't have to schedule
hours at a time for training away from their desks. They do
it at their convenience, right on the desktop. Moreover, studies
have shown that interactive multimedia training helps people
learn more effectively and retain more of what they've learned,
according to Starlight research. Studies have shown that return
on investment is as high as 400 percent.

Right now, OEMs must maintain at least two video-server strategies
 one for the corporate market and another for the consumer
market, says Dan Abouav, managing director of Hewlett-Packard's
Interactive Media Solutions Division, based in Cupertino, Calif.
"The requirements are different," he explains. "The number
of streams you have flowing from the server to the client is
different. But that will change over the next three to five
years. I believe that you're going to see solutions ranging
from the PC all the way up to tens of thousands of streams,
all basically using similar architectures."

In an alliance announced at Comdex/Fall for providing networked
video solutions on everything from PCs to Unix workstations,
HP and Starlight will sell preconfigured boxes through their
direct and indirect channels for corporate enterprise applications.
A new version of Starlight's NetWare-compatible video engine,
StarWare, will be bundled with HP's servers, delivering MPEG
1 video at 1.5 Mbits/s to as many as 100 simultaneous users
that is, if the server is wholly dedicated to managing multimedia.
The previous version could support only 10 concurrent users.
An HP NetServer preconfigured with StarWare, supporting 50
concurrent users and storing 20 hours of MPEG-1 video, will
cost about $28,000.

For its part, IBM is targeting training applications in the
corporate enterprise market by porting StarWorks, Starlight's
Unix solution, to its AIX platform for the RISC System/6000
server. StarWorks for AIX works in conjunction with IBM's Ultimedia
Services client software, says John Unthank, program manager
for AIX multimedia marketing in the RS/6000 Division, based
in Austin, Texas.

As for Sun, that company's Interactive Services Group, based
in Mountain View, announced a training-on-demand bundle last
October: a Sparcserver 20 preloaded with StarWorks for Sun's
Solaris operating system, along with MPEG-based video courseware
from vendors such as J3 Learning Corp., based in Minneapolis,
and ITC, based in Herndon, Va.

In a separate effort, Sun is also developing proprietary solutions,
such as the MediaCenter server, which incorporates home-brewed
software and some hardware tweaks. Based on standard Sun hardware,
the product is optimized for high-performance I/O and comes
with proprietary software that enables video to be treated
as a common data type in networks. The server runs only on
Fast Ethernet and Asynchronous Transfer Mode networks.

"We are trying to take Sun's basic hardware platform, add the
software expertise and a little bit of hardware in some cases,
and turn it into a video server," says Beverly Ulbrich, director
of marketing and business development at the group. Ulbrich
believes that MediaCenter technology will ultimately go into
both consumer TV and corporate LAN markets. Sun received design
guidance from its partners in the TV market, she adds, including
Thomson Multimedia SA, the Paris-based company that is working
with Sun on a digital interactive TV system using the OpenTV
operating environment.

Sun is also working with Siemens Stromberg-Carlson, based in
Boca Raton, Fla., and Scientific-Atlanta Inc., based in Georgia,
to deliver turnkey broadband interactive networks. Sun will
provide networking expertise and video servers for the alliance's
Interactive Multimedia Xpress architecture, while system integrator
Siemens contributes ATM technology and telecommunications systems,
and Scientific-Atlanta provides set-top boxes, cable systems
and processing equipment.

"The exact same products will go into those spaces [consumer
and corporate enterprise]," says Ulbrich, "although there may
be additional peripherals or software that we put together
to complete a solution for a specific application area." She
notes that the consumer and corporate products will be sold
through different channels.

Starlight dominates the market for video on corporate LANs today,
but it may not be able to hold that position for long. Third
party bundling strategies will undoubtedly conflict with new
solutions that emerge from Starlight's big OEM partners. In
addition, new, lower-cost video servers will arrive from lesser
known companies, such as the Network Connection, manufacturer
of a server called Cheetah with integrated hardware and software;
and Sea Change, which has carved out a 60 percent share of
the server market for commercial-advertisement insertion in
consumer video-on-demand.

Founded two years ago by Delaney and other former managers from
Digital Equipment Corp.'s Cable Television Unit, Sea Change
offers a low-cost server platform that could be easily adapted
to corporate or other markets, says Meta Group analyst Lehmann.
As for the Network Connection, the Alpharetta, Ga.-based company
already does 25 percent of its business in the corporate training
market.

But the real threat may come from video-server software being
developed by software powerhouses such as Microsoft Corp. and
Oracle Corp. "Those are the folks that could cause us trouble
down the road," Starlight's Long admits.

Not all server OEMs are enthusiastic about the corporate track
or training-on-demand. Some "just want to have video software
in their bag of tricks," says Long. But as the corporate market
takes on ever more importance, any OEM that plays the server
game will have to sit up and take notice.

Tam Harbert is a freelance writer based in Andover, Mass.




<A HREF ="#HeadList">Back to Headline List</A><BR>
<HR>
<A NAME = "16"></A><IMG SRC="/pubgifs/ec109.gif"><BR><BR>
Copyright
OEM MAGAZINE via NewsNet
February 1, 1996  Vol. 4 No. 

Tech law: Loose Lips Can Sink Standards

By:
Richard H. Stern

Dell Computer Corp.'s recent run-in with the Federal Trade Commission
over the VESA Local Bus has rightly worried some OEMs about
the legal risks inherent in the increasingly informal process
of developing standards. Until now, companies have felt free
to get together and agree on interfaces and protocols for new
technologies without worrying about patent and antitrust issues.
They assumed they shared an understanding that no group member
would take advantage of the others by later enforcing patent
rights over some key aspect of the spec. They also assumed
nothing they were doing would violate antitrust or trade-regulation
laws. The Dell case may have shattered those assumptions. Dell's
representative to the Video Electronics Standards Association,
which promulgated VL-Bus, assured the group the Austin, Texas
based company had no patents on the technology he urged it
to incorporate. But Dell did have patents. And when it demanded
that other PC makers license them or face litigation, the FTC
stepped in.

Ultimately, the commission took the side of free information
exchange, requiring Dell to open up its VL-Bus patents to all
comers. On the heels of this debacle, VESA is trying to buttress
itself against a similar occurrence by having members sign
a conflict-of-interest statement. This agreement, which was
being revised at press time, states that all information exchanged
at VESA meetings is confidential, and that no participant shall
make intellectual-property claims on the basis of it. In this
VESA is emulating the good example of larger, formal standards
groups, such as the American National Standards Institute and
IEEE.

Many people in the computer industry consider formal groups,
with their burdensome bureaucracy and time-consuming legal
requirements, too slow to keep pace with today's markets. But
the Dell action showcases the legal risk developers run when
they exchange information outside the protection of a formal
standards body. Three members of an informal, ad hoc group
may take the resulting proposal to a formal standards group,
while a fourth stabs the other three in the back by taking
it to his patent lawyer.

Of the three remedies to this problem, only one is practical.
OEMs could draft broad technology cross-licensing agreements
with any companies they hold informal discussions with; but
many firms would be unwilling to sign. Equally unlikely, companies
could supply their engineers with blank intellectual-property
agreements, to whip out for a signature should they find themselves
in the midst of a standards discussion.

But there's a more realistic solution available. And it's a
simple one. OEMs should be extremely selective about the people
they discuss emerging standards with, remembering that in many
cases, the best advice is to zip your lips. If a serious standardization
effort is posed, a written agreement on intellectual-property
rights is an essential first step.

Richard H. Stern practices patent and antitrust law at the Washington,
D.C., firm of Ablondi, Foster, Sobin & Davidow. He may be reached
by fax at (202) 296-3922.




<A HREF ="#HeadList">Back to Headline List</A><BR>
<HR>
<A NAME = "17"></A><IMG SRC="/pubgifs/ec109.gif"><BR><BR>
Copyright
OEM MAGAZINE via NewsNet
February 1, 1996  Vol. 4 No. 

The big picture: Jigsaw-puzzle integration

By:
Ray Alderman

Today, computer companies must define themselves as either com
ponent suppliers or component in-tegrators, and many of the
old in-dustry leaders are trying to be both, pushing them into
an identity crisis. The largest computer makers on the planet
hit the wall in the early 1990s and are in the process of re
engineering themselves. For many years, Apple, Digital, IBM
and others designed, manufactured and integrated all the components
of a computer system in a seamless, vertically integrated model.
The advent of standards-based hardware, software and communications
destroyed this model in the mid-1980s, and these companies
haven't fully adapted to the new market realities yet.

What really happened was that Microsoft and Intel established
themselves as the component suppliers to the computer world,
in spite of what Apple, Digital, IBM and other industry stalwarts
tried to do; and OEM manufacturers such as Dell, Gateway, Packard
Bell and others defined themselves as component integrators,
who could stitch hardware, software and communications components
into cost-effective products faster than the old school could
execute their vertically integrated product strategy. Consequently,
the old market leaders were "disintermediated," or bypassed,
in the value-added chain.

It's taken five years, but the old computer companies are learning
how to be component integrators, too, in order to compete by
the new market rules. But the rules have changed in the meantime,
with Intel redefining the role of the hardware-component provider
by selling motherboards to OEMs, rather than just chips.

The inherent problem with being a component integrator is differentiation
 Almost anyone can integrate standardized components into a
finished product. When the components are virtually the same,
the only differentiator is price. And since price is a function
of cost and cost is a function of volume, the ultimate differentiator
for a large component integrator is volume. For the smaller
component integrator, survival resides in specialization and
small niches.

But another change is on the horizon. It revolves around the
convergence of the television, telecommunications, teleconferencing
and entertainment markets. Will the delivery platform for these
services be the traditional computer or some new consumer product?
Intel's native-signal-processing initiative is its first defensive
act to keep the PC platform as the delivery mechanism for all
these new entertainment and communications services.

When component technologies are readily available, the only
decision the integrator needs to make about next-generation
computers is how they're packaged. Will it look more like a
PC or component-based stereo equipment?

Ray Alderman is executive director of the VMEbus International
Trade Association. He can be reached at exec@vita.com.




<A HREF ="#HeadList">Back to Headline List</A><BR>
<HR>
<A NAME = "18"></A><IMG SRC="/pubgifs/ec109.gif"><BR><BR>
Copyright
OEM MAGAZINE via NewsNet
February 1, 1996  Vol. 4 No. 

The PRET-A-PORTER PC -- (Ready-to-Wear Computing)

By:
Jacqueline Henry

A monitor that sits in a pair of glasses. A keyboard you wear
on your arm like a bracelet or a watch. A computer that clips
to your belt of slips inside your shoe. The vision of the far
future of personal computing steps outside today's gray boxes
to a brave new world where "personal" is personal indeed. In
this vision, computers become an extension of self, worn on
the body just like eyeglasses, jewelry and shoes--the ready
to-wear PC. Calvin Klein and Ralph Lauren may not be showing
on yet, but Reebok is taking the idea seriously. The sneaker
company is sponsoring some of the research in wearable computing
at the Massachusetts Institute of Technology, where a team
from MIT's Media Lab has cobbled together a prototype that
conjures up images of Maxwell Smart's oxford-shoe phone from
TV's "Get Smart," updated for the more casually clad '90s.
"The concept of power sneakers in intriguing," team member
Thomas Zimmerman wrote in his MIT master's thesis, Personal
Area Networks: Near-Field IntraBody Communication. "Shoes are
something that we always carry with us. For convenience, each
pair of our shoes could have their own [computer] insert."
In a high-tech twist on family values, Zimmerman suggests that
"at night when they are put in the closet, perhaps with our
spouse's and children's shoes, they could update each other
on the day's activities."

Though the power sneaker may sound farfetched, the wearable
computer has, in fact, been around for a while serving specialized
niches, such as the military and certain industrial sectors.
Though far from becoming a mass-market item-it's still too
bulky, expensive and power hungry-the pr t- -porter PC is making
strides toward the mainstream, thanks to advances in displays,
voice-recognition software and other enabling technologies.
Pioneering developers are weaving the fabric out of which tomorrow's
consumer wearables will be stitched, even as academics in places
such as the University of Oregon, Carnegie Mellon University
and MIT delve into ways to use the devices, once here, to their
fullest potential.

"I think that each of us will carry an information appliance,
as we [now] carry a pager or a cellular phone," says Zary Segall,
head of the computer-science department at the University of
Oregon. "We will be able to access information anyplace, anytime."

The MIT prototype is a step in that direction. It packs a CPU
and a pair of smart, battery-powered transceivers into an aluminum
case, measuring 3.76 x 4.8 x 1.48 inches, that slides into
a shoe like a Dr. Scholl's insole. It's far more than just
Mips and slippers, though: The system uses the human foot as
both a power source and a communications gateway.

"When you walk, you have all this power," explains Thad Starner,
a graduate student and researcher at the Media Lab, referring
to both body heat and movement as potential sources of energy.
"We're trying to recover some of that and use it to power your
computer." Starner estimates that of the 57-odd watts of energy
a person generates by simply walking, "we think we can recover
between 5 and 17 watts."

The Media Lab crew has demonstrated its prototype operating
as a node in a "personal-area network," which uses the human
body as the medium for computer-to-computer communications.
In such a linkup, two suitably soled human beings could, for
example, exchange electronic business cards merely by shaking
hands.

"When a woman and man [wearing these computers] are in close
proximity, particularly when they shake hands, an electric
circuit is completed, allowing a picoamp signal to pass from
the transmitter through her body to his body to the receiver,
and back through the earth ground," says MIT's Zimmerman. Once
the circuit is established, the transceiver in the woman's
shoe would send a string of ASCII characters to the receiver
in the man's shoe; his data likewise would take the footway
back to her.

"Wearable computers are going to change everything," Starner
proclaims. Baseball and football fans might wear one to a big
game or Super Bowl party, he suggests, calling up players'
stats and settling barroom bets on the spot. Reporters covering
a trial might load their wearables with pattern-recognition
software that helps them place names with faces, and then call
up biographical data on a subject before they go in for an
interview.

"We could eventually get to the point where you've got a cigarette
sized PC," says Scott Clark, business-development manager for
mobile information systems at Rockwell International Corp.'s
Collins Avionics & Communications Division, based in Cedar
Rapids, Iowa. "It will be inexpensive. It will be wireless.
And people will literally wear it around the house."

For this vision of the ubiquitous wearable PC to become a reality,
the computer must become more "human-literate," says Segall
of the University of Oregon. That means it must understand
not only speech, but gestures and eye movement and, on a more
advanced level, even "sense" what the user needs. For instance,
the wearable information appliance an airline maintenance worker
uses might automatically bring up a schematic of the specific
747 that has just landed and is taxiing down the runway.

"The challenge is how to come as close as possible to the understanding
of human behavior and how this translates into the commands
of an information appliance," Segall says. Then you find a
means of transmitting this data "in a way that enhances the
productivity of whatever the user is doing."

Neil Gershenfeld, an associate professor at MIT's Media Lab,
pushes the idea a quantum leap further. Ten years from now,
we will all be wearing computers, Gershenfeld said at an anniversary
bash for the lab last October. Twenty years from now, we will
implant the computing devices we want into our bodies. And
30 years from now, we will use genetic engineering to grow
the devices we desire. From this angle, "Intel Inside" takes
on a whole new meaning.

Shoe phones and implants aside, just what is a wearable PC?
Today it's most likely to be a belt-mounted computer powered
by a battery with a life of about four hours, along with a
heads-up display integrated into a headband, helmet, goggles
or eyeglasses, plus some type of input device-an arm-mounted
keyboard, for example, or a head-mounted microphone for voice
input. The addition of voice control is a must in some markets,
where users need hands-free operation to do their jobs effectively.

In terms of features, wearables have "everything that's in your
desktop PC," says Clark of Rockwell, which makes a version
called Trekker. "Today, it's a 486DX [CPU]; tomorrow, it will
be a Pentium. We're taking your desktop and shrinking it down."

Currently, companies such as Rockwell and Computer Products
& Services Inc. (CPSI), based in Fairfax, Va., are mining vertical
niches, producing wearable PCs for maintenance workers, firefighters,
overnight-delivery companies and other road warriors. This
market is expanding fast. For example, Symbol Technologies
Inc. sold around 5,000 of its first-generation Gladiator systems,
developed in 1992 for warehouse workers fulfilling orders for
a large pharmaceutical distributor. The Bohemia, N.Y.-based
company has just landed an order valued at $15 million from
United Parcel Service for "well into five digits" of its second
generation Gladiator II.

"We expect there will be a sizable market for wearable computers
in vertical industries, and by sizable we mean tens of thousands
of units a year," says marketing director Barry Issberner.

Today's wearables don't come cheap, and manufacturers are preoccupied
with driving down the price tag without stinting on features.
Rockwell's Trekker, for example, is priced at $13,000. It boasts
a tiny LCD screen-about 0.7 inch on the diagonal-coupled with
an optical system that makes the displayed image seem as though
it is on a 14-inch monitor positioned 18 inches from the viewer's
eyes. "That lens system was a major milestone," Clark says.
Without it, "people would get vertigo."

A high-end unit from CPSI-a Unix machine that weighs less than
three pounds and totes a 1-Gbyte hard drive and video acceleration
costs about the same. "People are not willing to pay an absolute
arm and a leg for these things," Clark says. "We continue to
drive prices down even though customers continue to demand
more in the unit. We're decreasing power, decreasing size,
decreasing weight and increasing performance."

Startup Phoenix Group, based in Plainview, N.Y., recently bowed
a computer not much bigger than a cigarette pack-3 x 6 x 1.3
inches-for just under $4,000. Designed to be worn in a shirt
pocket, the 12-ounce Hummingbird packs a 25-MHz 486SLC microprocessor,
170-Mbyte hard drive, 2 Mbytes of DRAM and voice recognition.
An arm-mounted keyboard is an optional accessory.

"We're working on an Air Force R&D contract to put the entire
computer in a headband," says Phoenix president Richard Pandolfi.
"We're using chip-on-board technology to reduce the size and
weight. It's the same technology used in many of our missile
systems."

Like many of the makers of wearable PCs, Phoenix initially targeted
military applications but is now seeking new niches in an age
of defense cutbacks. Rockwell's Space System Division, based
in Downey, Calif., is adapting the computer-simulation and
sensing technologies it developed for U.S. defense systems
into wearable gear for the public-safety sector, hoping to
equip police and firefighters with the latest in computing
fashions. The same predictive tools and computer-modeling programs
used to simulate a missile launch can track a moving fire front
and simulate how it will change in relation to wind, terrain
and time, the company says.

"We envision that every emergency person-whether [facing] fire,
flood or hurricane-will have such a computer on them to give
them access to higher levels of support and connect them with
all of the resources and predictive tools that we have in the
space business," says Chuck Daniher, program director for conversion
programs at Rockwell.

The utility industry is another potential market. Technicians
from the Long Island Lighting Co. (Lilco) are testing a system
jointly designed by Phoenix and defense contractor AIL Systems,
based in Deer Park, N.Y., for servicing customers' gas burners
and monitoring combustion-control systems at the company's
own power plant.

Instead of consulting thick manuals, explains Timothy J. Driscoll,
Lilco's director of R&D, the technicians access digital "hypermanuals"
on their heads-up displays, including diagrams and blueprints.
If they need a part, they can request it via a link to Lilco's
host system. Instead of scribbling data on a clipboard and
typing it into a computer back at the office, they can record
their notes by voice and download them while still in the field.

Besides size and power consumption, manufacturers have had to
overcome other technical hurdles to get wearables to where
they are today. The first challenge was reliable, rugged CPUs.
"This unit is bouncing on someone's hip while they're walking,"
says Clark of Rockwell's Trekker program. "The hard drive in
any desktop or laptop is not meant to be bumped around." That
problem was solved by using ruggedized casing and component
packaging technology developed for the military.

Voice recognition was also a challenge. "People don't realize
how far we've come with voice interfaces," says Edward Newman,
president of CPSI, which has been licensing voice-recognition
software from a third party but is developing its own technology.
"[These days] you train the system; you don't have to train
the user. The system adapts to the user. It's a combination
of independent speech with on-the-fly training."

Which is not to say voice systems are fully mature. "I still
think the speech technology has a way to go," says Segall at
the University of Oregon. "We need continuous-speech recognition
systems with a very high degree of accuracy and with a large
vocabulary." Displays need work too, says Segall, predicting
an evolution from direct-view screens to displays that project
information right onto the retina. "We need a type of display
technology which is amenable to humans," he says. "It's still
a ways out."

On another front, vendors are enthusiastic about wireless technology,
with Clark of Rockwell predicting that the headsets connecting
to belt-mounted units will soon be untethered. "The first step
is wireless from headset to CPU. The next stage-which is actually
available now-is a unit where you can pipe right into an existing
local-area network." Trekker, he notes, can handle wireless
LAN cards; "in the future, I see this expanding into wide-area
networks as well."

Tim O'Neal has embedded infrared links into the latest version
of the belt-worn printer his company-O'Neal Product Development
Inc., based in Irvine, Calif.-develops for mobile workers.
O'Neal says the InfraRed Data Association is just beginning
to promote IR for wearables after initially focusing on higher
volume targets, such as notebook computers. Though O'Neal has
only sold 10,000 wired printers to customers, such as Hertz,
in two and a half years in the market, he says business is
heating up for the wireless versions.

InterVision, based in Raleigh, N.C., is also exploring IR technology.
"We're working on a new version [of its wearable PC] that will
incorporate an infrared camera on the other side of the helmet,"
says president and founder Dan Glaser. "It doesn't block peripheral
or forward vision, so you can see where you're going while
at the same time displaying infrared images." These can guide
an emergency worker through smoke or at night. "We're also
incorporating a radio link so all images can be piped back
to a first-response team," says Glaser, who foresees many applications,
from bomb disposal to tele-medicine.

With vertical markets expanding nicely, developers can't be
faulted for looking longingly at the consumer side. But no
one expects to see a power sneaker of the MIT variety soon.
"I don't think anyone will be interested in wearable systems
for anything but industrial markets for some time," says Symbol
Technologies' Issberner. "PDA players like Apple and Hewlett
Packard are still trying to determine users' needs; they are
not even worried about making their products wearable yet."

Still, as some wearables, such as Phoenix's PDA-like Hummingbird,
press into the market from above, so do developers in the corporate
and consumer mainstream push from below. For instance, Motorola
has deemed its new palm-sized, 3.1-ounce Startac as a "wearable
cellular phone." In October 1994, Timex Corp. launched a kind
of wearable computer of its own-the Datalink watch, a Dick
Tracy-style gizmo touted as "a personal organizer on your wrist"
that can download information from a computer via a proprietary
protocol. The device got a big boost last August, when Microsoft
Corp. launched a version of its Scheduler Plus application
that bundled the Datalink protocol software.

Jan Mladek, who manages the Datalink group, says Timex had sold
about 100,000 of the watches as of August 1995. At $100 plus,
"it's quite a successful product for a new concept at that
price point," he says. However, Mladek acknowledges that there
are limits to what can be done in the form factor of a wristwatch;
the current product has the equivalent of just 1 kbyte of memory.

A 1-kbyte Timex at one end, a 1-Gbyte wearable Unix machine
at the other. And way out in the distance, perhaps, a strange
new horizon where biology and electronics begin to merge. Somewhere
in the midst of all this, observers say, lies an industry waiting
to be born.

The only thing to come between you and your Calvins? Someday,
perhaps, a computer.

Jacqueline Henry, a freelance writer based in Holbrook, N.Y.,
is on MCI Mail at 693-3096.




<A HREF ="#HeadList">Back to Headline List</A><BR>
<HR>
<A NAME = "19"></A><IMG SRC="/pubgifs/ec109.gif"><BR><BR>
Copyright
OEM MAGAZINE via NewsNet
February 1, 1996  Vol. 4 No. 

Weight watchers for wearables

By:
Jacqueline Henry

Dieting is as much of an obsession for the wearable computer
as it is for your average supermodel. Symbol Technologies has
been slimming down for years. "Our first products [20 years
ago] used what were, in effect, car batteries on carts hooked
up to adding machines that people used in grocery stores,"
says marketing director Barry Issberner. "Over time, we turned
that into something you could hold in your hand."

In 1992 came the Gladiator, so called because its hand-, arm
 and belt-mounted components made users-warehouse employees
for a big pharmaceutical concern-look like electronic warriors.
Essentially a bar-code scanner with a 28-ounce computer, the
system "was fairly elaborate to wear, with three separate pieces
of equipment linked by six to 12 feet of cabling," says product
manager Robert Hurt.

In January, Symbol was expected to launch Gladiator II, a single
box, 9-ounce computer, powered, like its predecessor, by an
NEC V25 chip and priced at $2,200 and up. Replacing the bulky
hand scanner for bar-code inputting is a finger-mounted unit
that Symbol employees refer to as a "Liz Taylor ring."

InterVision has seen a similar evolution in its wearables. Now
on its sixth product generation, the company's first device
was a belt-mounted computer based on an Intel 286 processor,
with little internal memory, no expansion capabilities, a miniature
101-key keyboard that had to be held and a battery life of
only 30 minutes. The PC came in two boxes, each measuring 5
x 7 x 2 inches; a third box was added for voice activation.

Today's model, by contrast, weighs less than two pounds. It
comes in a single box, complete with voice activation, lots
of expansion capabilities and a 640 x 480 VGA flat-panel screen.
Weighing in at under 4 ounces, the display can be hooked to
a headband, hard hat or firefighter's helmet, enabling the
wearer to, say, bring up the plans of a burning building while
walking through it, determining where the exits are and whether
dangerous materials are present.

"Most people think about products in terms of pounds," says
Issberner. "We think in terms of half ounces."




<A HREF ="#HeadList">Back to Headline List</A><BR>
<HR>
<A NAME = "20"></A><IMG SRC="/pubgifs/ec109.gif"><BR><BR>
Copyright
OEM MAGAZINE via NewsNet
February 1, 1996  Vol. 4 No. 

A wearable skids off the runway

By:
Rick Boyd-Merritt

About four years ago, in a bid to raise its profile, a now-defunct
pioneer of portables launched what might have been the first
ready-to-wear computer. It was the brainchild of marketer Ken
Dulaney, looking for a compelling spin for Grid Computer's
new GridPad handheld PC.

"We couldn't make our computers any more rugged, so we came
up with the idea of putting a strap on them," says Dulaney,
now a research analyst at the Gartner Group Inc., based in
Stamford, Conn. "Then we designed a whole line of peripherals
radios and printers and so on-you could hook on a belt."

To promote this strap hanger, "we staged a press conference
at the Hotel Macklowe near the New York City garment district,"
Dulaney recalls. "We hired this gorgeous model who came walking
down the runway with this cape on. When she swung the cape
aside, there was this big, brown, three-pound computer on her
arm."

This early experiment fizzled, the wearable GridPad flopped
and the company was ultimately acquired by PC maker AST Research,
based in Irvine, Calif. Many Grid designers fled to developers
of vertical-market systems, such as Norand, Symbol and Telxon.
And those who recall that first wearable seem a little embarrassed
by it.

"It was all just hype to get some press," says Jeff Hawkins,
a former Grid designer. And when counseling clients for Gartner,
Dulaney tells them that wearables are solely for vertical markets.
"Other than pagers and cellphones, people don't want a lot
of bulk," he says.




<A HREF ="#HeadList">Back to Headline List</A><BR>
<HR>
<A NAME = "21"></A><IMG SRC="/pubgifs/ec109.gif"><BR><BR>
Copyright
OEM MAGAZINE via NewsNet
February 1, 1996  Vol. 4 No. 

The Interview: Carl Yankowski



As president and chief operating officer of Sony Electronics
Inc., Carl J. Yankowski is, in effect, the head of Sony's diverse
hardware businesses in the United States that generate some
$8 billion in annual revenues. An MIT graduate with degrees
in both business and electrical engineering, Yankowski is himself
a product of convergence. He has worked at companies as diverse
as General Electric, Memorex, Polaroid, Pepsi and Procter &
Gamble, and bubbles with ideas about how to drive Sony into
the heart of new, cross-industry markets. We spoke with Yankowski
late last year in his booth off the bustling show floor at
Comdex/Fall, where Sony tipped its long-anticipated plans to
become a player in personal computers. The Park Ridge, N.J.
based company demonstrated prototypes of workstation-class
machines that use high-speed serial interfaces (1394) to tie
together a suite of digital audio and video services and devices,
thus playing PCs to Sony's home-court advantage.

How do you define the consumer PC?

A consumer PC would be something you would want to have in your
living room or family room. It would be an appliance that would
be inviting and fun to use. Hopefully, it would have one switch
to turn off the CPU and monitor simultaneously, which is something
we don't seem to be able to accomplish in the industrial design
of what's out there in the market today.

It's part of your stereo rack?

Exactly. The content and the hardware would be somehow bundled
together in a way that would make you feel this isn't just
a box that lets you do other things-this is a box that creates
new things for you to do in its own right. It's a link between
all the audio/video components and communications occurring
today in the home. It's kind of like a melting pot.

What are Sony's plans for getting into PCs?

The reason we want to get into the PC business is to learn how
to leverage our audio/video strengths into a PC line, which
will gradually cascade over a whole landscape of appliances
and products. So it's not [so much] that we are entering the
PC business as that we want to develop the landscape of where
communications, computer and audio/video equipment go. It's
a broader point of view than most of the people in this industry
are taking.

How does Intel fit into this vision?

We think Intel has a lot of interesting technology, not just
in their chips but in their Architecture Labs and their systems
skills. When you combine that with our strong A/V experience,
it just makes sense for us to be in business together.

Our relationship right now is simply a memorandum of understanding
that says we are going to develop exciting new products using
our technologies and their technologies, to be sold and marketed
by Sony under the Sony brand name and marketing channels. Anything
more specific than that we have yet to work out.

What will the Sony PC look like?

Exactly what we are working on is proprietary now. But you can
say that we want the Sony PC to have a unique look and feel.
We want it to be fun. We want it to have a positive impact
on changing people's lives, just the way the Walkman had a
positive impact. We're planning to launch the first products
in the fall [of 1996]

We think we are expert in combining different technologies from
different disciplines. The Walkman was not an invention, it
was a combination of products already on the shelf. We made
our existing headphone technology stereo, married it to our
business-dictation technology and got the Walkman.

What do you think about the so-called $500 PC?

I take issue with this fragmentation theory that is being spread
around that says the computer industry is going toward large
or small PCs or networked PCs or personal digital assistants.
There will be a whole landscape of products.

John Sculley said PDAs will predominate. Some people at Qualcomm
say that smart phones will predominate. People at Mtel will
tell you that ultimately two-way paging is going to be the
thing. I think it is wrong to say that PCs, or this little
surfer box, or the set-top box will predominate. I think it
will be a landscape of all these products living together.

I think there will be a market for $500 PCs, but I don't think
it will blow away other markets. It's [similar to how some]
people said PCs would eliminate paper, but paper usage actually
went up. It's too narrow and non-visionary to say some particular
device is going to be the savior across a broad category of
consumer applications. It never works out that way.

How will TV and PC products merge?

I think TV/PC products will go two ways for us. One, we will
look at interactive TV [as a means] to get to the Internet.
We are looking at set-top boxes in an arrangement with Microsoft.
And we think the 1394 [serial interface] will let us take digital
signals from direct-broadcast satellites [DBS] and bring them
to a computer, add value to them and pump them out to a whole
variety of devices [for video editing or storage]

It's not just the video side, but also the audio side. Our 100
CD audio changer is a good-news/bad-news machine. It's selling
great. You can put 100 audio CDs in one box and access any
title or song, but you have to sit down and program everything
in manually. It takes forever. So why not scan in the number
of tracks and their length and sequence when you load the CD?
Then [you can] automatically match that with a database lookup
table you downloaded from Compuserve or AOL, and you have all
the information about the content, the author, title, date,
background video clips-whatever you want.

You can do that in a nanosecond on the PC. So the PC becomes
a lubricant to make the A/V easier. You can also think about
the number of remote controls you have in your home today to
control components in an A/V system. You can simplify that
a lot by a digital line to the PC.

What effect will this vision have on Sony's structure?

We see a tremendous synergy here, and basically what we are
going to do is decompartmentalize Sony. Traditionally, because
of our strengths and roots, we were an A/V company, and No.
1 in A/V. We plan to remain so. But when you look at A/V, computers
and communications, the rich area is these crossovers, these
intersecting circles. This is a territory no one has put a
stake in. It is a high-margin, high-value-added, high-society
impact area. That's where we'll add value.

Is this philosophy borne out in how you are organizing your
PC design teams?

We are having the PCs designed by the one industrial design
center inside Sony.

What about the electronic design?

Right now in Japan, we are integrating [people from] the A/V
groups as well as the business information-systems group, which
has the most experience [inside Sony] in computers. We are
hand-picking the best people and putting them together to make
these PCs happen. We'll do some of the work in Japan and some
in the United States.

Are you really "decompartmentalizing Sony," or creating a new
compartment for PC products?

Both. You want to get people from different divisions to work
together. You want to have a focus within a division, and you
want to have [separate] A/V and communications and computer
divisions. But the richest [market] area is the crossover.

A month ago, I formed a telecom [group]. In that group I put
our wireless-telecom group, which is partners with Qualcomm
on CDMA [code-division multiple access]. With Qualcomm, we
comanufacture-and are finalizing a joint agreement on marketing
terminal sets to major systems suppliers. Our MagicLink [PDA]
division and home portable-phone groups were also put in that
sector. And we are moving engineers to the United States, because
the digital base of the home systems are overlapping even within
a sector. That's decompartmentalizing.

Then, in the beginning of [1996], I'm starting up a new business
development group [to oversee the] product-planning and priority
process, so we can develop crossover opportunities independent
of any given business group by assigning resources of different
groups to look at these opportunities. That's new for us. That
will also help us generate a whole new breed of innovative
general managers and engineers, because they are going to be
exposed to so many more tools for developing products.

I also want to create a new model for hardware/software systems,
and I'd like to do that in the PC business, too. I'd like to
create a model where the hardware guys get some percentage
of the downstream revenues of the system. This [should be]
a razors-and-blades model, [but] the equation is out of balance
right now.

Hasn't there, in fact, been debate within Sony as to whether
it's a hardware or software company?

Yes, and the answer is yes and yes. I don't think of myself
as running the hardware part of Sony.

But you do.

I do, but I don't think of myself that way.

How do you think of yourself?

I think of myself as coming at systems software and services
from a hardware angle and leveraging that. The old Sony is
basically a big A/V unit, a little computer unit extrapolated
basically from Trinitron [display] technology and a little
comms, mostly by partnership from the outside. Then we had
pictures, music and electronic-publishing centers. The interactivity
of these groups was not so great historically. Old Sony.

The new Sony is an A/V unit [plus] much expanded computer and
communications units. And I want to strike a tent peg in the
crossover areas. Our camcorders, for example, should have an
infrared link to our TV sets. Why not bypass the VCR?

What about software?

We have to have people in Sony Electronics who write software,
whether it's firmware device drivers or integration software.
Over the last two years, we've invested $7 million in our broadcast
division for programming capabilities. That's how we [built]
the Hughes DBS uplink station. It was a $50 million project,
and $20 million of it was code.

Also, we created a whole new concept when we took the hardware
side of A/V and the software side of content and made a Playstation
[videogame] division. It's not just content or A/V. I feel
ownership for it, and I'm supporting it with logistics and
distribution. Ultimately, it helps Sony [generate] profit.
That improves the overall ability for me to invest in new systems
concepts as well.

A third issue is distribution. We have to be savvy about different
forms of distribution into the home, such as DBS. We need partnerships
with telephony providers for [delivering] new features into
and out of the home.

Sony is one of several DBS licensees. With the growing competition,
will this still be a significant business for you?

Yes, we will sell several hundred thousand pieces this year.
As an industry, we risk making it too [much of a] commodity
too fast. It's my hope we can work with DirecTv to open up
[new service offerings]. For example, Sony has a gaming station
that we'd love to see on DirecTv because we could develop tie
ins on the hardware side. That's the kind of differentiation
we need, or else you have a bunch of people competing on a
commodity basis, and that's not good for the industry.

What impact will digital TV have on your plans?

As video becomes digital, we can manipulate the data and add
value to it in terms of new appliances and applications. I
think the Federal Communications Commission should mandate
a date by which the industry should have decoders available
in TV sets for a whole range of digital-broadcast schemes.
By that date, if the TV sets are ready, we should see applications
develop.

Is that likely to happen?

I don't think it's a likely scenario, but it would be neat if
it would happen. I think everyone would hope we can get there
voluntarily. I think [digital TV services] will start to be
turned on around the turn of the century.

What's on Sony's agenda in terms of standards for digital TV?

Our initial point of view is that we would like to see those
standards generated voluntarily. But right now, we are hoping
we can get some guidance from the FCC in terms of dates by
which to get those standards done. I think there's nothing
like the pressure of time and a commitment to force people
to work together. Before commenting on which standard is better
than others, I think we should just have a drop-dead date by
which the industry should say, "OK, all the technology is out
there, we know we can do it, the government says we have to
be ready by this date, so let's get going."

If you were FCC chairman Reed Hundt, what date would you set?

1998, January 1. I think something like that is doable.

Are you going to lobby the FCC to set such a date?

This is something we are going to bounce off the Electronic
Industries Association.

In hopes they take that position to the FCC?

Right.

What will bring back the PDA?

I don't think of it as a hardware business. I look at it as
applying a small, portable communications device to mostly
diagonal and vertical markets. I think we were a bit overzealous
initially as a team in saying that simply e-mail would sweepingly
change our lives. So we are still investing in this business.
And now we have a second generation of the [MagicLink] hardware.

Your PDA was designed as a horizontal consumer device. If you're
going to reposition it as a vertical-market device, would it
make sense to get out of your relationship with PDA software
developer General Magic Inc.?

That's always a possibility. But I think the Magic Cap OS still
has some unique advantages. We've found that people in some
vertical applications like the simplicity of the operating
system. For example, Visa is very interested in an electronic
wallet, and you want to have something easy to use. Or you
might want to have an application for betting on the state
lottery.

I do wish, frankly, that Telescript [General Magic's smart networking
software] was further developed in terms of its application
capability. I think it's in competition with [Sun Microsystems']
Java for that distinction. I was just talking to the guys at
Netscape today, and they said they wished there were more Telescript
applications.

We've radically improved our PDA hardware, and we are still
learning about the systems solution. We aren't investing that
much money [in General Magic], and the money we are investing
is valuable to me because the learning will apply to the PC
world, the A/V world, the telephone world. It's a generic investment
in a core competency for Sony. Ultimately, I think it will
blend nicely with our smart phones, too.

Where do you see the main wireless opportunities?

In the United States, we feel CDMA, and PCS extensions of CDMA,
are the major opportunities. But we also do business with McCaw
in TDMA [time-division multiple access]. Ultimately, there
will be a shakeout of some sort. But our strategy right now
is to keep a foot in each product and see where the market
goes. And we also have a partnership with Mtel to sell and
distribute their pagers.

I've been expecting to see Sony devices that would work with
its two-way paging network.

They're coming. That's part of the vision.

Is there a danger the smart phone or pager will be the PDA of
the telecom world?

We'll have to find that out. In any event, people will need
a terminal, and we are going to be experts in building good,
compact terminal sets.




<A HREF ="#HeadList">Back to Headline List</A><BR>
<HR>
<A NAME = "22"></A><IMG SRC="/pubgifs/ec109.gif"><BR><BR>
Copyright
OEM MAGAZINE via NewsNet
February 1, 1996  Vol. 4 No. 

PCs step into 3D -- The journey into the world of three-dimensional
computing will be a difficult one for PC makers, according
to the publisher of the PC Graphics Report

By:
Jon Peddie

PC designers preparing to take the plunge into three-dimensional
graphics would be well advised to stick to the shallow end
of the pool. The 3-D waters are treacherous for the uninitiated.
Developers can't even agree on exactly what features make up
3-D, nor come to any kind of consensus on 3-D architecture.
Moreover, the designation is not monolithic: Applications that
use 3-D vary greatly as to what sort of hardware acceleration
gives them the best boost for the buck. And yet, virtually
every vendor of 2-D graphics silicon and software, as well
as several untested startup chip companies, seem to be stampeding
into the 3-D arena. It's a sudden rush, not a careful and thoughtful
migration-never mind that the underlying graphics chips behind
many of these products have only recently been enhanced with
video capabilities. Three-dimensional technology is not really
new; 3-D has, in fact, been available in the PC market for
nearly a decade, with two companies leading the pack: Artist
Graphics, the St. Paul, Minn. based pioneer in graphics boards
for computer-aided design; and Matrox Graphics Inc., the Dorval,
Quebec, developer that was first to market with a consumer
level 3-D product. Until fairly recently, however, not much
software could take advantage of 3-D. With prices for 3-D boards
starting at around $5,000, sales have typically been in the
tens of thousands of units, and few software developers could
justify the investment required to support such niche devices
with their applications. The market was left to a handful of
companies which implemented their 3-D rendering completely
in software, often targeting scientific, engineering or other
niche markets and running on workstations.

Then, in the early 1990s, computer games grew more graphically
sophisticated, fueling a need for 3-D technology on the desktop.
The rise of a graphics-intensive game market spurred developers
to bring out several 3-D application programming interfaces
(APIs) for the PC. With the door open, traditional 3-D apps
such as scientific visualization, CAD, modeling and rendering
hopped on the PC 3-D bandwagon.

The growing momentum is bringing new applications to the fore,
including 3-D viewers and editors for the World Wide Web. Low
cost 3-D goggles are emerging for use with computer games.
And even virtual reality is beginning to sound less like a
virtual fantasy.

The pressure is forcing PC makers to pick a 3-D solution today
for systems they will offer next Christmas. These will need
to evolve gracefully into 1997, when an industry-standard 3
D approach may finally become clear. But while the market drivers
for 3-D are locked in fast-forward, the technical details of
executing a clean implementation on today's PC architecture
are stuck in slo-mo. The first problem is defining what, exactly,
3-D is.

The feature sets being offered in current and upcoming 3-D chips
are very diverse, and there is considerable controversy among
games developers, API suppliers, analysts and controller manufacturers
as to which should be required and which optional. Indeed,
when people bandy about the term "3-D" with respect to hardware,
they're often not talking about the same basic features. The
Direct3D API for Windows 95 that Microsoft is expected to ship
in April could become a major market accelerator, and definer,
of what 3-D is on the PC.

The pace of competition among chip makers is another factor.
If all the declared and undeclared chip makers enter the market
as forecast, the flood of product offerings could cause today's
average selling prices to plummet. As volumes rise, startup
and fabless chip providers could become non-starters if they
don't have manufacturing capacity lined up. There is no spare
fab capacity this year.

Generally, the top four features that make a chip 3-D are Z
buffering, color interpolation, texture mapping and rendering
of 3-D triangles and polygons. These basic tasks can be further
enhanced with various texture-mapping options and special rendering
features. Of course, some 3-D chips don't even offer all four
of the basics.

Z buffering is the baseline. A 2-D graphics frame buffer has
an X dimension and a Y dimension. In a 3-D world, some drawing
occurs in front of and behind the existing 2-D graphics via
a third, or Z, dimension that provides a (roughly) real-time
coordinate comparator for each pixel in the frame buffer. When
a 3-D object is drawn, each new pixel's Z value is compared
against the Z value of the previous pixel. If the values vary,
the new pixel is written from a Z buffer to the regular graphics
frame buffer.

The depth of the frame buffer determines how much 3-D accuracy
a given rendering will have and how much the Z coordinate must
be scaled to fit within the Z buffer. Typically, anything less
than a 16-bit Z buffer will produce unsatisfactory visual results,
but anything more is required only by high-end CAD and visualization
software. The Z buffer also needs significant bandwidth to
its internal memory if it is to support animation.

To create realistic lighting and shading of 3-D objects, color
interpolation is mandatory. In 2-D drawing, objects are flat
shaded, meaning that each object is a single color. Color interpolation,
also known as Gouraud shading, goes the next step, allowing
for a separate color at each vertex of an object. This provides
for more realistic lighting and on-the-fly shading of each
pixel.

A 3-D hardware accelerator doesn't do much without some kind
of 3-D building blocks, or primitives, to accelerate. The simplest
primitives are spans, which are horizontal sequences of 3-D
pixels. Some older 3-D devices only support spans, requiring
software packages to break down their polygons into these simple
units.

Beyond spans, the next step up is the triangle, then either
quads (polygons with four vertices) or tri-strips (triangle
strips). Most up-and-coming 3-D chips offer triangles. An exception
is the NV1 media-accelerator from startup NVidia Corp. of Sunnyvale,
Calif., which supports only quads.

After drawing Gouraud-shaded objects, the hardware's next task
is to apply a texture to a polygon. A texture can be a picture,
a background or a pattern-basically, anything that can be stored
digitally. Some of the more capable 3-D chips even support
a "texture" that consists of real-time digital video.

In a game such as Doom, all the walls, ceilings and floors are
texture mapped. The technique allows for the simulation of
a lot of detail in a scene without requiring lots of polygons.
For example, looking down an endless corridor would require
only four polygons-one for each wall, two more for the ceiling
and floor-assuming they all had the proper textures mapped
onto them. In the simplest form of texturing mapping, this
corridor would probably be rendered rather poorly. The illusion
of a corridor depends on perspective, but the texture would
be mapped orthogonally. That means faraway points-the most
distant bricks in a brick wall, say-would appear to be the
same size as those nearer the viewer.

To combat this problem, many texture mappers attempt some form
of texture scaling or reduction, leading to several methods
of making sure textures aren't significantly distorted by the
mapping process. The most basic is called mip-mapping (from
the Latin multium in parvo, or "many things in a small place"),
a technique that requires several versions of a texture, each
a different scale. The next step up is bilinear mapping, which
uses a blend of the texels (texture-map pixels) of two mip
maps when an match is available. Trilinear texture mapping
is more sophisticated, requiring three reads and one write
to load a pixel.

As if that weren't enough, some manufacturers add a couple more
texture-mapping features. The first is perspective correction,
which visually fixes the brick-wall-corridor mapping problem,
and is usually combined with bilinear or trilinear interpolation.
Also, some devices provide for "lit" texture mapping, allowing
parts of a texture to be brightened or dimmed as it's mapped,
based on theoretical light sources that may exist in a scene.

Every 3-D chip or software developer wants some kind of market
differentiator. Thus, under the umbrella of 3-D graphics come
a host of other advanced features, including visual effects
such as fog and stipple masks, as well as anti-aliasing. One
semiconductor company, S-MOS Systems, based in San Jose, Calif.,
even offers a chip that serves as a geometry processor, meaning
it can potentially offload the transformation, display list
and lighting calculations that software normally performs on
a computer's host processor.

Except in some early chips-such as the Glint controller from
3Dlabs Inc. of San Jose-these 3-D features aren't all typically
bundled in a single product. The trend is to package them with
an accelerated VGA graphics controller or, even better, an
accelerated VGA chip with video-processing features-the so
called video-graphics controller. Here, digital video can combine
with 2-D and 3-D graphics acceleration for maximum visual impact.
But some chip vendors, such as NVidia and another startup,
Chromatic Research Inc., of Mountain View, aren't content to
leave it at that. They also add high-quality sound capabilities
to their chips.

While pretty much all of the chips considered 3-D-capable offer
Z buffering, color interpolation and some form of polygon drawing,
not all of them offer texture mapping or the texture-mapping
extensions that will be required by upcoming 3-D applications.
Nor do they all offer backward compatibility to VGA acceleration.
However, the convergence of these functions will be found in
all of the survivors in this highly competitive market.

With some sense of the terrain of features, the next question
is how chip makers implement these functions. Clearly, 3-D
on a PC is different from 3-D on a workstation. The main difference
is the division of labor, commonly known as load balancing.

On a PC, the CPU typically handles all of the geometry-related
operations. The rendering functions-that is, making the pictures
look nice-are typically done by the 3-D controller. But not
all 3-D controllers do all the rendering functions; some leave
a number of these tasks to the CPU. There are trade-offs involved
in balancing the functional loads between particular CPUs and
particular 3-D chips. A chip that offers excellent video capabilities
may not offer the most advanced rendering functions, and vice
versa.

There are also a variety of ways for a controller to handle
3-D processes internally. The classic pipeline approach handles
one function in generating one scene after another. An approach
called forward rendering can deliver extremely high pixel rates
because it does not require some memory-read operations. Yet
another approach, ray casting, enables lighting effects that
closely model shadows and search lights in the real world.
Some chips construct scenes out of triangle primitives, others
use quadrilaterals and still others rely on tri- or quad-strips.
All of these approaches have advantages and drawbacks. They
all strive for a cost trade-offs in hardware, measured primarily
in memory usage and type of memory needed.

No 3-D traveler's baggage is complete without a strategy for
advanced memory technologies. For instance, some 3-D chip designs
do not use a Z buffer, while others claim to need 1 Mbyte.
Some use low-cost EDO (extended-data-out) DRAM, while others
use higher-performance memories, such as Rambus, video RAM,
window RAM and so on. The type and amount of memory have a
major impact on the cost of a total 3-D solution.

One major implementation issue among 3-D controllers is the
level of integration: of primary functions such as video control,
accelerated VGA, rendering and digital/analog conversion; and
ancillary functions such as audio and interfaces for graphics
oriented input devices such as joysticks, trackballs and the
like. Here, generally, more is less. Some solutions come in
three, four and, in one case, five chips-not counting memory.
Higher integration reduces board space, power draw and assembly,
which saves on costs.

Beyond feature sets and implementation strategies, designers
must consider their target applications. This isn't as straightforward
as it seems. Even though PC makers building in 3-D in 1996
are all doing it for gaming applications, there are several
distinct classes of games, and their demands on 3-D capabilities
vary considerably.

A fast-paced action game such as Doom does not need the highest
quality scene rendering, for example, but it does need fast
screen updates and a high frame rate. On the other hand, interactive
role-playing movies such as Silent Steel don't need any 3-D
rendering capabilities at all but do need the highest frame
rate and excellent video-handling features such as color conversion
and scaling. Further, adventure-mystery games such as 7th Guest
and Myst don't need either strong 3-D capability or high frame
rate, but they do need the greatest resolution and color range.

Chip designers are aware of this diversity and try to pack as
many of these functions as possible into their controllers
to accommodate the full gamut of apps. Meanwhile, developers
are innovating a whole new generation of games.

The 3-D controllers available today can roughly be grouped into
four categories: high-end, midrange, mainstream and low-end.
Examples of super-high-end implementations are the Glint chip
from 3Dlabs, which kick-started the 3-D processor craze; the
RSD/100 chip set that Lockheed-Martin tailored for arcade games;
the I128-2 from Number Nine Visual Technology of Lexington,
Mass.; and the SPC1500 from S-MOS. These tend to have the highest
performance, the most features and-at typically more than $70
the biggest price tags. They also tend to sport the largest
packages or use the most chips to create a design. However,
these are the chips that come closest to delivering real workstation
class 3-D performance. Often chips in this category will not
be VGA compatible or will offer only minimal VGA-boot capability.

A step down from these products is a class of 3-D controllers
in the midrange. These high-performance chips generally are
not totally integrated solutions and carry price tags higher
than what's appropriate for mainstream silicon. Among them
are the SST-1 from 3Dfx, a Mountain View startup; the Permedia
from 3Dlabs; the Mondello from Cirrus Logic Inc., the Fremont,
Calif., chip maker; and products from Artist Graphics, Brooktree
Corp. and Matrox. Like their high-end counterparts, these chips
are often not VGA compatible or have only minimal VGA boot
up capability.

The much sought-after sweet spot of the market is the mainstream
entertainment portion. To be a contender in this segment, a
chip should be a fully or nearly integrated 3-D video-graphics
controller. It should come in an inexpensive (i.e., plastic),
"normal"-size package and tote a price tag of less than $35.
Companies that are, say they are or think they are in this
category include ATI Technology Inc., based in Thornhill, Ontario,
Cirrus Logic, NVidia and Samsung Semiconductor Inc., based
in San Jose.

NEC and VideoLogic will debut mainstream 3-D controllers in
March. S3 Inc., based in Santa Clara, Calif., offers its Verge
chip for this category. Other players include startup Rendition,
based in Mountain View, with its Verite chip set, and Trident
Microsystems Inc., also based in Mountain View, with its T3
D9692. Matrox may push into this segment later this year.

A few chips offer some, but not all, of the functions required
in a 3-D controller. They include the T3-D2000 from Trident,
the ET6000 from Tseng Labs Inc. of Newtown, Pa., and the YGV611
612 from Yamaha, based in San Jose.

Whatever the category, most 3-D chips will ultimately converge
in the $20 to $30 price range and offer full texture mapping
and support for accelerated VGA and video graphics, with an
integrated LUT-DAC-lookup table/digital-to-analog converter
all in one device. Even as 3-D chip makers move toward such
products by integrating graphics features, graphics-chip makers
will converge on the same point by building in 3-D functions.

At the end of the day, the PC developer is left with a complex
matrix of the particular needs of particular games, which must
be matched up against the features and implementation styles
adopted by an increasingly wide range of 3-D controllers and
APIs. What's worse, there's no agreement among controller chip
vendors on how to measure performance other than by their own
particular test procedures.

By the end of 1997, all mainstream graphics chips will include
full 3-D acceleration, and 3-D hardware support will become
a standard feature of systems and add-in boards. Just what
specific features standard hardware acceleration will provide,
how vendors will implement them and which vendors will emerge
as the leading suppliers remain to be seen. The answers lie
along the crooked road that leads into the third dimension.

Jon Peddie, president of Jon Peddie Associates in Tiburon, Calif.,
is the publisher of the PC Graphics Report. He can be reached
at (415) 435-1775 or via e-mail at jon@jpa-pcgr.com.




<A HREF ="#HeadList">Back to Headline List</A><BR>
<HR>
<A NAME = "23"></A><IMG SRC="/pubgifs/ec109.gif"><BR><BR>
Copyright
OEM MAGAZINE via NewsNet
February 1, 1996  Vol. 4 No. 

DVD -- Driving the Vidoe Disk

By:
Rebecca Day

Open just about any Business 101 textbook and you're sure to
read about Sony's Beta VCR format in some chapter on business
don'ts. The second and third editions might also touch on that
company's other big flop, recordable MiniDiscs, and perhaps
discuss the case of Apple Computer, which proved during the
1980's and '90s just how much market share you can lose with
a closed architecture. In the end, of course, the widely supported
VHS VCR format won consumers' hearts, minds and pocketbooks,
the compact disk became ubiquitous and IBM PC-compatibles sold
like semiconductors. The lesson: MultiFinder standards work.
This lesson has not been lost on the two rival camps scrambling
to bring to market a next-generation, high-density optical
disk that could, in one fell swoop, replace the laser disk,
videotape, CD-Rooms and audio CDs. Taking electronics history
to heart, Sony/Philips and Time Warner/Toshiba decided late
last year that instead of dividing the market, they will join
forces, creating a standard next-generation disk that will
play in any digital-videodisk (DVD) machine, whether it be
DVD-ROM, DVD video, DVD audio or DVD interactive. Moreover,
all these formats will be backward-compatible with previous
versions of the CD standard.

Such a DVD standard will surely fly-but how far, and how fast?
Will the nascent technology simply emerge as high-density replacement
products for the videotape, compact disk and CD-ROM? Or will
DVD realize its potential as a true point of convergence between
the personal-computer and consumer-electronics industries?

The consensus is that at least in its first iterations, DVD
is likely to appear as souped-up laser videos on the consumer
side and as capacious storage media on the computer end. Both
will offer significant improvements over mature media that
are bumping against technological ceilings. But beyond that,
all bets are off.

"Will computer companies learn how to design and sell the right
product?" asks Carl Stork, director of the Windows PC platform
team at Microsoft Corp., which is taking a wait-and-see stance
toward DVD. "Will consumer-electronics companies learn how
to play in the PC market? I don't know which is the harder
question."

Innovative software will be a key ingredient to fully exploit
DVD. "The DVD standard is a tool kit," says Robert van Eijk,
director of CD-recordable products at Philips Electronics,
the San Jose, Calif.-based arm of the Dutch consumer giant.
"The vehicle may be exciting, but we need to fill it with content
that's compelling to consumers."

Indeed, the opportunities afforded by the new high-density disk
are enormous: memory capacity of 4.7 Gbytes/side-enough for
133 minutes of movies on a side-plus variable-speed data transfer
at an average 4.69 Mbytes/second for image and sound, MPEG
2 digital video compression, multiple audio channels and a
file-management structure that's compatible with existing CD
ROM platforms.

On top of all that, disks cost 80 cents or so a pop, which makes
the aluminum platter "cheaper than a book," says Guy Johnson,
vice president of video-product management at Thomson Consumer
Electronics. "You can put lots of stuff on it, you can make
the medium cheap and you can still make money at it. You're
going to see a lot of people get into this as a way to get
information to customers."

Having pushed CD-ROM technology pretty much to its limits, the
PC industry is looking to DVD for a speed and density boost.
"DVD is going to be great for the computer world," says Alex
Daly, vice president of marketing at C-Cube Microsystems Inc.,
the San Jose-based maker of an MPEG-2/Dolby AC-3 audio/video
chip that will serve as the engine for DVD. "We've gone from
2x to 4x to 6x CD-ROM, and this becomes something like a 12x
CD-ROM, with 4 Gbytes or more capacity."

Technical specs, though, are "a little divorced from content,"
Daly warns. "You're going to see the same old content you use
today-Excel, Word, your organizer, whatever-sitting, perhaps,
on a DVD drive. Or maybe you go out and buy a more robust version
of Encarta [the Microsoft encyclopedia] on a DVD disk that
has more multimedia."

CD-ROM has maxed out after 10 years on the market, says Julie
Schwerin, president of InfoTech Inc., the Woodstock, Vt.-based
market-research firm. "When you start spinning a disk at speeds
six times faster than intended, you expose the disk to conditions
that require it to be manufactured to much higher tolerances.
If the disk physically performs near the limitations of the
spec, then the error rate goes up, requiring error-correction
circuitry to break in, which brings the disk speed back down
to 4x speed. DVD is just a bigger CD-ROM," Schwerin says. "And
that's good, because it means the DVD technology will have
a long life."

Consumer-electronics manufacturers, for their part, see DVD
as a magnet to pull consumers into audio/video stores. "What's
driving the consumer-electronics business right now is home
theater," says Craig Eggers, marketing manager for new products
at Toshiba America Electronics Corp., based in San Jose. "And
what home theater is about is reproducing the director's vision
as faithfully as possible. DVD comes as close to that as any
other product ever introduced. DVD is the ultimate home-theater
machine," he exults.

The advantages of the new high-density medium are easily measurable
for the consumer. Full-length movies can squeeze onto a CD
size disk along with five full-bandwidth channels of audio
in several languages and with subtitles. Digital video enables
both random access of scenes, which isn't feasible with sequential
access videotape, and the placement of supplemental information
on a disk, which isn't possible with the outdated laser disk.

Further, DVD's MPEG-2 video compression makes room for both
the 4:3 aspect-ratio pan-and-scan formats used by the current
NTSC broadcast system and the wide-screen formats that more
closely approximate the cinematic experience of a movie theater.
"The audio used in DVD comes as close to the best theaters
you've been in, and the video quality is capable of being as
good as the D-1 masters that all software is sourced from,"
Eggers says.

It all sounds great, but will consumers bite? And at what price?
Most suppliers can't afford to be anything but optimistic.
CD growth has slowed, VCR diffusion has topped out and the
laser disk is a 15-year-old format that has failed to excite
more than about 2 million video enthusiasts.

Eggers points to a $200 million multivendor awareness campaign
that will put the letters "DVD" in front of consumers' eyes
countless times before any product even ships to stores late
this year. A subsequent $200 million campaign will follow.
Time Warner vows that at least 250 software titles will be
available at DVD's launch, and RCA Corp. has boldly promised
to deliver mainstream-priced $500 players to stores by fall.
RCA predicts that DVD will post first-year sales figures rivaling
those of the company's Digital Satellite System (DSS)

Schwerin of InfoTech, on the other hand, believes market adoption
will be driven by the PC side. She predicts that worldwide
sales of DVD-ROM players will top 2 million units in 1997,
outpacing DVD movie players by two to one. As a movie medium,
"DVD faces the additional challenge of carving out a new place
in the home-entertainment center with consumers who have heretofore
strongly favored recordable VCRs over superior-quality analog
laser videodisk," Schwerin says. InfoTech sees the DVD-ROM
as the favored upgrade for PC-hardware manufacturers beyond
6x CD-ROM drives, thanks largely to greater density and more
efficient channel modulation.

Johnson at Thomson Consumer Electronics, the Indianapolis-based
parent of consumer giants RCA and General Electric, also expects
recordable DVD-ROM products to hit the market in 1997, ahead
of consumer DVD. Movie versions, he predicts, will be held
up, partly on account of resistance from Hollywood, which fears
pirating of recordable digital titles.

Johnson says the recordable DVD for computer applications will
have 2.6 Gbytes of memory, compared with the 4.7 Gbytes required
for a movie disk. "People will use recordable DVD for all kinds
of things they use recordable disks for now: CD-E[rasable]
or CD-R[ecordable]," he says. "This will replace all those
formats."

But simply replacing the CD-ROM and the VCR with beefier machines
may not hold much promise for the mainstream market. In fact,
some say that such a minimalist approach to DVD would demonstrate
both a waste of immense technological resources and a lack
of creativity on the part of the converging industries.

Rather, the key to DVD's scoring a big success is novel applications,
says van Eijk of Philips Electronics. "Just generating a high
density format for the sake of more capacity doesn't do much."
The burden falls to content providers, says van Eijk, who expects
DVD content to evolve the same way CD-ROM content did.

"In the early days of CD-ROM, content providers took existing
material like databases and transported it to CD," he says.
"But what was the compelling reason for consumers to switch?"
It was only when the CD-ROM imported motion, graphics and video
that it became exciting, van Eijk contends. Finally, more creative
implementations, such as hypertext and hyperlink, took CD-ROM
where consumers had never been before, integrating features
that took full advantage of the new format.

Many would like to likewise push the DVD envelope to its creative
limits. Rather than using it to, say, add more video and sound
clips to an existing product-for example, Microsoft's Cinemania,
a CD-ROM about movies and movie history-van Eijk pictures a
DVD movie disk that includes movie reviews and advertising.
With such a disk, consumers might have to sit through a trailer
before being able to access a review. "It's a horrible thing
to say," he adds, "likening a DVD-ROM to a commercial."

Van Eijk also sees great opportunity for user participation
in a movie DVD. "It's a very interesting idea to combine movies
with games," he says. "You could put, say, a Terminator 2 game
on the same disk as the movie or its soundtrack." That kind
of scheme could enhance revenues for movie-rental stores, he
says, winning converts at your local Blockbuster. "You might
have to unlock the game with a code, for which users would
somehow be charged."

Thomson's Johnson also sees great commercial possibilities in
DVD. He envisions content-related shopping on a disk, with
viewers, for example, punching in a code to buy a leather jacket
from a catalogue that resides on the same disk as a Clint Eastwood
movie. "Or you may take that disk and put it on your computer
and do the shopping from the computer," he speculates. "You
could also have a travel-destination disk for $1 or $2. Where
you want to watch that disk I don't know. The office? The couch?
It will play in our players and TV/DVD combination units, and
on somebody else's game machine. The way people perceive what's
going to be on a disk today is going to be much different than
what this disk has the potential to be."

The issue of whether DVD will be driven by the computer or the
consumer-electronics industry may be a moot point as suppliers
in each industry edge closer to the other side. Products like
Toshiba's TIMM, a multimedia TV and PC monitor, and the PC
world's plug-and-play initiative indicate rapid and inevitable
convergence.

Last fall, Thomson showed a prototype of a 35-inch TV with SVGA
resolution called Genius Theatre, described as a home-theater
system with the entertainment features of a PC-complete with
a wireless keyboard, a full array of surround-sound electronics
and speakers, and a six-disk CD-ROM changer. With the TV serving
as a gateway to the Internet, Johnson says, a DVD would be
a great host for a Web browser. He notes that much of what
comes off the Internet is not real-time information, so it
could easily be downloaded onto some sort of recordable DVD
for access later.

Downloading of information from Net to disk won't happen in
a vacuum, of course. The pipeline into the home will have to
widen in order to move the vast amount of data required for
high-quality video and audio information. Daly of C-Cube Microsystems
sees the cable modem as the enabler here. The last thing a
consumer wants is yet another box near the entertainment center,
Daly says. Instead, he foresees a combination set-top box that
bundles audio, Web browser, video-compression decoding and
modem.

"By 1997, you're going to start seeing multifunction platforms
emerge," Daly says. "You'll be able to play back a DVD, receive
a DSS signal and even surf the Net. You have to do MPEG for
a set-top box to receive digital cable or satellite signals
and for a DVD movie player. It would be more efficient to use
a single component at the center of that and create the periphery
around it."

As hardware designers tout DVD's vast capabilities, the software
industry is sniffing around the technology as well. Stork at
Microsoft says it's too early to make firm commitments about
DVD. "When CD-ROM drives first came out, they were more than
$1,000," Stork recalls. "It took about six years until any
significant amount of CD-ROM software became available. If
these [DVD] drives are inexpensive early, it will go to market
more quickly and encourage more titles more quickly." But if
drives are high-priced, the software will serve more specialized
markets, Stork predicts.

"Early on, CD-ROMs were only used for reference materials and
specialized applications," he says. "It wasn't until drive
prices got into the couple-of-hundred-dollar range that their
consumer usage flourished. I would expect to see a similar
kind of development here."

Ultimately, Stork believes DVD invites a new kind of player
into the home. "I think there's a new product opportunity here
that doesn't exist today but could in two to three years,"
he says. Such a player would have a display about 30 inches
or larger for movies, the processing power of a PC for games
and a TV tuner. "It could be the device you want in your family
room that you use to watch broadcast TV or play videogames
and watch DVDs," Stork says. "You may not do much computing
in the sense that we do computing on PCs today at all; you
may just be using it as an entertainment device."

Will this convergence product come from the computer world,
the consumer-electronics world-neither of which has much experience
in the other's domain-or a new industry altogether? Or perhaps
a single convergence product isn't necessary at all. Consumers
might want several DVD machines, in different flavors: one
for the office, for example, one for the family room and a
portable model for everywhere else.

"In the Far East, a lot is being done with convergence products
around CD-ROM," says Philips's van Eijk. "But in the United
States, we tend to buy multiple versions of the same thing.
In reality, people buy three or four CD players for different
purposes. [In that sense,] I don't believe DVD will be a convergence
product."

Computer and consumer companies have shaken hands and now they're
all heading for their own corners. The battle for consumers'
dollars has begun. But while vendors grapple with product issues,
there are still a number of real-world questions that need
to be answered.

"How do we package this, how do we describe this, how do we
sell this, how do we get people to buy this?" asks Microsoft's
Stork. "These questions are probably as hard as the technical
issues. It's a product nobody's asking for. Can [marketers]
come up with something that's compelling? To us guys that build
product, we think they ought to be able to, but they've got
a job to do."

Rebecca Day is a freelance writer based in Chestnut Ridge, N.Y.




<A HREF ="#HeadList">Back to Headline List</A><BR>
<HR>
<A NAME = "24"></A><IMG SRC="/pubgifs/ec109.gif"><BR><BR>
Copyright
OEM MAGAZINE via NewsNet
February 1, 1996  Vol. 4 No. 

Will Hollywood back DVD as a censor?

By:
Rebecca Day

The final specs for the videodisk and PC-ROM versions of DVD
were barely dry when Miami-based startup Nissim Corp. announced
the first potential application for the fledgling technology.
In an early-December press release, CEO Max Abecassis said
Nissim will build into its DVD players a "variable-content"
capability that includes a motion-picture rating technology.
Called Control, the feature will enable viewers to screen the
content of the video, blocking children from viewing violent
or sexual content.

Hardware suppliers have touted parental-control capability as
a selling point for DVD as a movie system. With Nissim's version,
viewers could select a rating, such as PG-13, and program the
player to skip scenes deemed beyond that level. Each disk would
have to be encoded with the appropriate information, and Hollywood
had not commented on the system at press time. Abecassis is
hoping public pressure will persuade the studios to consider
it. "I'm hoping to inform the public about the technology,"
he says, "and through their reaction, elicit greater support
than I would ordinarily get from studios."

The Control software, which requires about 100 kbytes of memory,
is the first to look at a movie as a "videobase" of individually
addressable segments, Abecassis says. The exact digital location
of every segment in a DVD is identified and assigned a content
category and level of explicitness. The resulting data, referred
to as a "variable content map," sends information to the player
that can be used to automatically generate a playlist of only
acceptable segments. "The essence of a DVD is random access,"
Abecassis says, "and this system exploits that capability like
no other technology available."

The concept would be one way for Hollywood to police itself,
thereby staving off attempts at censorship by government or
parental groups. At the same time, however, it gives users
control over a film's creative content, which is sure to draw
resistance from the artistic community.

"Say you took out 'Schindler's List' on DVD and wanted to watch
a PG version of it," says Guy Johnson, vice president of video
product management at Thomson Consumer Electronics. "Steven
Spielberg's going to say, 'B.S. You're not going to take out
my scenes that I've worked on and mess with my creative content.'
Hollywood has to go through some refining of how they're going
to handle this issue. I know for sure that a lot of directors
aren't going to want you to decide the rating level of a disk."




<A HREF ="#HeadList">Back to Headline List</A><BR>
<HR>
<A NAME = "25"></A><IMG SRC="/pubgifs/ec109.gif"><BR><BR>
Copyright
OEM MAGAZINE via NewsNet
February 1, 1996  Vol. 4 No. 

Wireless LANs -- Not of this World

By:
Barry Phillips

Just two years ago, companies selling wireless local-area networks
were ready to shoot the moon. Advocates predicted broad deployment
of untethered links in the corporate world by the end of the
decade. Their evidence: a nearly ready IEEE standard and a
wealth of new products any MIS Manager might find compelling
for the office of tomorrow. But today, it looks as though wireless
LAN technology might be lost in space. The IEEE's 802.11 standard
is still "nearly ready." Insiders say it could take until the
end of 1996 to fully nail it down. And though later this year
vendors expect to sell wireless PC-card LANs that poke along
at 1 or 2 Mbits/second for, perhaps, $350, desktop PC cards
for 100-Mbit/s wired LANs have already dropped to $170. Which
option do you think that forward-thinking MIS managers will
choose? "It's not a very compelling business proposition to
say we may be more expensive, but we're slower," quips Mack
Sullivan, director of marketing communications at Mountain
View, Calif.-based Proxim Inc., a supplier of wireless-LAN
chips and boards.

Apparently sensing a lost cause, National Semiconductor Corp.
disbanded its wireless-LAN semiconductor efforts in early 1994.
Motorola Inc. curtailed its high-profile push into wireless
LANs last September, when its licensed 26-GHz Altair system
died a quiet death after failing to generate much heat in the
marketplace. And startup companies pursuing breakthrough 10
 and 20-Mbit/s wireless LANs have met with less-than-stellar
results so far.

Instead of the broad deployment advocates predicted, wireless
LANs are making their greatest inroads in niche applications
in the factory, medical center and warehouse. In car-rental
return, inventory-management and point-of-sale niches, for
example, users have an overriding need for cordless data connectivity
and are willing to pay a premium for it.

"The biggest opportunity for us is where desktops don't exist
today, and there is a need for handheld systems," says Alan
Soucy, vice president of personal systems for Zenith Data Systems
(ZDS). That's a switch. When the Buffalo Grove, Ill.-based
company launched its CruisePad handheld terminal in June 1994,
Soucy was targeting "corridor cruisers," executives who spend
time away from their desks and who would presumably welcome
wireless terminals for tapping into the corporate LAN. CruisePad
gives roving users a window into their desktops via a built
in wireless-LAN module, co-designed with Proxim, that uses
2.4-GHz spread-spectrum technology.

But by and large, those nomadic execs failed to bite. Soucy
claims that today, "a few thousand" CruisePads are in use,
mainly beta units linking to Windows NT servers via redesigned
software. "In the corporate market, there has been a significant
investment in the current [wired] infrastructure, and a lot
of offices have performance-related requirements that are hard
to break out of," he says. Soucy now tags vertical markets
in hospital and industrial settings as the prime arenas for
CruisePad sales.

"If we are looking at replacing wired LANs in offices, that's
just not going to happen," concurs Todd Smith, director of
product marketing for Aironet Wireless Communications, based
in Fairlawn, Ohio. "We [at Aironet] don't walk around the office
carrying this stuff-and if we don't do it, who will?" Nor does
Smith see wireless LANs becoming more attractive to the corporate
mainstream any time soon. "Expenses for installing a wireless
LAN are front loaded," he says. "To get the first guy on-line,
it's going to cost a lot of money for the coverage you need.
Wireless LANs won't be a replacement, but a way to augment
wired LANs in places where that makes sense. So for the next
couple of years, the business will all be in vertical [markets]."

NICE NICHES

That assessment is quantified by Andy Prophet at AP Research,
a Cupertino, Calif.-based firm that tracks the PCMCIA market.
"The current market for wireless LANs is just under 1 million
PC cards in 1996, growing to 2.4 million cards by 1999," Prophet
says. "Most of today's applications are in vertical markets,
and I expect it to remain that way."

But small niches can spell big sales, and companies like Aironet
are thriving in vertical markets. Smith claims that since its
inception in 1991, his company has sold 300,000 radios for
wireless LANs. A third of them went out the door in the last
12 months, the vast majority to parent company Telxon Corp.,
based in Rosemont, Ill., which makes scanners and ruggedized
computers for vertical markets.

Proxim's business, too, is heavily skewed to selling OEM modules
to some 50 companies-such as Fujitsu Personal Systems Inc.,
based in Santa Clara, Calif.-that integrate the parts into
vertical solutions. Fujitsu is selling a wireless-ready 486
based portable called Stylistic 500 RF, which runs Microsoft's
Windows for Pen Computing.

Even though niches can be lucrative, many OEMs are still trying
to find a way into the mainstream corporate market, with its
promise of big bucks and long product life. According to BIS
Strategic Decisions, a market-research firm based in Norwell,
Mass., there are 15.2 million in-building travelers-the corridor
cruisers ZDS originally targeted-who could benefit from mobile
access to their corporate LANs. And yet, wireless-LAN sales
to the horizontal office segment are well under $100 million,
says Joe Baylock, an analyst at the Gartner Group Inc., based
in Stamford, Conn.

To broaden product appeal and tap into the corporate wellspring,
OEMs are scrambling to improve the price, performance and standards
picture for wireless LANs. Niche users typically pick a single
vendor's solution from the hodgepodge of proprietary, incompatible
systems operating within the 902-to-928-MHz industrial, scientific
and medical (ISM) band. In the office, by contrast, interoperability
of equipment is a paramount concern, and the dearth of wireless
LAN standards is an issue for both information-service managers
and end users.

To get into their servers, clients on a corporate enterprise
network need integration with existing Internet Protocol-based
networks and applications. They also want support for ad hoc
networking, which enables them to interconnect on a peer-to
peer basis. But the wireless-LAN industry has plodded along
for years with no standards for media-access control (MAC)
and physical-layer (PHY) interfaces.

Pending IEEE 802.11 standards for 2.4-GHz wireless LANs are
the key to broader deployment, observers say. That's because
many national and regional regulators-the Federal Communications
Commission and its counterparts around the world-have reserved
the 2.4-GHz frequency band for unlicensed use by wireless LANs.
Most wireless-LAN radio development has therefore shifted up
band from 900 MHz, the frequency used in earlier product generations.

The 802.11 committee hopes to complete its work in mid-1996,
but trudging the final mile to formal approval may take until
the end of the year. Whenever it's finalized, the standard
will help bring wireless LANs into the mainstream by reassuring
users that the technology is solid.

"The main thing the standard does is put a floor under the technology,"
says Craig Mathias, a principal at the Farpoint Group, a research
and product-consulting firm based in Ashland, Mass. "End users
won't necessarily buy wireless LANs because of the features
in the standard, namely interoperability, which I have some
doubts about," he says. "[But the standard] legitimizes the
technology."

Unfortunately for end users, however, the 802.11 standard will
institutionalize two incompatible radio-modulation schemes
that will be left to compete head on. In effect, customers
will choose between two types of "wire" for their mobile data
networks: direct-sequence spread-spectrum (DSSS) and frequency
hopping spread-spectrum (FHSS)

Though both systems offer bandwidth between 1 and 2 Mbits/s,
the DSSS camp-headed up by AT&T, maker of the WaveLAN product,
and radio-circuit supplier Harris Semiconductor-claims direct
sequence technology will have an easier time than FHSS taking
wireless LANs to higher speeds. Direct sequencing spreads the
signal by combining high-bit-rate, pseudo-random code with
the data being sent; several channels, all with their own pseudo
random code, share the spectrum.

With frequency-hopping technology, by contrast, discrete blocks
of data go out on a large number of frequencies. The radio
transmits bursts of information, and the receiver follows the
same sequence of hops to recover it. The FHSS camp comprises
the bulk of the wireless-LAN industry, including leading OEMs
such as Proxim and Xircom.

"We expect to see a shakeout in terms of the radio [approaches],"
says Roland Fournier, product-marketing engineer at Advanced
Micro Devices Inc., based in Sunnyvale, Calif. "There will
probably be one standard radio in a year."

The technical debate resembles the situation in digital cellular
and personal communications systems, where the industry is
split between two high-frequency radio-access methods: code
division multiple access and time-division multiple access.
There, the public network carrier makes the technology decision
for an entire geographical region; but with wireless LANs,
users must confront the dilemma themselves when they choose
which radio to buy. Not only are the two types incompatible,
but they interfere with each other, knocking out the other's
signals. Industry watchers say that placing a technology decision
on consumers' shoulders will confuse buyers and impede market
growth.

Meanwhile, uncertainty about the finality of the 802.11 standard
has chip and card vendors hedging their bets. Chip maker AMD,
for example, has opted to implement its MAC controller based
on a programmable part: the 80188 microcontroller. The resulting
PCnet-Mobile MAC controller can support FHSS, DSSS or infrared
interfaces. "If 802.11 got pushed out for any reason, we wouldn't
have to spin new silicon," says Fournier. "PCnet-Mobile can
be reprogrammed to reflect any changes."

For its part, Proxim hopes to challenge an expected rush of
initial standards-compliant products running at the 1-Mbit
s IEEE spec with a proprietary 1.6-Mbit/s product. "We think
[standards-based products] could be dead on arrival," Sullivan
says. "The 802.11 group is basically building a product by
committee. How many compromises have the 15 vendors on the
committee made along the way?"

"Within the 802.11 standard, there are a lot of options," says
Baylock of the Gartner Group. "This is a key time for the formation
of coalitions among OEMs. Certain groups of vendors will choose
to work together and select a subset among those options that
they all can build to. We'll see implementation agreements
within the 802.11 framework."

Fournier of AMD predicts that updatable 802.11 network-interface
cards will emerge in mid-1996, much as updatable V.34 modems
appeared before that standard was formally ratified. Designers
will need to add flash memory, at a cost of about $10 per card,
until the standard finally solidifies.

So far, one manufacturer of 2.4-GHz transceivers for the wireless
LAN market has bought into AMD's chip solution. Pulse Engineering
Inc., based in San Diego, plans to introduce a complete FHSS
based PC card that will sell in OEM quantities for $169. AMD
is also supplying drop-in MAC silicon that works with the new
Prism chip set from Harris, based in Palm Bay, Fla., a highly
integrated four-chip RF subsystem for wireless-LAN DSSS radio.

Prism is the first off-the-shelf DSSS radio solution aimed at
low-power Type II PC-card designs. A design still requires
two external gallium-arsenide power-amplifier ICs to drive
an antenna, but Harris is working to integrate these functions
into silicon to further reduce power consumption. Besides mopping
up more than a dozen ICs, the DSSS radio operates at up to
4 Mbits/s-twice the rate of the fastest 802.11 mode.

Samsung has adopted the Prism chip set as the basis of a 3-V,
2-Mbit/s Type-II PC card for mobile computers announced late
last year. Aironet has also said it will use Prism for its
next-generation wireless-LAN products.

The availability of standard, off-the-shelf MAC silicon-expected
from multiple vendors in 1997-will have a beneficial impact
on the cost, complexity and time-to-market of wireless-LAN
products. However, having two different radio standards makes
it harder for the industry to consolidate behind a single PC
card design or access-point architecture. Once a single radio
standard does shake out, perhaps by 1998, the market may expand.
As volumes increase, AMD predicts that the combined cost of
a wireless-LAN card and a port on a wireless access point will
drop from $330 in 1996 to $250 in 1998. By then, of course,
a 100-Mbit/s Ethernet card could dip to below $100.

The access point itself could become a fertile area for OEMs
to differentiate and add value, says Bruce Sanguinetti, executive
vice president of sales at BreezeCom Wireless Communications
Inc., based in Carlsbad, Calif. He predicts that access points
the bridge to the wired network-will keep getting smarter,
improving wireless-LAN functionality.

BreezeCom's access points, based on a multicell wireless switching
configuration, support up to five users operating at 3 Mbits
s, or a total of 15-Mbits/s of aggregate bandwidth. The company
(formerly called Lannair) is sufficiently convinced of the
stability of the 802.11 standard to already market wireless
adapters and access points as 802.11 D2-compatible.

BreezeCom is not the only wireless vendor to have ventured beyond
the standard's 2-Mbit/s frontier. But the battle for bandwidth
is not without its casualties. One high-profile startup, RadioLAN
Inc. of San Jose, Calif., failed to hit a promised ship date
last September for a 10-Mbit/s wireless LAN, despite having
won some big venture-capital bucks and a best-of-show award
at Interop in the spring of 1995. A second startup, WiLAN,
has been aiming at an even loftier goal: 20 Mbits/s; but the
Canadian company has shipped nothing to date.

The IEEE itself is talking about a 10-Mbit/s 802.11 follow-on,
and the European Telecommunications Standards Institute is
working on a 20-Mbit/s version, HiperLAN. Whether such standards
efforts or a startup's proprietary push will make the bandwidth
breakthrough for wireless LANs is still anybody's call. "Our
goal is to get to 10 Mbits/s, but we don't know how long it's
going to take," says Smith of Aironet.

According to Dave Bagby, a lead design engineer with AMD who
sits on the IEEE 802.11 committee, the 1- to 2-Mbit/s speed
limit on most of today's wireless LANs is a result of the limited
bandwidth allocated by the FCC for unlicensed use. "The current
802.11 PHYs had to account for FCC regulatory restrictions
in their design," he says. "If they allocate more spectrum,
you could get a different answer. The issue is political, not
technical."

Several vendors who tinkered with wireless LANs in the 5-GHz
spectrum managed to attain data rates of 5 to 10 Mbits/s. Yet
FCC power-output restrictions limited their usefulness, and
most of the high-frequency projects were abandoned. According
to Michael Rothenberg, president of BreezeCom, the industry
is organizing to lobby for expanded spectrum allocations in
the 5-GHz range for unlicensed LAN operation. Widening the
frequency slots currently assigned in the 5-GHz range from
1 to 5 MHz is seen as the key to the next bandwidth breakthrough.

In the meantime, some vendors are focusing on bringing audio
and video applications to wireless LANs. While many argue that
multimedia support is an issue of raw data pipe size, special
protocol options in IEEE 802.11 enable wireless-LANs to reserve
bandwidth and set up circuit-style connections for the guaranteed
bandwidth multimedia traffic needs.

"The standard attempts to support time-bounded services," both
data and voice, says AMD's Bagby. "To this end, there is a
fair amount of complexity involved with the portion of the
draft that handles what we call the point-control function."
Digital Ocean, an OEM working in conjunction with AT&T, is
developing a wireless access point that implements point control,
a function that can allocate bandwidth to support mobile multimedia
transport applications. But given the status of wireless LANs
in general, the idea of an untethered multimedia link may be
a bit over the moon.

-- Rick Boyd-Merritt and Loring Wirbel contributed to this story.

Barry Phillips can be reached via e-mail at barryp@ico.com.




<A HREF ="#HeadList">Back to Headline List</A><BR>
<HR>
<A NAME = "26"></A><IMG SRC="/pubgifs/ec109.gif"><BR><BR>
Copyright
OEM MAGAZINE via NewsNet
February 1, 1996  Vol. 4 No. 

Infrared aims for the corporate LAN



Infrared (IR) data transmission may be moving beyond its niche
as a point-and-shoot medium for moving files between mobile
PCs and wire-networked computers and printers. Companies such
as IBM, Hewlett-Packard and Sharp are trying to leverage the
low costs and new fast speeds of IR transceivers to help wireless
LANs gain a foothold in the corporate market.

In April 1995, the three companies came together to support
a new 4-Mbit/s option to the existing InfraRed Data Association's
IrDA 1.1 spec. IBM's Microelectronics Division began offering
4-Mbit/s IR components and LAN drivers early in the year, and
Extended Systems incorporated them into an adapter board in
time for Comdex/Fall. The Boise, Idaho, company's JetEyeNet+,
connecting an IR-equipped notebook computer to a LAN, lists
for $425 and may sell for $295 on the street.

IBM believes the ability to link to the LAN at these prices
will drive IR into corporate networks. But lower prices and
higher speeds aside, IR networks still suffer from an inherent
limitation, offering only line-of-sight connectivity. Upping
the ante, NTT Data Communications Systems, a subsidiary of
Nippon Telephone and Telegraph, has developed a repeater and
PCMCIA card for an Ethernet-compatible, 10-Mbit/s IR LAN. The
catch: its range is limited to 1 meter.




<A HREF ="#HeadList">Back to Headline List</A><BR>
<HR>
<A NAME = "27"></A><IMG SRC="/pubgifs/ec109.gif"><BR><BR>
Copyright
OEM MAGAZINE via NewsNet
February 1, 1996  Vol. 4 No. 

Multiplexing modem



The Sportster modem from U.S. Robotics delivers the appealing
prospect of transmitting digital voice at the same time as
data or fax traffic. It complies with the Digital Simultaneous
Voice & Data modem spec. An 8-kHz voice channel supports real
time telephony between modem pairs while sustaining a 19,200
baud data or fax channel with compatible remote DSVD modem
systems. Call (800) Dial-USR.





<A HREF ="#HeadList">Back to Headline List</A><BR>
<HR>
<A NAME = "28"></A><IMG SRC="/pubgifs/ec109.gif"><BR><BR>
Copyright
OEM MAGAZINE via NewsNet
February 1, 1996  Vol. 4 No. 

Power designer



The McCAD integrated electronic design software from Vamp Inc.
speeds the path from concept to schematic capture and printed
circuit-board layout, even generating a bill of materials.
Modular software plug-ins allow it to add features at any stage
of the design process. The package is for the PowerPC processor
for Power Macintosh and Common Hardware Reference Platform
host systems. Call (213) 466-5533.





<A HREF ="#HeadList">Back to Headline List</A><BR>
<HR>
<A NAME = "29"></A><IMG SRC="/pubgifs/ec109.gif"><BR><BR>
Copyright
OEM MAGAZINE via NewsNet
February 1, 1996  Vol. 4 No. 

Soft sound synthesis



The Yamaha Soft-Synthesizer YSS100GM brings software-based wavetable
sound synthesis to a PC. The package permits the storage of
up to 1 Mbyte of wavetable sound samples in RAM, supporting
a synthetic symphony of up to 128 samples and one drum kit.
Features include a MIDI/karaoke function and user control panel.
A Pentium 75 or faster host is recommended. Call (714) 522
9330.





<A HREF ="#HeadList">Back to Headline List</A><BR>
<HR>
<A NAME = "30"></A><IMG SRC="/pubgifs/ec109.gif"><BR><BR>
Copyright
OEM MAGAZINE via NewsNet
February 1, 1996  Vol. 4 No. 

Fast Mac graphics



The industry's most powerful video card for PCI-based Power
Macintosh and other PowerPC-based hosts is available in the
form of Number Nine's Imagine 128 for PowerMac. Screen resolutions
to 1,600 x 1,200 are supported using 8 Mbytes of VRAM. The
board handles 8-, 16- or 32-bit graphics calls in a single
instruction, accelerating all Mac graphics functions. Call
(617) 674-0009





<A HREF ="#HeadList">Back to Headline List</A><BR>
<HR>
<A NAME = "31"></A><IMG SRC="/pubgifs/ec109.gif"><BR><BR>
Copyright
OEM MAGAZINE via NewsNet
February 1, 1996  Vol. 4 No. 

No smoke, just mirrors



The Digital Light Processing subsystem from Texas Instruments
uses TI's Digital Micromirror Devices to transform tricolor
reflected light into 640- x 480-pixel projection images for
RGB data or analog video display. This high-resolution module
is for rear-projection screens. The resulting display is free
of errors and aberrations that often plague alternatives. Call
(214) 995-2011





<A HREF ="#HeadList">Back to Headline List</A><BR>
<HR>
<A NAME = "32"></A><IMG SRC="/pubgifs/ec109.gif"><BR><BR>
Copyright
OEM MAGAZINE via NewsNet
February 1, 1996  Vol. 4 No. 

Sizing up JPEG



C-Cube's Edit Pro card lets users evaluate the effects of motion
JPEG compression by helping them rescale their source images
to obtain an optimum video data stream. A Genesis GM833X2 scaling
chip helps deliver JPEG imagery at maximum quality for C-Cube's
popular CL560 JPEG chip. This 32-bit PCI card handles 16-bit
YUV 4:2:2 source video at rates up to 60 fields per second.
Call (408) 944-6300.





<A HREF ="#HeadList">Back to Headline List</A><BR>
<HR>
<A NAME = "33"></A><IMG SRC="/pubgifs/ec109.gif"><BR><BR>
Copyright
OEM MAGAZINE via NewsNet
February 1, 1996  Vol. 4 No. 

Dream this: The Mainframe's Last Stand

By:
Portia Isaacson

Is the $500 network computer just a lot of noise, or is it the
inevitable replacement for the PC? I think it's the mainframe's
last stand. The $500 "NC" was the brainchild of Larry Ellison,
Oracle's outspoken president and chief executive officer. The
world's second largest software company, Oracle leads the enterprise
software market but has failed to make significant inroads
in PC software. Besides being renowned for its mainframe and
Unix software, Oracle is also known for testosterone overload
in the presence of Microsoft, its chief rival. Oracle's proposed
NC client would replace the PC for users who are always connected
to a server and who run a limited set of applications. The
NC differs from a PC in that applications processing, except
for GUI display and user-input handling, is done on the mainframe,
where all user data is also stored.

If the NC can take application-processor cycles and file-storage
megabytes away from PCs and put them on mainframe or Unix servers,
Oracle stands to gain market share at Microsoft's expense.
The company has unabashedly stated that one of the most appealing
features of the NC is that it uses no "Wintel" parts.

Mainframes have experienced amazing growth in recent years,
driven by the explosion of PCs that suck data off enterprise
servers. But the trend has been to shift applications processing
to client PCs while leaving the database on the mainframe,
where it can be secured, backed up and shared. Nonetheless,
PC applications processing is one step closer to distributed
object, information-bus software architectures, which enable
more choices in database location and implementation. Therefore,
even Oracle's database-software sales could be in jeopardy.
No wonder it wants to start a war to bring applications back
to the mainframe.

The main argument for Oracle's proposed NC is the Web, which
begs for a very low-cost Web-access terminal. Applications
are already being created for the Web or front ended with Web
interfaces, but Oracle would have us believe that Web applications
can replace PC applications. But most apps will not be done
on a Web terminal, because they are too tightly connected to
the user interface to be replaced by a mainframe app connected
to a terminal by a slow wire.

Which hardware manufacturer will take the risk to actually build
and market an NC? Given the specs and price tag, the ones that
will profit in the NC scenario are mainframe and Unix-server
vendors, which will sell more computers and disk drives to
support the helpless NCs they control. Perhaps we can expect
to see Sun and IBM start giving away NCs to help them sell
more big iron.

Portia Isaacson is president of Dream IT, Inc., a Boulder, Colo.,
based consultancy on emerging technologies and markets. She
can be reached at 303-417-9313 or portia@dreamit.com.




<A HREF ="#HeadList">Back to Headline List</A><BR>
<HR>
<A NAME = "34"></A><IMG SRC="/pubgifs/ec109.gif"><BR><BR>
Copyright
OEM MAGAZINE via NewsNet
February 1, 1996  Vol. 4 No. 

Rainbow room -- A company spun out of the Massachusetts Institute
of Technology has devised a unique way to generate multiple
colors in miniature flat-panel displays

By:
Sunny Bains

A slew of miniature flat-panel display chips now under development
pro-mise to enable a range of high-tech gadgets, from inexpensive
wrist-wearable computers and lightweight virtual-reality headsets
to sophisticated video pagers. Some minidisplay advocates further
project that future large-screen opportunities may not lie
in physically large displays but in high-resolution minidisplays
coupled with the appropriate optics to leverage them into TVs,
computer monitors and projection display systems.

With the minidisplays-many smaller than 1 inch in diagonal and
with resolutions as high as 1,000 dots/inch-pixel structures
and control electronics are built into a substrate using more
or less conventional semiconductor-fabrication techniques.
The substrate may be glass, quartz, ceramic or a silicon wafer;
the display medium may be a conventional twisted-nematic (TN)
LCD, a more exotic LCD such as a ferroelectric or polymer-dispersed
device, an electroluminescent display, a field-emission display
or a tiltable-mirror technology.

One problem minidisplay makers face is how to generate multiple
colors. A new approach to the multicolor dilemma is under development
at startup Micro Display Corp. (MDC), a Berkeley, Calif., spin
off from work done at the Massachusetts Institute of Technology
by company founder Philip Alvelda. Alvelda's idea is to make
multiple colors an inherent part of the display structure itself.
He has applied the approach to demonstrate a prototype 640
 x 480-pixel multicolor LCD with a mere 4.5-mm diagonal.

Though Alvelda is still putting financing together and doesn't
expect to have the first generation of mass-produced displays
out until 1997, he claims high interest from OEMs. "There is
almost a universal response that if we can make what we have
demonstrated in the lab at anywhere near the costs we predict
for volume manufacture, they will most definitely buy them,"
he says. "As for naming specific clients, it's a little early
to disclose. But we can say that a major PC manufacturer-the
largest one, in fact-is a strong partner in this venture and
that several VR-headset manufacturers are vying to be partners
as well."

The opportunities for the new crop of displays are great, Alvelda
says, but so are the challenges. "Lifetime and reliability
must be very good and well documented," he says. "You have
to be able to throw your pager across the room or leave it
on the dashboard of your car in the middle of summer and still
have some expectation of its working.

"This also means that new displays face a significant barrier
to entry into these markets, where Motorola, for example, can't
afford to recall 3 million new pagers because of a problem
with material lifetime or stability."

A BROAD FIELD

Among those in the mini game are a variety of Japanese LCD vendors
with small, high-density polysilicon thin-film transistor (TFT)
active-matrix (AM) LCDs; Micron Display Technology Inc., of
Boise, Idaho, which has demonstrated a small FED display for
camera viewfinders; Planar Systems, of Beaverton, Ore., which
has developed an AM electroluminescent (EL) display for head
mounted display applications; Displaytech Inc., of Boulder,
Colo., with its ferroelectric mini; Dallas-based Texas Instruments
Inc., with digital micromirror devices; and Kopin Corp., of
Taunton, Mass., which uses a proprietary process to form AM
LCD circuitry on a conventional silicon wafer and then transfer
it to a transparent glass substrate. All confront the color
generation problem.

With conventional AM LCD displays, such as those used in notebook
computers, manufacturers lay red, green and blue filters over
a trio of subpixels to give each pixel its multicolor capabilities.
While that is fine for pixels that are one-third of a millimeter
in size, proper alignment is extremely difficult for miniature
displays, given their small size and high density, some sources
say. And as for multicolor displays such as FEDs, which make
use of RGB phosphors, very small pixel geometries may strain
the limits of current phosphor-deposition technology, according
to sources.

A promising solution to the multicolor challenge may lie in
generating colors temporally, rather than spatially, as in
the old "color wheel" model. Displaytech, for instance, uses
a sequence of light pulses from red, green and blue light-emitting
diodes (LEDs) as the light source for its mini ferroelectric
LCD. TI and Kopin are also pursuing time-sequential routes
to crank multiple colors out of their miniature displays.

One drawback to the temporal schemes, of course, is the need
for three times the frame rate to display the three monochrome
components of an image sequentially in one frame time. A drawback
to the spatial color schemes is that dedicating three subpixels
to the three primary colors reduces effective resolution of
the display by one-third.

In Alvelda's microdisplay scheme, tiny diffraction gratings
are fabricated on top of the CMOS circuitry on a silicon substrate
using conventional fabrication processes. The grating associated
with every subpixel acts like a tiny prism, spreading white
light out into a rainbow-like spectrum. Though all the spectra
in a triad of subpixels are identical, the gratings are designed
so that the spectra emerge at different physical angles. By
using a "window" to mask out all but one color band of each
spectrum, the three subpixels display red, green and blue light.

The gratings of MDC's microdisplay are sets of grooves, reflective
at the top and black at the bottom, which are easily built
into chips during normal fabrication. Creating the offset color
bands is merely a matter of varying the groove spacing. Because
the display's color capabilities are built in during microchip
fabri-cation, the color chips require no more effort or expense
than monochrome ones.

But Displaytech president Mark Handschy questions the advantage
of MDC's approach. "The device has the appearance of being
more monolithic because all the color generation is done on
chip," he says. "But in practice, it's very easy to make red,
green and blue switchable lamps out of LEDs. We're not worried
about integration in that sense: it's not a practical obstacle."

Alvelda counters that many potential minidisplay applications
are somewhat constrained in their lighting possibilities, adding
that the white-light source in his prototype is just "a standard,
garden-variety Maglite bulb from Radio Shack." In a virtual
reality headset, he explains, the microdisplays could be positioned
at the corners of a pair of glasses, illuminated by light sources
located in the ear pieces of the spectacles.

There's a problem with such applications, though, in that the
displays are really too small to be viewed directly, and good
magnification optics, such as microscope objectives, are big,
heavy and expensive. There's no point in a having cheap, high
resolution screen if you can't afford the optics to view it.
But, Alvelda, leveraging the IC technology on which his miniature
display is based, claims to have crafted a solution to the
problem.

MDC's answer is to build a computer-aided-design tool into the
CMOS design software that allows for optical predistortion
of the pixel array. Once OEM customers have specified the inexpensive
optics they plan to use and calculated the aberrations that
the imaging system will produce, the information is used to
design a pixel array whose shape is appropriately distorted,
as well as the size and shape of the individual pixels. Another
CAD tool randomly jitters the pixel position to prevent aliasing,
or the unwanted Moir patterns that often can be produced by
periodic structures.

Particular strengths and weaknesses aside, the miniature display
technologies have the potential to radically change the field.
Alvelda says that in high volumes, the unit price of his display
could fall to as low as $5. Though Handschy won't commit to
numbers that low, he does agree that silicon-based devices
have the potential to be very inexpensive.

"It's fair to say that the dominant cost input is going to be
the cost of the silicon," he says.

Sunny Bains is a technical journalist based in Edinburgh, Scotland.




<A HREF ="#HeadList">Back to Headline List</A><BR>
<HR>
<A NAME = "35"></A><IMG SRC="/pubgifs/ec109.gif"><BR><BR>
Copyright
OEM MAGAZINE via NewsNet
February 1, 1996  Vol. 4 No. 

Fault line: Fishing on the Internet

By:
Andrew W. Davis

Just when you thought it was impossible to say anything new
about the Internet, along comes a company with a novel announcement
that represents a true breakthrough in thinking, with far-ranging
implications for the technical community. At the recent DSPWorld
trade show in Boston, Texas Instruments Inc. introduced an
Internet-based service called Online DSP Lab, a kind of virtual
laboratory available to users via cyberspace.

If you have engineers using signal-processing products, doing
embedded design or thinking about the next generation of multimedia
peripherals, the TI announcement may change the way you evaluate
vendors and use computer-based development tools. Today there
are four major corporate applications of Internet technology
 e-mail; publishing of product information, newsletters, press
releases, advertisements and other marketing materials; giving
and receiving technical support; and actual sales and distribution
of products. Into this environment comes a new breed of fish
 Online DSP Lab.

Here, any engineer or software developer with an Internet connection
can access a computer to test and evaluate TI's DSP-application
design and development tools. (The actual computers are located
at the lab of DSPnet, an on-line DSP Yellow Pages run by DSP
Associates, which also runs the DSPWorld conference.) Users
can also compile, link and download code to lab computers to
debug and benchmark their own software and TI's software-engineering
tools and new DSP chips. Being Internet-based, the virtual
lab is available free, 24 hours a day, seven days a week to
anyone, anywhere with Net access.

The virtual laboratory is a strategic move. It lets engineers
familiarize themselves with the company's software-development
environment and even test their own code before buying the
tools. This could give TI a competitive advantage in the eyes
of the bench engineer.

The availability of software-development platforms and DSP hardware
reduces or eliminates the need for customers to wait for loaner
equipment. Managing loaners is a sales/support headache for
both the channel and vendors. And to the extent the facility
is used internally by engineering and marketing teams, they're
likely to develop even better tools.

Suppliers who use the Internet will certainly have an advantage
over other companies in the electronics OEM market, and those
who show innovation and creativity are likely to have even
more of an advantage. Creating an on-line laboratory is not
exactly a slam dunk, and the resources involved are significant.
But Online DSP Lab is an intuitive service that directly applies
available technology to the needs of a defined set of customers.
Such trend-setter investments should keep us all on our toes.

Andrew W. Davis, president of Wainhouse Consulting, in Southborough,
Mass., can be reached at andrewwd@wainhouse.ultranet.com.




<A HREF ="#HeadList">Back to Headline List</A><BR>
<HR>
<A NAME = "36"></A><IMG SRC="/pubgifs/ec109.gif"><BR><BR>
Copyright
OEM MAGAZINE via NewsNet
February 1, 1996  Vol. 4 No. 

Back panel: Digital Cellular Takes Its Toll

By:
Herschel Shosteck

For six years, U.S. digital cellular has been hyped as the technology
wave of the future. In truth, it is more a ripple than a wave.
Digital cellular has been promoted in two flavors: TDMA (IS
54/136), supported primarily by telecommunications giant Ericsson;
and CDMA (IS-95), vigorously promoted by Qualcomm, an R&D company
that owns the enabling patents. Among the major infrastructure
manufacturers,

Northern Telecom, Motorola and AT&T have committed to CDMA.
TDMA has been slow to take off in the United States since its
1992 commercial launch on Southwestern Bell Mobility's Cellular
One System. By yearend 1995, TDMA users accounted for only
about a million of the United States' 33.4 million cellular
subscribers.

TDMA's slow adoption has stemmed from the early illusion that
it could be simply "overlaid" onto the existing analog AMPS
system by swapping a TDMA radio for an AMPS one. In reality,
multipath issues degrade the timing of TDMA RF signals. To
compensate, TDMA cell sites must be placed closer together
than analog sites, resulting in 30 percent higher cell density
than analog-and thus requiring a complete re-engineering of
the entire cellular system.

MetroCel (now AT&T Wireless) announced a $150 million investment
in March for TDMA conversion in Dallas. Many cellular carriers
that are nominally committed to TDMA have been reluctant to
make a similar investment. But only when established analog
cellular systems are properly reengineered to support TDMA
will they deliver adequate reception and find acceptance.

CDMA cellular's path has not been much different. Commercial
CDMA was originally promised to PacTel (now AirTouch) in January
1994. As of yearend 1995, industry expectations of "true" commercial
deployment ranged from first quarter 1996 to late 1997 or beyond.

CDMA was originally promoted as facilitating far fewer cell
sites than conventional analog while providing far higher capacity.
But the technology is cursed with the "near-far effect," or
the need to balance power from the cell site to all terminals
regardless of their distance from the cell. To overcome that,
CDMA proponents may be compelled to deploy a cell density similar
to that of TDMA, which could abrogate CDMA's claimed economic
advantages.

Both digital technologies have claimed a capacity advantage
over conventional analog. However engineering refinements have
continued to improve the capacity of analog systems to far
beyond what was thought feasible in the mid-1980s. Those who
focus solely on digital cellular to the exclusion of analog
thus may find a limited market for their products for a long
time to come.

Herschel Shosteck is president and CEO of Herschel Shosteck
Associates, in Wheaton, Md.





<A HREF ="#HeadList">Back to Headline List</A><BR>
<HR>
</PRE>
</BODY>
</HTML>
</DOC>