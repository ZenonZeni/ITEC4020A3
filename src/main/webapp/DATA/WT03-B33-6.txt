
<DOC>
<DOCNO>WT03-B33-6</DOCNO>
<DOCOLDNO>IA059-000323-B025-139</DOCOLDNO>
<DOCHDR>
http://newsnet.com:80/libiss/ec40.html 205.156.212.5 19970114222639 text/html 267090
HTTP/1.0 200 OK
Server: Netscape-Commerce/1.12
Date: Tuesday, 14-Jan-97 22:22:41 GMT
Last-modified: Thursday, 24-Oct-96 22:46:52 GMT
Content-length: 266902
Content-type: text/html
</DOCHDR>
<HTML>
<HEAD>
<TITLE>/data/webdev/libiss/ec40.html Sample Issue</TITLE>
</HEAD>
<BODY BGCOLOR="FFFFFF">
<FONT SIZE = 3>
<IMG SRC="/pubgifs/ec40.gif"><BR><BR>
<A NAME=HeadList"></A>
Copyright <BR>
NETWORK COMPUTING via NewsNet <BR>
January 15, 1996  Vol. 7 No.1<BR>
SAMPLE ISSUE HEADLINES<BR><BR><BR>
<BR>
<H3>WHAT'S INSIDE</H3>
<UL>
<A HREF = "#1"><LI>Fancy Passes Are Probably Just Passing Fancies</A>&nbsp&nbsp&nbsp<NOBR>(1036 words)</NOBR></LI>
</UL>
<BR>
<H3>LETTERS</H3>
<UL>
<A HREF = "#2"><LI>Letters: Generation X.500</A>&nbsp&nbsp&nbsp<NOBR>(179 words)</NOBR></LI>
<A HREF = "#3"><LI>Letters: The Net Dimension</A>&nbsp&nbsp&nbsp<NOBR>(75 words)</NOBR></LI>
<A HREF = "#4"><LI>Letters: Dream A Wireless Dream</A>&nbsp&nbsp&nbsp<NOBR>(333 words)</NOBR></LI>
<A HREF = "#5"><LI>Letters: Tape Rewind</A>&nbsp&nbsp&nbsp<NOBR>(191 words)</NOBR></LI>
<A HREF = "#6"><LI>Letters: Correction</A>&nbsp&nbsp&nbsp<NOBR>(30 words)</NOBR></LI>
</UL>
<BR>
<H3>THE H REPORT -- NEWS, TRENDS AND ANALYSIS</H3>
<UL>
<A HREF = "#7"><LI>It's Quiet Out There...</A>&nbsp&nbsp&nbsp<NOBR>(167 words)</NOBR></LI>
<A HREF = "#8"><LI>ATM Forum To Downsize</A>&nbsp&nbsp&nbsp<NOBR>(145 words)</NOBR></LI>
<A HREF = "#9"><LI>Auto Network Goes Virtual</A>&nbsp&nbsp&nbsp<NOBR>(126 words)</NOBR></LI>
<A HREF = "#10"><LI>NT Shares Room With A SystemView</A>&nbsp&nbsp&nbsp<NOBR>(203 words)</NOBR></LI>
<A HREF = "#11"><LI>Statshot: Internet A Hit With IS Managers</A>&nbsp&nbsp&nbsp<NOBR>(84 words)</NOBR></LI>
<A HREF = "#12"><LI>BuzzNet</A>&nbsp&nbsp&nbsp<NOBR>(47 words)</NOBR></LI>
<A HREF = "#13"><LI>Quick Bits: It's A TV! Poof! It's A Phone!</A>&nbsp&nbsp&nbsp<NOBR>(56 words)</NOBR></LI>
<A HREF = "#14"><LI>Quick Bits: The Wrist Risk Of The '90s?</A>&nbsp&nbsp&nbsp<NOBR>(105 words)</NOBR></LI>
</UL>
<BR>
<H3>THE H REPORT: CONTEXT -- BACKGROUND NEWS ANALYSIS</H3>
<UL>
<A HREF = "#15"><LI>Which Operating System For Your 'Intranet'</A>&nbsp&nbsp&nbsp<NOBR>(1140 words)</NOBR></LI>
</UL>
<BR>
<H3>THE H REPORT -- INTERNET</H3>
<UL>
<A HREF = "#16"><LI>Cisco Acquisition Deals Blow To IPv6</A>&nbsp&nbsp&nbsp<NOBR>(199 words)</NOBR></LI>
<A HREF = "#17"><LI>The Internet Hit List</A>&nbsp&nbsp&nbsp<NOBR>(221 words)</NOBR></LI>
<A HREF = "#18"><LI>V-ONE Chisels Commerce Drawbridge In Internet Firewalls</A>&nbsp&nbsp&nbsp<NOBR>(191 words)</NOBR></LI>
</UL>
<BR>
<H3>COLUMNISTS</H3>
<UL>
<A HREF = "#19"><LI>The Networkologist: Salary Survey Shows Satisfaction</A>&nbsp&nbsp&nbsp<NOBR>(667 words)</NOBR></LI>
<A HREF = "#20"><LI>FreeWire: A New Year And Some New Challenges</A>&nbsp&nbsp&nbsp<NOBR>(1463 words)</NOBR></LI>
<A HREF = "#21"><LI>Corporate View: Businet: A Call To Arms Within The Internet</A>&nbsp&nbsp&nbsp<NOBR>(1018 words)</NOBR></LI>
</UL>
<BR>
<H3>SNEAK PREVIEWS</H3>
<UL>
<A HREF = "#22"><LI>Novell XTD: Message To The Wise</A>&nbsp&nbsp&nbsp<NOBR>(338 words)</NOBR></LI>
<A HREF = "#23"><LI>NetWare 4.1 Climbs To New Heights With SMP</A>&nbsp&nbsp&nbsp<NOBR>(545 words)</NOBR></LI>
<A HREF = "#24"><LI>Lotus Notes 4 Throws Down Messaging Gauntlet</A>&nbsp&nbsp&nbsp<NOBR>(981 words)</NOBR></LI>
<A HREF = "#25"><LI>Single Object Store Remains In Notes 4</A>&nbsp&nbsp&nbsp<NOBR>(62 words)</NOBR></LI>
</UL>
<BR>
<H3>FEATURES</H3>
<UL>
<A HREF = "#26"><A HREF = "#26"><LI>The Riches Of Switches -- 12 Affordable, Fast, High-Qualilty&nbsp;Ethernet Switches Under $250 Per Port</A>&nbsp&nbsp&nbsp<NOBR>(3295 words)</NOBR></LI>
<A HREF = "#27"><LI>Putting Ethernet Switches To The Test</A>&nbsp&nbsp&nbsp<NOBR>(233 words)</NOBR></LI>
<A HREF = "#28"><LI>Classic Bridges vs. Switches</A>&nbsp&nbsp&nbsp<NOBR>(150 words)</NOBR></LI>
<A HREF = "#29"><LI>Middleware -- Driving Applications On the Network</A>&nbsp&nbsp&nbsp<NOBR>(4037 words)</NOBR></LI>
</UL>
<BR>
<H3>CENTERFOLD</H3>
<UL>
<A HREF = "#30"><LI>The Redeeming Qualities Of Val-Pak's Network</A>&nbsp&nbsp&nbsp<NOBR>(466 words)</NOBR></LI>
</UL>
<BR>
<H3>REVIEWS</H3>
<UL>
<A HREF = "#31"><LI>Serving Up HTML Documents On The Web</A>&nbsp&nbsp&nbsp<NOBR>(3613 words)</NOBR></LI>
<A HREF = "#32"><LI>Web Servers: A Status Update</A>&nbsp&nbsp&nbsp<NOBR>(403 words)</NOBR></LI>
<A HREF = "#33"><LI>How We Tested: Hardware And Operating Systems</A>&nbsp&nbsp&nbsp<NOBR>(547 words)</NOBR></LI>
<A HREF = "#34"><A HREF = "#34"><LI>Network Analyzers Race To Capture Fast Ethernet But Need To&nbsp;Rev Their Engines</A>&nbsp&nbsp&nbsp<NOBR>(1957 words)</NOBR></LI>
<A HREF = "#35"><LI>How We Tested Fast Ethernet Analyzers</A>&nbsp&nbsp&nbsp<NOBR>(343 words)</NOBR></LI>
<A HREF = "#36"><A HREF = "#36"><LI>20 V.34 PC (Politically Correct) Card Modems Can't Change The&nbsp;Weather</A>&nbsp&nbsp&nbsp<NOBR>(2020 words)</NOBR></LI>
<A HREF = "#37"><LI>Whatever Happened To IBM and US Robotics?</A>&nbsp&nbsp&nbsp<NOBR>(157 words)</NOBR></LI>
<A HREF = "#38"><A HREF = "#38"><LI>Online Services And The Internet: The Network Manager's Friend&nbsp;Or Foe?</A>&nbsp&nbsp&nbsp<NOBR>(2058 words)</NOBR></LI>
<A HREF = "#39"><LI>Online Services Pricing And Payment Options</A>&nbsp&nbsp&nbsp<NOBR>(87 words)</NOBR></LI>
<A HREF = "#40"><LI>Other Internet Access Options Are In The Wings</A>&nbsp&nbsp&nbsp<NOBR>(223 words)</NOBR></LI>
</UL>
<BR>
<H3>COLUMNISTS</H3>
<UL>
<A HREF = "#41"><LI>On The Edge: ATM: Ready Or Not? Here It Comes!</A>&nbsp&nbsp&nbsp<NOBR>(1693 words)</NOBR></LI>
</UL>
<BR>
<H3>WORKSHOPS</H3>
<UL>
<A HREF = "#42"><LI>Middleware & Cross-Platform Development: To DB2 From The Desktop: Too Many Ways</A>&nbsp&nbsp&nbsp<NOBR>(1888 words)</NOBR></LI>
</UL>
<BR>
<H3>COLUMNISTS</H3>
<UL>
<A HREF = "#43"><LI>Net Results: The Sweet Music Of Network Management</A>&nbsp&nbsp&nbsp<NOBR>(1382 words)</NOBR></LI>
<A HREF = "#44"><LI>E-Mail & Messaging: Adding Interactive Services To Your Web Server</A>&nbsp&nbsp&nbsp<NOBR>(3378 words)</NOBR></LI>
<A HREF = "#45"><LI>E-Mail & Messaging: Sample File: DIRSEARCH.CMD</A>&nbsp&nbsp&nbsp<NOBR>(100 words)</NOBR></LI>
<A HREF = "#46"><LI>E-Mail & Messaging: ISINDEX Replacement</A>&nbsp&nbsp&nbsp<NOBR>(30 words)</NOBR></LI>
</UL>
<BR>
<H3>BUYER'S GUIDE</H3>
<UL>
<A HREF = "#47"><LI>Fast Networking: Appearing In The NIC Of Time</A>&nbsp&nbsp&nbsp<NOBR>(1555 words)</NOBR></LI>
<A HREF = "#48"><LI>Check List: Making Networking Pay Off</A>&nbsp&nbsp&nbsp<NOBR>(48 words)</NOBR></LI>
<A HREF = "#49"><LI>And The Winner Will Be</A>&nbsp&nbsp&nbsp<NOBR>(195 words)</NOBR></LI>
</UL>
<BR>
<H3>PRODUCT UPDATE</H3>
<UL>
<A HREF = "#50"><LI>Product Update</A>&nbsp&nbsp&nbsp<NOBR>(537 words)</NOBR></LI>
</UL>
<BR>
<H3>COLUMNISTS</H3>
<UL>
<A HREF = "#51"><LI>The Last Byte: ISDN Blues: And The Internet Beat Goes On</A>&nbsp&nbsp&nbsp<NOBR>(572 words)</NOBR></LI>
</UL>
</FONT>
<BR><BR>
<HR>
<PRE>
<A NAME = "1"></A><IMG SRC="/pubgifs/ec40.gif"><BR><BR>
Copyright
NETWORK COMPUTING via NewsNet
January 15, 1996  Vol. 7 No.1

Fancy Passes Are Probably Just Passing Fancies

By:
Fritz Nelson

The first time I saw Magic Johnson play basketball, I was compelled
to run to the nearest playground. On that chilly winter day,
I discovered the wonders of the no-look pass by drilling the
ball into the frozen face of an unsuspecting friend.

While surprise passes on the playground can make you a legend,
either by basket or broken nose, surprise passes in the magazine
world are a constant source of trouble and amusement.

For instance, Cisco and NBase submitted products that didn't
qualify for Rob Kohlhepp's cover story on low-cost switches this
month "The Riches of Switches").  I suppose our product request
letter, which we sent to more than 50 vendors, wasn't explicit
about the price requirement.  Maybe we should have said, "Your
product must list for $250 per port or less, and we really mean
it."

Unfortunately, we did a great deal of switch testing before
we discovered the little anomaly, using, I think, that famous
10th RMON group that reports price per port (RFC $99.99). Cisco
offered a new, adequately priced configuration, so we included
the company.

But I never can tell whether vendors do this on purpose or not.
Last year's review of SNMP-manageable UPSes yielded a handful
of UPSes without SNMP capabilities. Perhaps some vendors missed
that requirement in our letter. I can never remember the rule
 Am I supposed to state the requirement three times, or four?
More likely, some vendors didn't support SNMP-they probably
didn't know what it was.

No matter. The philosophy, it seems, is just send it, and hope
we don't get our noses bent out of shape.

Readers dribble the ball off their feet also.  Some just dribble
down their chin.  This month, one of you sent the following
message to Judy Biener, who creates our E-mail Poll "Olde Lang
Syne and All That Jazz"):  "Judy, There's a dinner in it for you
if I win the IBM 360CE Notebookmy treat, anywhere between NYC and
New Hampshire."  As if.

Judy's response: "That's a little cheap for a laptop. I'd expect
at least dinner and a show." You go girl.

Something Up Our Sleeves

Such slight-of-hand only takes you so far. Magic Johnson's real
talent, for example, wasn't his wizardry with the ball, but
how effortless he made it seem (and please, old geezers, no
letters about Bob Cousy and Earl Monroe). The hard work is
what lurked somewhere up his sleeve. Art Wittmann and Dave
Molta lurk up Network Computing's sleeve. Their voices may
be subtle to some of you, heard only through occasional feature
stories or product reviews, but they've firmly guided us with
their unique real-world perspectives for the past five years.
We unleash them this month with their first columns.

Art, always the fearless pioneer, will use "On the Edge" to
explore new trends and technology. His column will alternate
with Bill Alderson and Scott Haughdahl's currently running
"On the Wire" missives. This month, Art begins on familiar
ground, with some insights on the latest ATM developments ("ATM
 Ready or Not? Here It Comes!" page 149), inspired by recent
visits with Cisco and Bay Networks-two vendors who, he says,
enjoy getting not-so-subtle digs on one another during his
visits.

Dave's "Net Results" column will alternate with Bruce Robertson's
"In the Middle" reports, which is both fitting and ironic.
Where Dave has been far more involved behind the scenes, Bruce
has had an overt presence since Network Computing began life
five years ago. But Bruce will be fading away this month as
he begins a new job with The Meta Group, a consulting firm
based in Reston, Va. Although he will continue to write his
"In the Middle" column, I will miss his abundant middleware
metaphors, his insights, his shot-blocking talent, his friendship.
He also leaves us with two final masterpieces: "Driving Applications
on the Network," page 68, and "To DB2 From the Desktop: Too
Many Ways," page 152.

Dave Molta's masterpiece ("The Sweet Music of Network Management,"
page 159) kicks off a column that will uncover network management
from the inside, exploring how technical managers can work
within organizations to make a real impact on the bottom line.

Dave's impact includes having season tickets to Syracuse
University's men's basketball games.  At center court.  Row J.
(Want his number?)  Recently, Bruce Boardman was in the Syracuse
University lab on a Saturday afternoon, working on his bottom
line-turning in this month's review of Fast Ethernet Analyzers
"Network Analyzers Race to Capture Fast Ethernet But Need To Rev
Their Engines")-when Dave strolled in dangling tickets to that
day's game.  But Bruce turned Dave down so he could finish his
Fast Ethernet analyzer testing.  Now that's certainly dedication.

Yet, between you and me, I'm just a little bit worried about
Bruce. Dave tells me that when Bruce does go to games, Bruce
tends to talk about filtering and decoding referee signals.
Once he suggested to coach Jim Boeheim that he put a Sniffer
in the other team's huddle during timeouts. I'm not one to
pry or, ahem, probe, but I'd suggest Bruce try a little more
social interaction.

Playing Well With Others

Maybe he should take a queue from this month's workshop, "Adding
Interactive Services to Your Web Server".  It offers an in-depth
exploration of building feedback into your Web server-an
important element, given that much of today's content is, in the
words of Tim Haight, executive editor, about as interactive as a
Buckingham Palace guard.

The workshop's writer, our former colleague, Eric Hall, who's
had his share of run-ins with a palace guard or two, used several
Web server products in his testing, but stuck mostly with his
favorite: O'Reilly Software's WebSite. Meanwhile Syracuse University's
Scott Campbell and Josh Linder, and our own Rob Kohlhepp were
testing Web Servers (see "Serving Up HTML Documents on the
Web"), where WebSite did extremely well.

Sometimes these things just come together, like a perfect alley
oop pass, or an IBM ThinkPad and a scrumptious meal in New
Hampshire.

Oh, were you expecting someone else in this space?

Fritz Nelson can be reached at fnelson@nwc.com.




<A HREF ="#HeadList">Back to Headline List</A><BR>
<HR>
<A NAME = "2"></A><IMG SRC="/pubgifs/ec40.gif"><BR><BR>
Copyright
NETWORK COMPUTING via NewsNet
January 15, 1996  Vol. 7 No.1

Letters: Generation X.500



I am still shaking my head in amazement. How can any journalist
worth his salt write an article on directory services (Bruce
Robertson's "Too Many Directories and Too Many Applications,"
November 1, 1995, page 119), and fail to include even one reference
to the ISO X.500 directory service standard? Moreover, how
can he have the gall to state, "There is no agreed-upon directory
standard," when an agreed-upon international directory standard
is exactly what X.500 is?

I understand that there's an obsession today with proprietary
vendor directory standards, but to write about directory service
without acknowledging that the problem is being solved in the
international standards arena strikes me as incompetent journalism.

Karen Goertzel

Manager, International Programs

Secure Systems and Services Operation

Wang Federal

goertzek@wangfed.com

Bruce Robertson replies:

X.500 hasn't penetrated corporate application developers outside
the X.400 e-mail space much. It's not an oversight, just a
question of focus. If I had included X.500, the overall points
of the article would not have changed. X.500 is yet another
directory.




<A HREF ="#HeadList">Back to Headline List</A><BR>
<HR>
<A NAME = "3"></A><IMG SRC="/pubgifs/ec40.gif"><BR><BR>
Copyright
NETWORK COMPUTING via NewsNet
January 15, 1996  Vol. 7 No.1

Letters: The Net Dimension



Thanks very much to Patricia Schnaidt for an excellent article,
"The Four Dimensions of Net Management," (November 15, 1995,
page 31). I couldn't agree with her words more. To quote Schnaidt,
"The problem isn't managing any one box-it's managing all of
them." You might agree IT organizations always lean toward
one or the other, sometimes more than one.

Joseph Michael Edelen

Supervisor,

Information Technologies

Sharp (SMT)

je@sharpwa.com




<A HREF ="#HeadList">Back to Headline List</A><BR>
<HR>
<A NAME = "4"></A><IMG SRC="/pubgifs/ec40.gif"><BR><BR>
Copyright
NETWORK COMPUTING via NewsNet
January 15, 1996  Vol. 7 No.1

Letters: Dream A Wireless Dream



I really enjoyed Bill Frezza's piece on the wireless data debacle
("Bewildered in the Wireless Market," November 1, 1995, page
35). I concur that the writing's on the wall if the astrolabe
doesn't start pointing to the right star soon.

What of commercial digital cellular? In Europe, GSM and DCS
1800 (and soon PCS-1900 in the U.S. and Canada) digital data
is "going potty" and then some. People are using PC Cards in
their handheld Psion Series 3a's and others a-plenty. More
PC Card makers are releasing faster, cheaper models for more
types of handsets. The race is on among vendors to offer customers
compatible software, and it is not through another door in
the wireless data room, but a window in the ceiling-the skylight.

No wonder the dedicated packet mobile data offerings have found
it tough-going. The GSM standard, for example, is well underway
in adding a General Digital Packet Data (GDPD) to carry the
data packets so as not to flood voice channels. Tariffing will
be per-second billing on the whole, though some providers may
bundle data packet traffic as a USP. Where the existing offerings
will end up is clear: Where they are is where they'll stay.
Just because a couple of RD-LAP and Mobitex protocols are here
today, let's not get confused. Gates will get Teledesic to
work with it, too, if the satcomm bubble holds out long enough.

Recall CP/M and M/PM. Nothing wrong with them. They just timed
out before the market rebooted into another command environment.
RD-LAP and Mobitex will have places in wireless history, as
will AMPS, TACS, NMT, C-450, POCSAG and GWBASIC. Let's compare
like with like in the wireless time-continuum. When we realized
that the radio spectrum was as good for data as it had always
been for voice, we were guilty of jumping the gun and becoming
too Asimovian in our dreams of what could be done.

Paul Quigley

Chairman

Synergis

professor@cix.compulink.co.uk




<A HREF ="#HeadList">Back to Headline List</A><BR>
<HR>
<A NAME = "5"></A><IMG SRC="/pubgifs/ec40.gif"><BR><BR>
Copyright
NETWORK COMPUTING via NewsNet
January 15, 1996  Vol. 7 No.1

Letters: Tape Rewind



While we appreciate Exabyte's EXB-10h and EXB-210 8-mm tape
libraries having been included in Eric Carr's October 1, 1995,
autoloader roundup review ("As Storage Needs Grow, Tape Autoloaders
Help Take Away the Pain," page 100), we also want to clear
up some incorrect information.

Regarding the EXB-10h, several factual errors appeared. The
EXB-10h has neither a bar-code option nor dual-drive capability.
In addition, the product identified in the photo on page 102
as being an EXB-10h is in fact an EXB-210. And, while the EXB
10h and the EXB-210 are stated to be original and second-generation
products, respectively, they are actually third-generation
products featuring more robust and reliable designs. As a clarification,
the starting price of the EXB-210 is $9,800.

It may seem odd for us to wish to point out that the EXB-10h
actually has less features than you attributed to it. However,
we feel it's important to note that Exabyte offers a family
of tape libraries built to fit a variety of needs within a
range of market segments.

Mark W. Canright

Senior Vice President, Worldwide Sales and Marketing

Exabyte Corp.




<A HREF ="#HeadList">Back to Headline List</A><BR>
<HR>
<A NAME = "6"></A><IMG SRC="/pubgifs/ec40.gif"><BR><BR>
Copyright
NETWORK COMPUTING via NewsNet
January 15, 1996  Vol. 7 No.1

Letters: Correction



Our October 1, 1995, Product Update should have stated that
Brooktrout Technology's QuadraFax v2.0 is an integrated fax
and voice processing platform that sends and receives faxes.




<A HREF ="#HeadList">Back to Headline List</A><BR>
<HR>
<A NAME = "7"></A><IMG SRC="/pubgifs/ec40.gif"><BR><BR>
Copyright
NETWORK COMPUTING via NewsNet
January 15, 1996  Vol. 7 No.1

It's Quiet Out There...

By:
Timothy Haight and Christine Hudgins-Bonafield

Too quiet! While Internet security breaches haven't made headlines
recently, Carnegie Mellon University's CERT Coordination Center
recently said it is receiving daily reports of "widespread
attacks against Internet sites."

CERT spotlighted several trends in a recent advisory: The use
of protocol analyzers whose presence on a network is disguised
with Trojan horse software; exploiting the rpc.updated or loadmodule
vulnerabilities to gain root access; using automated tools
to scan sites for NFS and NIS vulnerabilities, and launching
IP spoofing attacks.

Additional information about these trends, plus information
and tools to combat them, can be accessed at ftp://info.cert.org
pub/cert_advisories/CA-95:18.README. CERT advisories and bulletins
are posted on the USENET newsgroup comp.

security.announce. To be added to the mailing list for CERT
advisories and bulletins, send your e-mail address to cert
advisory-request@cert.org. If you see activity indicating an
attack in progress, CERT urges you to contact other sites involved
and the service providers, as well as the CERT Coordination
Center.




<A HREF ="#HeadList">Back to Headline List</A><BR>
<HR>
<A NAME = "8"></A><IMG SRC="/pubgifs/ec40.gif"><BR><BR>
Copyright
NETWORK COMPUTING via NewsNet
January 15, 1996  Vol. 7 No.1

ATM Forum To Downsize

By:
Timothy Haight and Christine Hudgins-Bonafield

The ATM Forum is about to get a trim, with Chair and President
Steve Walters wielding the scissors. Faced with an increasing
threat to desktop ATM by 100-Mbps Ethernet and the uncontrolled
growth of the Forum, Walters advocates curtailing or cutting
some of its working groups and forming a management committee
to screen the formation of new ones. Today, about 750 people
attend the organization's bimonthly week-long meetings, each
of which costs $90,000. Walters says he isn't trying to target
particular working groups. However, he says it's important
to ATM's desktop positioning to find out quickly whether LAN
emulation will scale, and to come up with an alternative if
it won't. He also wants to avoid potential overlaps, like those
that now exist between the Forum's PNNI specification and the
work of the Internet Engineering Task Force.




<A HREF ="#HeadList">Back to Headline List</A><BR>
<HR>
<A NAME = "9"></A><IMG SRC="/pubgifs/ec40.gif"><BR><BR>
Copyright
NETWORK COMPUTING via NewsNet
January 15, 1996  Vol. 7 No.1

Auto Network Goes Virtual

By:
Timothy Haight and Christine Hudgins-Bonafield

Economics prompted the top three U.S. automakers to restructure
the industry's plans for an exclusive Internet, complete with
a network exchange point (See Online News on Network Computing
Online at http://www.techweb.cmp). The proposal, known as the
Automotive Network eXchange, now calls for traffic between
the auto industry and its partners to be mixed with that of
the public Internet over backbone links. Last November it wasn't
yet clear whether access links would remain pristine or go
virtual, according to Chrysler's Fred Hakim. Plans remain in
place to seek better performance, reliability and security
over the Internet through certification of suppliers by an
"ANX authority." Hakim says the group hopes to have an RFP
available in the second quarter.




<A HREF ="#HeadList">Back to Headline List</A><BR>
<HR>
<A NAME = "10"></A><IMG SRC="/pubgifs/ec40.gif"><BR><BR>
Copyright
NETWORK COMPUTING via NewsNet
January 15, 1996  Vol. 7 No.1

NT Shares Room With A SystemView

By:
Timothy Haight and Christine Hudgins-Bonafield

IBM-make that the "new IBM"-tells us to expect a SystemView
management console on Microsoft's NT to ship this year. Lynn
Wilczak, IBM SystemView marketing director also expects beta
tests to begin early this year on at least one point product
that takes a piece of SystemView-NetView Distribution Manager
and couples it with NT. Wilczak says the marriage is one users
have urged. They've also indicated a need for storage management
on NT and NetView on NT (delivered in early fall through a
cooperative agreement of IBM and Digital Equipment). In early
December, IBM was also expected to announce an object-oriented
performance management architecture for client/server computing
on SystemView for AIX. The goal is to measure, centralize and
automate performance management tasks over LANs and WANs to
address client/server computing. Key to the architecture are
software probes that can be deployed on any device, with management
traffic controlled by user-defined thresholds. Beta testing
of the probes was expected to begin last month with management
application testing this month. Since May, IBM has announced
SystemView for AIX (shipping), OS/2 (expected to ship by late
last October), MVS (expected by last December) and the AS/400
(expected by mid-96)




<A HREF ="#HeadList">Back to Headline List</A><BR>
<HR>
<A NAME = "11"></A><IMG SRC="/pubgifs/ec40.gif"><BR><BR>
Copyright
NETWORK COMPUTING via NewsNet
January 15, 1996  Vol. 7 No.1

Statshot: Internet A Hit With IS Managers

By:
Timothy Haight and Christine Hudgins-Bonafield

A recent Dataquest study shows that IS managers in medium-size
to large organizations in the U.S. have enthusiastically embraced
Internet use. More than 60 percent of the 100 IS decision-makers
surveyed said all their departments had Internet access. More
than 80 percent considered the Internet a reliable source of
information. When asked how they preferred to receive the kinds
of information listed below, they chose Internet delivery by
the large percentages shown. More info: (508) 871-6630.




<A HREF ="#HeadList">Back to Headline List</A><BR>
<HR>
<A NAME = "12"></A><IMG SRC="/pubgifs/ec40.gif"><BR><BR>
Copyright
NETWORK COMPUTING via NewsNet
January 15, 1996  Vol. 7 No.1

BuzzNet

By:
Timothy Haight and Christine Hudgins-Bonafield

Microsoft Network. A somewhat inflated name for a very large
World Wide Web site.

Bermuda Triangle. A subnetwork of the Internet where packets
go during increasingly frequent periods of router flapping.

M.P.O.A. Many Proprietary Objectives for ATM.




<A HREF ="#HeadList">Back to Headline List</A><BR>
<HR>
<A NAME = "13"></A><IMG SRC="/pubgifs/ec40.gif"><BR><BR>
Copyright
NETWORK COMPUTING via NewsNet
January 15, 1996  Vol. 7 No.1

Quick Bits: It's A TV! Poof! It's A Phone!

By:
Timothy Haight and Christine Hudgins-Bonafield

Northern Telecom must not think cable-telco competition is so
far-fetched. It just freely lent a DMS-500 central office switch
to CableLabs. CableLabs' Bob Cruickshank will test local service
over hybrid fiber/coax first, then interexchange services,
to provide one source to hotels for cable, TV, PC and Internet
access.




<A HREF ="#HeadList">Back to Headline List</A><BR>
<HR>
<A NAME = "14"></A><IMG SRC="/pubgifs/ec40.gif"><BR><BR>
Copyright
NETWORK COMPUTING via NewsNet
January 15, 1996  Vol. 7 No.1

Quick Bits: The Wrist Risk Of The '90s?

By:
Timothy Haight and Christine Hudgins-Bonafield

Do you work at a keyboard, travel extensively, feel stressed
and forget to eat? Do you come in from the cold and hit that
PC? Are you a woman facing mid-life hormonal changes? All these
factors-not just ergonomically incorrect furniture-can contribute
to repetitive stress injuries. In 15 years, RSI is up from
14 to 60 percent of all occupational illnesses. The best prevention
book we've found is Joan Stigliani's The Computer User's Survival
Guide, O'Reilly & Associates, $21.95.

If there is something we ought to know, we welcome proposals
for articles. Please e-mail us at H-REPORT@nwc.com.




<A HREF ="#HeadList">Back to Headline List</A><BR>
<HR>
<A NAME = "15"></A><IMG SRC="/pubgifs/ec40.gif"><BR><BR>
Copyright
NETWORK COMPUTING via NewsNet
January 15, 1996  Vol. 7 No.1

Which Operating System For Your 'Intranet'

By:
Christine Hudgins-Bonafield

It's enough to make a marketeer drool. By the year 2000, Forrester
Research estimates that the public network server hardware
market will hit $2.5 billion, but that's small change. The
"intranet" market of corporate networks based on Internet technology
is expected to be five to 10 times larger. With that much at
stake, it's little wonder that Jamie Zartman, Meta Group program
director, compares server manufacturers to 2,000 supertankers
making a mid-course correction. Just about every operating
system from IBM's mainframe-based MVS to Microsoft's NT is
being repositioned as an Internet server platform. It's too
early for the prognosticators to single out winners, but the
victor is clear in one large market segment-Internet service
providers (ISPs)

Among ISPs, Unix rules, and it's worth examining why some of
the most experienced TCP/IP users refuse even to contemplate
giving up the OS. Of course, as Michael O'Dell, UUNET Technologies
vice president of research and development, puts it, Unix "is
the devil you know versus the one you don't." Still, the issue
is much larger and has relevance for large intranet builders
as well as those shopping for an ISP.

For ISPs, Unix is a natural because the openness of its variants
like Berkeley Software Design Inc. (BSDI) and LINUX-allows
easy access for kernel performance tweaking as well as security
enhancements. Many ISPs also insist that only Unix provides
the scalability they need. Among the many variations of Unix,
Sun Microsystems SunOS, is the leader. Experts estimate that
50 to 75 percent of today's ISP-based servers are Sun-based.

Of course, even within a given provider network, the particular
purpose of a server-whether it's domain name services, e-mail,
news feeds, shell account access, PPP/SLIP access, router/applications
or Web hosting-can lead to differences in approach. Some ISPs
make it a policy to ban the use of any off-the-shelf OS running
stored programs on a transit network, since that network is
so vulnerable to security breaches. Instead, those applications
run behind a firewall along with the administrative network.
While Unix is the OS of choice, the box isn't always a workstation.
Some ISPs run e-mail services on Unix-based Macintoshes. Intel
PCs are also used extensively.

Web hosting is expected to be provided on a variety of platforms,
of course. Many ISPs are serving NT because of its growing
base on corporate networks. Beta testing of Microsoft's own
Internet Information Server on NT began in late November, with
shipments expected this quarter. Where demon performance is
needed, Silicon Graphics servers have often been the first
choice, although some ISPs grouse about SGI's lack of software
portability, skimpy management capabilities and high price.
Nevertheless, SprintLink, one of the world's largest backbone
providers, relies extensively on SGI servers. Hewlett-Packard,
with its reputation for price-performance, lays claim to more
than half of America Online's servers.

Lessons for Users What are some of the lessons ISP reliance
on Unix brings to users building large intranets? A fundamental
one is that Unix's performance edge comes at the price of hiring
a highly sophisticated staff. UUNET's O'Dell is among those
bringing on more Intel Pentium platforms running a variety
of Unix OSes, especially BSDI.

UUNET's Intel boxes are from Digital Equipment Corp. While such
boxes often let ISPs reduce costs as they ramp up, what O'Dell
likes most about BSDI is its openness. "We can whack on these
boxes and tune them to squeeze out every drop of performance."
The downside, however, is that their keyboards and monitors
make their operation in a central location klugey at best and
have so far ruled out their use in lights-out centers.

What should users conclude about an ISP relying extensively
on Unix PCs? On the upside, Dave Crocker, principal with Brandenburg
Consulting, says ISPs that depend instead exclusively on Unix
workstations may overburden their machines, rather than quickly
bringing new Unix-based PCs online. One way for users to check
out such a situation is to see if the ISP will offer a per
port guarantee, such as no more than 10 users per port. The
downside for PCs is the fact that some lack parity checking,
he says. "One reason people go for the workstation is that
it's constructively mindless. The buyer has to be more informed
and intelligent to get a PC platform that is good." Analysts
agree that some large ISPs have this knowledge, while many
smaller ones can vary dramatically in their skill levels.

Getting TCP/IP Techies Another feature to pay attention to in
a server OS-especially for a Web server-is the accompanying
TCP/IP implementation in the server's Unix kernel. Different
kernels support varying numbers of TCP/IP Control Blocks (TCBs).
Internet Architecture Board member and Network Computing columnist
Robert Moskowitz warns that because the Netscape browser retrieves
objects in parallel, rather than sequentially, it can tie up
a larger number of TCBs. Each TCB must then wait a given interval
two minutes in many cases-to clear errors and once again be
usable. Moskowitz believes that the popularization of Netscape
means that more and more TCBs are used to download numerous
small objects on a home page, elongating the user's waiting
period and causing more timeouts.

Unless Netscape and other browser vendors address the problem
using new technologies like persistent connections, Moskowitz
says it will be up to Web providers to tune existing kernels,
pick those supporting the most TCBs and impress on designers
the importance of using maps versus individual objects.

Does It Scale? ISPs also like Unix's scalability. Sun President
Ed Zander says NT "like everything else Microsoft does, is
technically very average, and they hope to get it to market
based on sheer economics, volume and market size. NT doesn't
scale well." Nigel Ball, director of marketing for HP's General
Systems Division, also boasts that NT "might scale on two-CPU
Intel systems, but we have 12-way RISC scaling-orders of magnitude
better than NT."

Analysts say NT's chief asset will be the many inexpensive applications
developed for it as an Internet server. A Microsoft official
also emphasizes NT's ease of deployment, management and application
support. He also calls the argument that NT doesn't scale "a
red herring" because "99 percent of the market needs four processors
or less, and the eighth-largest Internet site in the country
is a two or four-processor NT server" at Microsoft "that scales
quite well."

Cisco Technical Lead, Tony Li, has his own assessment of the
server's future: "the ISPs will continue to suffer scaling
problems as they try to cope with exponential growth. The server
that lets them roll their own, fix their own problems, and
provide stable, cost-effective performance will win. I expect
some flavor of BSD just based on the programming environment.
Hardwarewise, it's probably still a Pentium/Intel architecture."

Christine Hudgins-Bonafield can be reached at cbonafield@nwc.com.




<A HREF ="#HeadList">Back to Headline List</A><BR>
<HR>
<A NAME = "16"></A><IMG SRC="/pubgifs/ec40.gif"><BR><BR>
Copyright
NETWORK COMPUTING via NewsNet
January 15, 1996  Vol. 7 No.1

Cisco Acquisition Deals Blow To IPv6

By:
Christine Hudgins-Bonafield

Cisco Systems fed the fire of doubt surrounding IPv6 when it
announced this past fall that it is buying address translation
firewall leader Network Translation Inc. (NTI). Peter Long,
a marketing manager for Cisco, speculates-without seeing the
code-that NTI's integration into the Cisco OS will occur by
the second half of 1996. Long admits that if IP address translation
like NTI's is made widely available by Cisco, it will delay
IPv6. That isn't necessarily bad, he says, since it gives everyone
more time to think about next generation technology. Long says
Cisco will show an IPv6 prototype this month and provide product
support when the standard is formalized, perhaps by year's
end. The key technology question raised by Cisco's effort to
mix the stateful technology of NTI with stateless router technology
is whether Cisco is working on its own next generation router.
An NTI source says that the only way to integrate the products
is to change the nature of routing. That's already happening,
he says, as basic packet pushing is moved off the router onto
switches. So, will we see a new stateful "router" from Cisco,
replete with security and address translation features?




<A HREF ="#HeadList">Back to Headline List</A><BR>
<HR>
<A NAME = "17"></A><IMG SRC="/pubgifs/ec40.gif"><BR><BR>
Copyright
NETWORK COMPUTING via NewsNet
January 15, 1996  Vol. 7 No.1

The Internet Hit List

By:
Christine Hudgins-Bonafield

Had problems traversing the Internet lately? Want to know if
you are among those inefficient addresses ("Users Cut Off as
Internet Nears Routing Limit," November 1, 1995, page 24) being
exorcised in transit across SprintLink? Here's a clue: You
obtained your address directly from the InterNIC and you have
32 or fewer Class C allocations in block 206 or higher. (The
first three numbers in your address are the block.) Originally,
SprintLink began filtering at 64 or fewer Class C allocations
in block 206, but cut back to 32 last autumn. For block 207
or higher, SprintLink is still filtering at 64 Class C allocations
or fewer. Because so many service providers use upstream providers,
like SprintLink, traffic between two parties can be dropped
even though neither party uses SprintLink for Internet Access.
Also, some small ISPs may be using "inefficient addresses"
to maintain independence from upstream providers. If you think
you may be on the "hit list," check with your ISP and any upstream
providers. The InterNIC advises users to read ftp://rs.interNIC.net
policy/interNIC/interNIC-IP-1.txt, and to use the InterNIC
only as an address provider of last resort. Our own caveat
is that as address space declines and router congestion becomes
more of an issue, the bar at which today's addresses are filtered
can only be raised.




<A HREF ="#HeadList">Back to Headline List</A><BR>
<HR>
<A NAME = "18"></A><IMG SRC="/pubgifs/ec40.gif"><BR><BR>
Copyright
NETWORK COMPUTING via NewsNet
January 15, 1996  Vol. 7 No.1

V-ONE Chisels Commerce Drawbridge In Internet Firewalls

By:
Christine Hudgins-Bonafield

Security startup V-ONE, Rockville, Md., is being heralded by
security experts as a shining example of Internet innovation
perhaps as much as a year ahead of the pack. The company, which
is reportedly going public early this year, was expected to
introduce its SmartGate technology in mid-December. Instead
of simply erecting a barrier to Internet access, SmartGate
is intended to provide controlled access. Using a separate
key for each network session, SmartGate promises to regulate
how far authenticated individuals (not machines) can drill
into a Web server or even through nested firewalls. General
Electric Information Systems and Computer Systems Corp. are
among those planning to use the product. Other V-ONE security
products are used by several military intelligence agencies
and the Securities and Exchange Commission. Forrester Research
analyst Iang Jeon says SmartGate represents the next wave for
the Internet: beefed-up authentication. And CSC's Matt Mancuso
says he is especially "enamored" of SmartGate's ability to
authenticate end users through software or smartcards, instead
of through their machines. Jeon is also impressed with the
product's promised use with any TCP/IP application and the
crytographic background of V-ONE's employees.




<A HREF ="#HeadList">Back to Headline List</A><BR>
<HR>
<A NAME = "19"></A><IMG SRC="/pubgifs/ec40.gif"><BR><BR>
Copyright
NETWORK COMPUTING via NewsNet
January 15, 1996  Vol. 7 No.1

The Networkologist: Salary Survey Shows Satisfaction

By:
Patricia Schnaidt

With client/server applications no longer the dreamy stuff of
magazine headlines, but rather the gritty reality of IS shops,
it's no wonder that the people who can build reliable networks
are in hot demand. Network Computing's first survey of our
readers' job satisfaction and salary tells the story of the
average network professional: a 39-year-old male making $51,840
a year, satisfied with his work, enjoying the challenge of
his demanding job but nevertheless feeling overworked. Let's
dig a little deeper and get at what's behind "the average"
network professional.

In 1995, the mean salary, including bonuses and incentives,
was $54,810, and the median salary was $51,360. In 1994, the
same group's mean salary was $51,980 and the median was $47,520.
That's a 5.2 percent increase in average compensation. The
network professionals we surveyed were 88 percent male and
12 percent female.

Challenged and Busy

The vast majority of network and IS professionals like their
jobs: 27 percent reported being very satisfied and 58 percent
reported being moderately satisfied, while only 15 percent
reported being not very or not at all satisfied. Challenge,
not wages, fuels most of the job satisfaction, while conversely,
wages, not challenge, fuels most of the job dissatisfaction.
Of those who expressed satisfaction with their jobs, 37 percent
said that the challenge of their work contributes the most
to their satisfaction, 24 percent said that exposure to new
technologies is what they like best, and 7 percent said that
the most satisfying aspect of their work is their company's
progressiveness in IT. Only 6 percent cite compensation.

So what makes network professionals unhappy? For all those surveyed,
topping the list at 18 percent is salary. On the heels of underpay
is overwork at 16 percent. Eleven percent identified the company's
lack of progressiveness in IT, 8 percent said that the organization
offered little potential for career advancement, and 6 percent
said it was a lack of professional development.

With the plethora of new technologies, ever-expanding enterprise
networks and client/server application deployment, it's no
surprise that network professionals barely have time to answer
their beepers. Forty-five percent said they were very challenged
by their work; 47 percent said they were moderately challenged.
A scant 9 percent said they were not very or not at all challenged.
What disturbs network professionals' evenings, weekends and
daylight hours is keeping up with changing technology (at 38
percent), keeping up with the organization's demand for IS
projects and deployments right behind it (at 36 percent), a
lack of organizational funding for IT projects (at 24 percent),
a management that doesn't understand the role of MIS (at 23
percent), and users' demand for services (at 19 percent). Six
percent said that the available technology didn't meet business
requirements.

While network professionals name the challenge of their work
as one of its greatest appeals, 60 percent said that they were
consistently overworked, while only 36 percent said they had
the right amount of work and a scant 5 percent said that they
didn't have enough work to keep them busy. No surprise. Network
professionals are stressed out and they say that their job
related stress is rising: 90 percent said that their jobs were
very or moderately stressful. Maybe CFOs should offer stress
reducing massages and consider distributing multivitamins with
network professionals' paychecks.

What are network professionals doing all day? Topping the to
do list is network management and problem solving, followed
by assisting users, planning and strategy, deploying networks,
managing personnel and working with division managers to implement
systems.

But Wait, There's More

Check out our very extensive salary and job satisfaction survey
on Network Computing Online at http://techweb.cmp.com/nwc.
A quick word on the methodology: In September, we mailed questionnaires
to 1,500 network professionals, randomly sampled from our subscriber
list, and 586 completed the survey. Toppmeyer Research, an
independent research firm, mounted the survey and compiled
the data.

Patricia Schnaidt can be reached at pschnaidt@nwc.com.




<A HREF ="#HeadList">Back to Headline List</A><BR>
<HR>
<A NAME = "20"></A><IMG SRC="/pubgifs/ec40.gif"><BR><BR>
Copyright
NETWORK COMPUTING via NewsNet
January 15, 1996  Vol. 7 No.1

FreeWire: A New Year And Some New Challenges

By:
Bill Frezza

As you have probably read, Network Computing is expanding to
20 issues this year-a testament to the growth of the industry.
This provides a wonderful opportunity to delve more deeply
into favorite subjects as well as explore new ones. I intend
to do both, continuing my coverage of the struggling wireless
data industry while branching out into what I am convinced
will be the decade's most significant trend in computing and
telecommunications-Public Network Computing.

Fellow columnist Timothy Haight, in the August 1, 1995, issue,
identified Public Network Computing as the fifth wave in the
evolution of the computer industry. The first three waves have
become the stuff of history-mainframes, minicomputers and PCs.
Mark Stahlman, former analyst at Alex Brown & Sons and former
columnist for this magazine, is generally credited with identifying
the fourth wave-network computing-forecasting the dynamic growth
of workstations as they broke from the PC model and the LAN
as it became the essential nervous system of corporate America.

Public Network Computing-the extension of distributed computing
outside the enterprise conjoined with networks of other public
and private entities-is ready to take center stage in the lives
of the people who read this magazine. Public Network Computing
will be as different from today's telecommunications and computing
environments as PCs and servers were to dumb terminals and
minicomputers.

The first thing you'll notice is that the rigid demarcation
between public and private facilities will blur as IS managers
reach across town and across continents with new network configuration
and control capabilities. As this happens, the transport elements
of the public infrastructure will be forced to unbundle, after
which they will fragment, commoditize and then take off on
a new round of exponential growth. Simultaneously, a new driving
factor and large element of uncertainty will be introduced
into the mix, namely the consumer. Long fenced off behind antiquated
local telephone networks, this PC- and credit card-equipped
change agent will charge onto the Public Network Computing
scene just as corporations are trying to find new ways to reach
both customers and their employees.

Creative Destruction

Squatting in the middle of all this is the venerable Public
Switched Telephone Network (PSTN). The PSTN is a highly tuned,
monolithic, application-specific switching system that today
is tightly bundled with enormous quantities of generic transport.
Its design principles evolved over the course of 100 years,
the last 60 of which occurred in a rigid and protected market.
Most important, the massive central office switches that control
call processing, routing and billing-special purpose mainframes,
really-are architected around underlying traffic models that
have barely changed in decades.

Similarly, the business models that support the financing, deployment
and operation of the PSTN have been frozen by Public Utility
Commissions. Their main purpose is to protect and preserve
the status quo while pandering to their political constituencies,
namely the local Bell Operating Companies and their stockholders.
Investment decisions are based on large upfront capital expenditures
and guaranteed long-term cost recovery. This is accomplished
in highly mature markets with no competitive uncertainty, no
substitution of services and no obsolescence that isn't carefully
planned. This economic model has had an enormous impact on
both system architectures and corporate cultures.

Truth be told, the PSTN is very, very good at what it was designed
to do, just as mainframes did an admirable job of batch processing
the payroll. The fact that the phone network can do anything
else at all, like support fax and circuit switched data, is
a complete accident made possible because this other traffic
was disguised to mimic the parameters of a voice call. This
is hideously inefficient, but if the computer industry had
waited for the phone companies to figure out how to provide
data services rather than go ahead and invent analog modems
that are really voice-call spoofers, we would all still be
computing in splendid isolation.

There is actually an important lesson here. The more the phone
companies try to hold back the tide of Public Network Computing,
forcing customers to live with bundled switching and transport,
as well as tariff structures and business models that are hopelessly
out of date, the more surely phone companies will be rendered
obsolete. Just watch the major Internet Service Providers (ISPs)
like PSI, UUNet and Netcom frantically laying down track, building
depots and switching centers in every city and town. The traffic
that their local Points of Presence (POPs) are bleeding away
from the PSTN may not represent much volume today, but it could
account for most of tomorrow's growth.

Oddly enough, the underlying transport the ISPs use is exactly
the same transport used by the phone companies-after all, a
DS3 line is a DS3 line. The difference is that this transport
is configured to provide application-independent services with
a high degree of modularity and a presumption of distributed
intelligence. None of the applications that the ISPs are chasing
are mature, forecastable or readily characterizable. (Heck,
many of them probably aren't even real.) But, flexibility allows
them to bet on the aggregate. Both the ISPs and Competitive
Access Providers (CAPs), to a certain extent, are learning
to optimize business against a moving target with no guaranteed
rate of return, no decades-long cost recovery and every possibility
that a substitute service can come along tomorrow and take
their business away. Just think how fierce this makes both
their system architectures and their management team.

Now, imagine these two together in a competitive market. In
the Darwinian struggle that looms ahead, the PSTN can be compared
with the marsupials of Australia: highly evolved to fill a
protected environmental niche. The Internet is a wild predatory
mammal, let loose by a massive wave of immigrants that have
little knowledge or concern for the native flora and fauna.
Take a wild guess which is going to kick butt when the last
barriers to competition come down.

Ah, but can't the PSTN always retreat to its lair and live off
voice traffic? Don't be so sure about that. Internet phone
is being scoffed at as an oddity by the phone companies, but
you can bet that at the very least it will massively destabilize
the international long distance tariff structures. Only an
idiot is going to pay $35 an hour for international long distance
a few years from now. Network arbitrage-the ability to use
application-independent switching systems on top of commodity
transport to undercut application-specific bundled and tariffed
services-will become a big game in the age of Public Network
Computing.

Summing It Up

So, putting it all together, what are the top 10 defining technical
and business characteristics of Public Network Computing? 1)
Competitive yet compatible application-independent switching
connected with commodity transport; 2) Distributed intelligence
flexibly accommodating ever-changing traffic models; 3) Modular
deployment aimed at hot markets, unencumbered by social mandates
to guarantee "Universal Service"; 4) Incremental provisioning
with lower first-costs, shorter amortization periods and a
rate of capital equipment turnover closer to the PC industry
than the telecom industry; 5) Destabilizing network arbitrage
that will do more to dissolve the Public Utility Commissions'
cozy little club than any act of Congress; 6) A high rate of
new market entry, and a correspondingly high rate of mergers
and failures; 7) Massive trauma and angst as the local Bell
Operating Companies are forced to cut the dividend, break the
bloated unions and toss out their old-boy, brain-dead management
as they struggle to reinvent themselves; 8) A mind-boggling
array of choices for businesses and consumers; 9) A massive
increase in aggregate telecommunications traffic; and 10) Virtual
integration of selected pieces of all of this under the control
of the enterprise. What a great time to be a Networkologist!

So boot up, tune in and let's watch the game. (I forgot to mention
that the advent of Public Network Computing will also provide
full employment for cheeky commentators.) While I will continue
to bring you FreeWire in each issue of Network Computing, I
have also begun writing biweekly op-ed pieces on technology,
culture and politics for our sister publication, CommunicationsWeek.
In addition, CMP Interactive recently launched a Web service
called TechWeb Gurus in which I am participating. You can visit
us at http://techweb. cmp.com/gurus/gurus.html and put in your
two cents. I've posted a wireless data directory online so
you can get phone numbers and Internet contact information
for any company ever mentioned here. I will also be leading
an ongoing series of interactive debates and dialogue on issues
central to the industry.

So come on by. As the saying goes, I'll see you on the Net.

Bill Frezza is the president of Wireless Computing Associates.
He can be reached via e-mail at frezza@radiomail.net or on
the Web at techweb.cmp.com/gurus/ gurus.html.




<A HREF ="#HeadList">Back to Headline List</A><BR>
<HR>
<A NAME = "21"></A><IMG SRC="/pubgifs/ec40.gif"><BR><BR>
Copyright
NETWORK COMPUTING via NewsNet
January 15, 1996  Vol. 7 No.1

Corporate View: Businet: A Call To Arms Within The Internet

By:
Robert Moskowitz

I have had the opportunity to view the Internet phenomenon from
both inside and out. In the annals of computing, the Internet
has no parallel for us to turn to for comparison or guideposts.
However, like every major computing innovation, it is yet another
double-edged sword.

While many of you at the very least have dipped your toes in
the Internet by creating a Web server of generally available
information about your company, most of you are holding off
on more involved usage of the Internet. Why are you holding
off? Probably because you just don't see how it can meet the
business demands of performance, reliability, management and
security.

Even the word "Internet" is a source of concern. For the Internet
is, in truth, a collection of large sets of interconnected
networks-each separately managed and operated under different
business models. Further, for all practical purposes it seems
impossible to accomplish anything meaningful over the Internet
that will give your company a competitive advantage that will
not require you to use a significant part of this inter-network.
Seeing this from the business perspective, you wonder how can
business criteria be imposed on such an internetwork that lacks
a central body to listen to the needs of business and to get
its various units to adhere to these criteria?

I am constantly hearing that since there is no central authority,
the Internet is not ready for business use. Just recall your
worst nightmare of a leased-line circuit: your Regional Bell
Operating Company (RBOC) for local connection; a long-haul
carrier into another RBOC; perhaps a bypass carrier for some
tariff avoidance; finally, the remote RBOC into the branch
office. What did you do for managing this circuit? Now consider
again that on the Internet your packets are at the mercy of
the provider's routing policies. That 10-mile "link" between
your office in north Boston to Harvard University may be routed
through Washington, D.C., through five service providers. To
whom do you turn when performance is worse than a 56-Kbps direct
link and no part of your visible path is less than T3?

A Diamond in the Rough

As long as the business community continues to wring its collective
hands about the lack of control on the Internet, there will
continue to be little or no control. If the business community
sidesteps the Internet, it will continue to be designed for
its consuming public. Some corners will work well, but the
main will be mediocre. It's time for us to visualize the Internet
as a rough-hewn gem and play the role of the gem cutter.

A four-phase approach can result in a "businet" that exists
within the Internet. The first and foremost process is to publish
a set of business criteria for performance, reliability and
management. This criteria set will need to be a living document
to be tuned as we learn. The best source for this document
is each of our respective trade organizations, working as much
in concert as time will permit. Once these criteria are known,
a certification process for the service providers is finally
possible, based preferably on ISO-9000.

This certification must be administered by an organization trusted
by the business community and preferably recognized by the
service providers and the various national governments. If
none of the current organizations can step up to the task,
the business community can create such a company to look out
for their interests. With all of my involvement in the Internet
operation and standards efforts, I have not found any existing
structure for this certification. Thus, the selection or creation
of an "Internet Quality Association" is the second phase.

A one-time certification of conformance to a criteria set via
ISO-9000 is obviously inadequate. The third phase is the creation
of a service provider monitoring and reporting system run by
our "IQA." This reporting should work much like the FAA's airline
performance metrics. There's nothing like a quality mirror
to whip an industry into shape. Through this monitoring effort,
the IQA could also function as a "court of last resort" for
the service providers and their customers to resolve compliance
issues before real litigation is needed.

Finally, security is needed. The fourth phase would be to standardize
a set of security methodologies. The standards community is
finally making headway, but we businesspeople have to demand
these in products, from IP Security Protocol (IPSP) up through
MIME Object Security Standard (MOSS). They all play a role
and need to be available in every part of the world on all
platforms. Sophisticated two-way authentication for business
interactions is also needed. These may be the best defense
from spoofing attacks that have the security consensus tied
up in knots. Current efforts for secure Web usage are only
a small part of the overall picture.

Ante Up

All of this will cost lots of money to get started. The value
returned on this investment is the creation of the extended
enterprise for whatever enterprise is envisioned at any given
point in time. The funding model for this work may actually
be more challenging than the actual implementation of a business
quality network within the Internet. The certification fees
and security registry fees can be used to offset some costs,
but they cannot be expected to foot the bill fully. Note that
security registry fees amount to network membership fees. Government
funding has a place for any research efforts needed. The most
noteworthy effort is the development of meaningful metrics
for a service provider. For the most part, we should not look
to any government for funding help any more. If we want to
run it ourselves, we need to fund it ourselves.

So do some soul searching. If you're interested in really creating
an extended enterprise, it's time to roll up your sleeves and
get in there.

Robert Moskowitz is a software systems specialist at Chrysler
Corp., Detroit, Mich., and a member of the Internet Architecture
Board (IAB). He can be reached on MCI Mail at 385-8921.




<A HREF ="#HeadList">Back to Headline List</A><BR>
<HR>
<A NAME = "22"></A><IMG SRC="/pubgifs/ec40.gif"><BR><BR>
Copyright
NETWORK COMPUTING via NewsNet
January 15, 1996  Vol. 7 No.1

Novell XTD: Message To The Wise

By:
Barry Gerber

Novell's GroupWise division presents a new student to the class
of client/server messaging systems-GroupWise XTD. Similar to
the current shipping version of GroupWise (v4.1), which does
not have a client/server architecture, GroupWise XTD continues
to perform e-mail, calendaring, scheduling and connectivity
tasks. Building upon these capabilities, XTD adds a noteworthy
set of system administration and management tools, and an impressive
workflow application development environment. It also has a
well-integrated multifunction client that includes full document
management capabilities and a brilliant front end for phone
conferences. Despite its heavy dependence on NetWare, XTD goes
straight to the head of the class.

GroupWise XTD implements full client/server functionality. GroupWise
v4.1 let clients write to the messaging database through a
server process. However, clients must read from the database
directly through a mapped drive.

XTD runs on top of NetWare 3.x and 4.1 and supports TCP/IP clients,
on-server rules, cross-directory synchronization and public
folder replication. Novell promises XTD servers for Windows
NT Server and seven different Unix flavors, but has not yet
determined when these implementations will be delivered. In
addition, it is still deciding if XTD should support IPX/SPX.

Still, XTD is heavily NetWare-oriented. Its messaging system
directory must be stored in a NetWare 4.1 NetWare Directory
Services directory. You can learn about the health of the XTD
server and other components via reports generated through Novell's
NetWare Management System (NMS). Simple Network Management
Protocol (SNMP) agents also will be available.

Impressive Applications XTD has an impressive workflow application
development environment, which uses a basic implementation
of FileNet's WorkFlo product. Not only can you use the XTD
database or third-party databases to develop messaging-enabled
applications, but also you can use XTD's replication capabilities
to distribute applications to other servers and users. You
can also replicate database records so that everyone using
the database can see new records as they are created. XTD's
workflow development environment may give Lotus Notes a run
for the money.

Barry Gerber can be reached atbgerber@nwc.com.




<A HREF ="#HeadList">Back to Headline List</A><BR>
<HR>
<A NAME = "23"></A><IMG SRC="/pubgifs/ec40.gif"><BR><BR>
Copyright
NETWORK COMPUTING via NewsNet
January 15, 1996  Vol. 7 No.1

NetWare 4.1 Climbs To New Heights With SMP

By:
Jay Milne

As an application server, NetWare has been geared mostly toward
workgroup applications but not enterprisewide applications
because of its single processor design-until NetWare 4.1 SMP.
While I ran a beta version on a hefty Compaq ProLiant 4500
with four Pentium 100s and 300 MB of RAM, I was unable to test
many of NetWare 4.1 SMP's capabilities because no applications
were available. However, it promises to let current uniprocessor
systems that are running databases or other CPU-intensive applications
scale and reach higher performance levels without a major architectural
change.

NT and some of the Intel-based Unix systems have implemented
SMP for quite some time. While we don't think NetWare 4.1 SMP
will take any customers away from other SMP operating systems,
it will breathe additional life into NetWare. Certainly it
will benefit those users who have run out of steam with NetWare's
uniprocessor architecture.

By the time NetWare 4.1 SMP is released, only a few SMP-enabled
applications will be on the market. Oracle plans to release
Oracle for NetWare SMP as NetWare SMP ships, and Sybase has
announced plans to ship System 11 for NetWare SMP sometime
in the first half of 1996. Novell should ship GroupWise for
SMP early this year, but internal NetWare 4.1 facilities like
NDS and internal routing will not be SMP-enabled initially.
Unfortunately, that decision will hamper the scalability of
a single NetWare server with a large number of users and a
large NDS tree.

Price for the SMP add-on is set by OEMs. NetWare 4.1 SMP comes
with a license for up to four CPUs (additional CPU licenses
in bundles of four cost extra). No pricing on additional CPUs
was available at press time.

Does Increased Scalability Move Mountains? NetWare 4.1 SMP is
similar to NetWare 4.1, except for a second kernel that handles
and executes SMP threads on all processors. You'll need a Platform
Specific Module (PSM) file (the layer of software that interacts
with the SMP hardware and NetWare) from your hardware vendor
because of the different ways hardware vendors implement SMP.
Having the latest NetWare 4.1 network card drivers is important
to maintain system stability. I got NetWare SMP up and running
within 30 minutes, and SERVER.EXE was updated to handle the
SMP functions.

If you're only using NetWare as a file and print server and
running some utility NLMs, the SMP functionality won't do you
much good. If your applications aren't multithreaded and haven't
been written to take advantage of SMP, NetWare SMP won't be
much help. CPU-bound applications, such as database applications,
will really benefit from SMP. While the operating system of
choice for large, enterprise databases is still Unix, NetWare
4.1 SMP and an SMP-enabled database will let users with existing
databases scale without changing operating systems or hardware
architectures.

Worrying that NetWare 4.1 SMP will break your existing NetWare
applications is a legitimate concern. But because the current
kernel hasn't been replaced and all of the non-SMP applications
continue to run on processor 0, these applications should run
well. We were successful in running several existing applications,
but with any new operating system, make sure to do in-depth
compatibility tests.

Jay Milne can be reached on the Internet at jmilne@nwc.com.




<A HREF ="#HeadList">Back to Headline List</A><BR>
<HR>
<A NAME = "24"></A><IMG SRC="/pubgifs/ec40.gif"><BR><BR>
Copyright
NETWORK COMPUTING via NewsNet
January 15, 1996  Vol. 7 No.1

Lotus Notes 4 Throws Down Messaging Gauntlet

By:
Barry Gerber

If you liked Notes 3, you're going to love Notes 4. It retains
Notes 3's client/server messaging features, including the single
object store. Notes 4 introduces better administrative and
application development tools, an improved user interface,
great support for mobile computing, more scalability and impressive
Internet integration (for more information on Notes 3, see
the feature story "Client/Server Electronic Messaging: Delivering
the Enterprise," November 15, 1995, page 68)

Notes 4 is due out at press time, which will give it a jump
on Microsoft Exchange Server. At this point, I find Exchange
Server easier and more intuitive to manage. I'm very partial
to Exchange Server's tight integration with NT Server's security
system, which makes a lot of administrative and management
tasks more automatic.

I installed a test release of Notes 4 over an existing Notes
3 installation, using a Windows NT server and the Windows95,
NT and 3.11 clients. A Macintosh client was also included.
Although Lotus couldn't provide schedules at press time, Notes
4 eventually will run on the same wide range of server and
client platforms as Notes 3.

Notes 4 is totally compatible with Notes 3 clients and applications.
I had no trouble accessing my Notes 4 server with a Notes 3
client, or running applications built for a Notes 3 environment
on a Notes 4 client. This makes migration from Notes 3 to Notes
4 a more civilized experience.

The new BASIC-like LotusScript 3.0 is compatible with Microsoft's
Visual Basic and ODBC standards, and can be used to develop
many applications that, under Notes 3, required the Notes API,
a great deal of C programming and a vast amount of third-party
vendor development tools. I had no difficulty building little
test applications with LotusScript. Because LotusScript programs
are embedded in the native Notes programming environment, you
can distribute them during standard Notes database replication.

Knockout Admin Apps

Notes 3 didn't have the most user-friendly administrative tools.
My memory, endurance and patience were sorely tested every
time I had to search for the magic combination of menu options
required to configure a server or its connections, or bring
a new user into the world of Notes 3. Notes 4 changes that
with a simple tree-view window for administering anything from
servers and local and wide area connections, to the master
address book database for a Notes organization. This holds
configuration information about users, groups, servers, server
connections and address books, and you can delegate administrative
rights for managing specific servers to any Notes user.

To pick the object class I wanted to administer (People, Server
and so on), I just clicked on it in the left pane of the address
book.  This transforms the address book into a tool kit for
administering the object class.  To add a new user to the
organizational address book, I clicked on "People" in the tree
and the window is transformed into the one you see in the screen
capture.  When I clicked on the Add Person button, the New Person
form appeared and I filled it in to configure my new user.  The
master address book interface is quite similar to the
Administrator program for Microsoft's Exchange Server, although I
prefer Exchange's tabbed property pages to Notes 4's all-at-once
forms.

I never liked Notes 3's client user interface. Notes 4 menus
and Lotus' Smarticons are better organized and labeled. Both
are context sensitive, changing as the task you're performing
changes. I especially like the second level of larger icons,
like the ones for specific e-mail functions. The real UI improvement
in Notes 4 comes with the incorporation of cc:Mail's drag-and
drop folder pane.

Notes' already noteworthy remote access capabilities are greatly
improved in Notes 4. Notes 3's ability to replicate databases
to portable PCs and keep these replicas in sync has made it
a favorite of road warriors. I discovered that Notes 4 brings
even more goodies. Replication is easier with a new graphically
oriented user interface, and field level (as opposed to full
document level) replication reduces replication times when
only a small part of a document has changed. Encryption protects
locally stored databases.

To save money and/or allow a single point of contact to an organization's
Notes system, Notes 4 supports what is called "pass-through."
This mechanism lets a remote client access its home server
by connecting to any Notes 4 server in an organization and
passing through it and any other Notes 4 servers on the network
path to its home server. Pass-through is also available to
LAN-connected Notes users.

Notes 4 will be a highly scalable client/server messaging system.
According to Lotus, Notes 3 can handle approximately 200 users
per server and Notes 4 supports up to 1,000 users per server.
Some of this increased scalability is because of performance
improvements, such as field level database replication, while
some is attributable to improvements in hardware and operating
systems, like Notes 4's ability to run on higher-end computers
with multiple processors. Notes 4 has also been optimized for
32-bit operating systems.

Notes 4 addresses are stored in X.400 format. X.400 and SMTP
message transfer agents will be available separately for Notes
4 servers. The Notes API is fully supported in Notes 4, along
with MAPI and the X.400-based Common Mail Calls API. MAPI support
goes quite deep. I could access my Notes 4 mailbox with a beta
Exchange client for Windows 3.11. I did have to install the
massive Notes and Exchange clients on my computer, but it worked.

Notes 4 comes with its own integrated World Wide Web browser
for your surfing convenience. Notes 4 documents can include
hypertext links not only to other Notes documents but to Web
pages, so you can build applications that integrate data from
Internet sources.

Barry Gerber can be reached at bgerber@nwc.com.




<A HREF ="#HeadList">Back to Headline List</A><BR>
<HR>
<A NAME = "25"></A><IMG SRC="/pubgifs/ec40.gif"><BR><BR>
Copyright
NETWORK COMPUTING via NewsNet
January 15, 1996  Vol. 7 No.1

Single Object Store Remains In Notes 4



One of Notes' real strengths exists in its support of an object
store that holds structured RDBMS-like data, unstructured open
ended data and message routing information.  Although you give up
some performance by including all data in one store, you gain
easier administration, maintenance and especially application
development.  Only Oracle's Oracle Office comes close to Notes in
this regard.




<A HREF ="#HeadList">Back to Headline List</A><BR>
<HR>
<A NAME = "26"></A><IMG SRC="/pubgifs/ec40.gif"><BR><BR>
Copyright
NETWORK COMPUTING via NewsNet
January 15, 1996  Vol. 7 No.1

The Riches Of Switches -- 12 Affordable, Fast, High-Qualilty
Ethernet Switches Under $250 Per Port

By:
Robert J. Kohlhepp

Ethernet switches aren't literally a dime a dozen, but the suggestion
fits. We set our threshold at $250 per port and found plenty
of takers. Whether you're setting up a new workgroup or adding
a little bandwidth to an existing one, there's probably an
affordable switch here for you. There may even be a couple
that could slip into the enterprise category-just because they're
cheap doesn't mean they lack critical features. Within this
pack you'll find everything from basic, no-frills switches
to those with VLAN support, protocol filtering, SNMP management,
high-speed uplinks and RMON support.

We tested the following 12 switches in our University of Wisconsin
labs: SMC EZ Switch-6x and TigerSwitch XLE; 3Com LinkSwitch
1000; NetVantage NV7500 Ethernet Switch; Black Box Ethernet
Switch LB9500A; Amber Wave Systems AmberSwitch; CrossComm EWS;
Cisco Catalyst 2100 and Catalyst 1700; D-Link DES-604 four
port 10BASE-T Switching Hub; Matrox Piranha SwitchNIC-10/8;
and FORE Systems ForeRunner ES-3810 Ethernet Workgroup Switch.

Our first concern was performance, but we also found MAC address
and protocol filters very useful in taking control of our network.
VLAN support is also beneficial since it helps segment the
network without further investment. Products that can manage
these features over the network are optimal.

In such a low-cost scenario, we thought we'd be hard pressed
to differentiate between these products. We were wrong. 3Com's
LinkSwitch 1000 offers the best all-round bargain with great
throughput, good management software and a redundant power
supply. SMC's TigerSwitch XLE offers excellent control over
traffic filtering, securing ports and VLAN setup. It also includes
the unique ability to forward defined packets between VLANs.
Avoid the SMC EZ Switch-6x: It has serious performance problems.

NetVantage NV7500 had the best entry price, without giving up
fast uplinks or manageability. For NetWare-only workgroups,
Matrox offers its Piranha SwitchNIC's eight 10BASE-T ports
at an amazing $1,300-on a PCI NIC, no less. Look to Amber Wave,
FORE and CrossComm for flexible, slotted systems. CrossComm's
EWS is a strong product with excellent management software.
Finally, Cisco's Catalyst 2100, a remnant of Cisco's recent
Grand Junction purchase, only lacks a graphical management
interface, which should be coming early this year.

3Com LinkSwitch 1000

3Com's LinkSwitch 1000 has almost everything you could need,
including a redundant power supply option. It excelled in our
throughput testing, successfully passing more packets than
any other switch. 3Com's management software runs on Windows
or almost any flavor of Unix. With a base price that comes
in under $200 per port, including a 100BASE-Tx port, you can't
go wrong, although the LinkSwitch would benefit from ATM support
and VLAN capability-both slated for 1996.

Configuration, as usual, started at the console. Then, using
3Com Transcend Workgroup Manager, we configured nearly every
aspect of the device. Using "port security," we could restrict
each port to serve only statically defined MAC addresses, which
we could enter manually or grab from the real-time display
of active addresses. Using secure learning, we were able to
set the maximum number of addresses allowed on a port. Once
those addresses are learned, no other station can talk on that
port until you reset the switch or change the configuration.

Using the management software or a standard RMON browser (we
used Cabletron's), you can get basic traffic statistics. In
future firmware releases, the LinkSwitch will add more of the
nine RMON groups. FORE and CrossComm offer all nine groups,
but not in their base offerings.

To help control heavy loads, the LinkSwitch can simulate collisions,
using 3Com's Intelligent Flow Control technique, to lower packet
loss. This slows the traffic from the workstations instead
of forcing the switch to drop packets to keep up. This helped
in the short burst situations we saw in our testing. You can
also control broadcast storms by setting limits on the percentage
of broadcast traffic. When the broadcast traffic hits an upper
limit, it can be handled by an alarm (SNMP trap), port disable
or blip (which disables the port and re-enables it in five
seconds)

Although the LinkSwitch supports only 500 addresses in the bridge
table, it is more than enough for any workgroup. The LinkSwitch
would benefit heavily from the workgroup VLAN definitions that
SMC's TigerSwitch has. In fact, the LinkSwitch offers only
basic MAC address filtering. The hardware supports VLANs and
the next revision of software will exploit it.

CrossComm EWS

CrossComm's EWS is an excellent switch that resembles FORE's
flexible slotted configuration. Although it only supports four
addresses per port, with 32 ports it can handle more than 100
stations. It didn't do as well as 3Com's LinkSwitch on performance.
But CrossComm adds IMS-a strong management application that
performed well for us. It can also run under HP OpenView (3Com's
Transcend runs on others). IMS has an excellent graphical interface
to switch statistics and configuration.

You set the basic IP and SNMP community names using the console,
and then you can configure the EWS remotely. CrossComm offers
MAC address security on all ports, as well as static entries
in the bridge address table (BAT) for basic filtering. Like
3Com and FORE offerings, the EWS offers secure learning, so
it can learn the first four addresses talking on a port and
lock out all others.

The device is fully manageable using IMS, from configuring the
MAC addresses that can talk on a port to viewing statistics
in graphical format. By clicking on a port, we could view the
active stations on a port and allow or deny them access. Like
the FORE box, we could also disable the transmitter or receiver
on individual ports for troubleshooting.

Since a switch sends traffic only to the target port, it is
difficult to use a protocol analyzer. The EWS, like Fore, Cisco
and Amber Wave products, allows "port mirroring," which defines
an "analyzer port." Any port's traffic can be "mirrored" to
the analyzer port. Another solution is to buy a more expensive
version of the switch that supports the full nine RMON groups.

SMC TigerSwitch XLE

SMC has greatly improved its switch line with the TigerSwitch
XLE, which now supports Fast Ethernet uplink ports (the XL
didn't). Although it just squeezes under our price ceiling,
the features may be worth it. Flexibility in port configuration,
bridge table management and excellent VLAN support make this
switch a good choice for any environment. SMC covers a wide
range of management platforms, but its Windows management software,
EliteView, has a very difficult time with large networks.

SMC offers a very flexible architecture that can secure addresses
on a per-port basis, add packet filters to control traffic
and "trunk" many ports to another TigerSwitch. With port security,
you can allow only certain MAC addresses to use a port. Packet
filters can be used to grant or deny certain MAC addresses
or protocols from being used on a port. And SMC's unique trunking
scheme connects up to eight ports to another TigerSwitch, aggregating
throughput between them.

Using workgroup definitions in conjunction with protocol and
address filters yields VLANs or virtual workgroups. Ordinarily
VLANs are completely segregated networks and pass no traffic
between them (without the help of an external router or bridge).
But SMC went a step further by letting predefined traffic pass,
based on protocol type, MAC addresses or other bit patterns.
We were able to configure two individual NetWare workgroups.
To give both groups access to our POP mail server (which is
in one group), we added a filter to pass IP traffic between
the groups. No other switch we tested has this kind of flexibility.

Configuring the TigerSwitch is quite simple through a command
line terminal session. Once the IP address is configured, all
management can take place from EliteView. However, EliteView,
by default, tries to manage your whole network. You limit the
scope of discovery by editing text files, which isn't a very
clean approach. Once our network discovery finished, the management
station (a Pentium 60) ran quite slow, finding it difficult
to manage all the data from our huge network (more than 150
subnets). EliteView also had difficulty identifying our routers,
which is crucial in the network discovery process. You may
want to step up to SMC's HP OpenView software component.

Cisco Catalyst 2100

As the Catalyst 1700's big brother, the 2100 offers a couple
extra features, not the least of which is multiple MAC addresses
per port. Using a terminal (there is no graphical tool), the
switch is very configurable, offering VLAN support, multiple
switching methods (cut-through, store-and-forward and fragment
free), and, unique to Cisco, multicast registration. This switch
pushes the $250 per port limit and could use better management
software, but it is flexible and has plenty of features.

Supporting 2,048 addresses, or optionally 8,192, your network
should never outgrow the bridge table-although it will outgrow
the management, which is simply a MIB definition. Cisco plans
to integrate management in 1996. Using a terminal or telnet
session, we were able to set security to allow only certain
addresses on particular ports; and we could define VLANs, although
Cisco should implement some packet filters that would allow
certain packets to pass between them, like SMC has done.

Cisco also lets you prevent unnecessary traffic on ports using
port configuration and multicast registration. Since multicast
addresses cannot be learned, they are usually flooded to all
ports. By configuring multicast registration through the terminal,
you can define which ports receive which multicasts.

FORE Systems ForeRunner ES-3810 Ethernet Workgroup Switch

FORE Systems is certainly no stranger to switching, and its
ForeRunner is no disappointment. A flexible chassis design
accepts more than the Ethernet modules we tested, including
ATM and an SNMP agent with all nine RMON groups. We received
none of these modules in the unit we tested. Like CrossComm's
EWS, ForeRunner is limited to four addresses per port. Filtering
and VLAN support are missing, and the network management module
was absent in the unit we tested, making the ForeRunner pricey
for its features.

The switch we tested consisted of the chassis and one 16-port
switch card. That starts at about $230 per port, but as you
add cards, the cost per port goes down quickly. Performance
ranked right along with the rest, and six wire-speed streams
were not a problem. You can configure one port as an uplink
port, and it will serve as the outlet for packets of unknown
destination.

Using the terminal configuration menu system, we could view
and change aspects such as full/half duplex, uplink port designation
and port mirroring. For troubleshooting, the ForeRunner also
lets you shut off the transmit and receive portions of the
port independently. For more features, FORE offers the inband
SNMP agent module for the chassis and still squeezes in under
the $250-per-port limit.

Amber Wave Systems AmberSwitch

Flexible configuration and cool design make this a fashionable
hub-and it compares well with the products we tested in terms
of its speed and its ability to grow (by filling the slotted
chassis). Although the eight-port configuration is quite high
at $250 per port, a full chassis drops to around $170 per port.
This no-frills switch, using a unique Digital Signal Processor
(DSP) design for controlling the bridge, leaves the door open
for adding new features, such as VLANs, using a simple software
update. Management, currently only a MIB definition, should
improve with the pending introduction of Amber Wave's Windows
management software. For now, lack of RMON, management software,
VLAN support and fast uplinks makes the AmberSwitch high on
potential but mediocre on today's list.

The terminal port let us view the current learned address table
and configure the bridging method on each port, including Amber's
"Adaptive" method. The Adaptive method uses cut-through switching,
but changes to store-and-forward switching if fragmented packet
rates rise. Then, once error rates subside (a configurable
threshold), it reverts back to cut-through switching. Port
mirroring will be included in the next revision, according
to Amber Wave.

The AmberSwitch grows by adding more eight-port card to its
five-slot chassis. It can grow to a total of 32 ports; the
first slot is taken up by the management agent. Soon, Amber
Wave will offer a two-port, 100BASE-Tx module as well. This
works well in the switched Ethernet market, but don't plan
on any ATM uplinks in this switch.

Amber's switching engine uses a hierarchical approach to managing
the address table. Individual cards keep a copy of the table,
and switch based on that table. When new addresses are seen
on a port, that information is passed to a master DSP in the
chassis, which maintains a master copy of the table. It inserts
the entry and replicates the table to all the cards. Using
a DSP is fast enough and lets AmberWave add more features with
simple software updates-and it costs less than a general-purpose
CPU.

Matrox Piranha SwitchNIC-10/8

Matrox cleverly built this fast PCI card-based switch to increase
bandwidth in existing NetWare workgroup servers. Although it's
a bit fickle about bus speed and offers no traffic filtering,
the entry price of $165 per port makes this a very attractive
switch for workgroups. In addition to being an eight-port workgroup
switch, it provides 80 Mbps throughput to the server. Like
the AmberSwitch, it lacks many of the crucial features-VLAN
support, filters, fast uplinks-that other products have.

Although Matrox reports that the Piranha operates slower on
a 30-MHz PCI bus (it prefers 33 MHz), in our testing, it still
performed as well as the standalone switches without loading
the server at all. Matrox's management application, Guardian
Manager, while only offering the most basic information, adds
a nice graphical interface for browsing.

Installation is quick, if you are familiar with NIC installation
under NetWare. We had TCP/IP and IPX/SPX working on the card
in a matter of minutes. However, with AppleTalk bound to the
board, we couldn't see the host server on the network. After
a driver update from Matrox, AppleTalk worked to the server
as well.

Guardian Manager, which runs under HP OpenView (for Windows),
offers an excellent graphical look into the Piranha card. However,
every time we tried to get the interface working, the initial
queries would crash our server. Matrox engineering found a
bug when using SNMP over TCP/IP, and sent a fix. Apparently
Matrox had only tested IPX. Once up and running, we were able
to view the port statistics and address tables. Ports can be
enabled or disabled, or switched to full duplex. All of this
can be done at the NetWare console, using Matrox's console
configuration utility.

NetVantage NV7500

At the top of the pack of small switches (eight ports and under),
NetVantage offers a great switch with slots for fast uplinks.
At $200 per port, it's a bit steep for the few features it
has, but at an entry price of $1,600, you can hardly go wrong.
Unlike some of the others, NetVantage offers a graphical interface
to the basic SNMP agent. However, configuration and maintenance
is very basic, with no support for advanced features, such
as RMON or filtering.

With an agent that's identical to the Black Box switch, the
NV7500 provides the same type of information and configuration.
However, NetVantage offers a graphical interface to the basic
MIB II statistics under HP OpenView, complete with a graphical
representation of the hub. It would be advantageous to add
the ability to manipulate the static filter entries through
the graphical interface, however.

The basic model is just an eight-port Ethernet switch, but NetVantage
ships a 100BASE-Tx uplink port. Although its FDDI module is
not due until early next year, IP fragmentation is already
built in. ATM modules are planned for 1996 as well.

Cisco Catalyst 1700

Because of its meager one address per port, the Catalyst 1700
is probably the most workgroup-oriented switch of the bunch.
But if you use its uplink port, you can connect it to the enterprise.
This switch, while short on management software (there are
no graphical tools) and packet filtering features (there's
no VLAN support), offers not only private Ethernet, but two
Fast Ethernet uplinks for connection to the server or campus
backbone at just under $160 per port. It's a solid, inexpensive,
no-frills workgroup switch.

Through a terminal menu system, we were able to configure the
port security and view statistics about traffic on them. You
can set the ports to bridge store-and-forward, fragment free,
or, as 3Com calls it, fast forward (cut-through to most). Port
mirroring is also available.

The Catalyst 1700 is a fixed-configuration switch with two Fast
Ethernet ports for servers. So, don't expect to upgrade this
with any other fast ports, such as ATM or FDDI. You will need
to move up to Cisco's Catalyst 2800 (which was not tested here
because it exceeded our price point) to get that added functionality.
Like the 2100, this switch will benefit from management under
CiscoWorks.

Black Box Ethernet Switch LB9500A

Keeping up with the big switches doesn't seem to be a problem
for this very basic switch, although Black Box's $250 per port
is high. Other than an optional redundant power supply, there
are no expansion slots for modules like high-speed ports. Basic
SNMP is included, but Black Box would benefit from a graphical
management tool like NetVantage's, rather than a simple MIB
definition (which didn't compile anyway)

Using a terminal to connect to the console, we were able to
configure the IP address of the bare-bones SNMP agent. Beyond
that, there was little to configure. Configuring bridging allows
for static address entries, which you can add manually or through
the listing of currently active addresses. We were able to
change the size of the bridge address table. If it's feasible
for your network, a smaller table means faster lookups.

Adding a graphical interface to manage the hub would be a great
improvement. As it stands, we were unable to compile the MIB
under HP OpenView or SunNet Manager. We even tried two different
"corrected" versions of the MIB.

D-Link DES-604 four-port 10BASE-T Switching Hub

With only four ports, this switch needs the help of a hub to
feed a workgroup. Luckily, the ports are designed for that
no crossover cable is necessary. Management is nonexistent,
but performance is better than most. The per port cost of $250
is steep for the features, but for $1,000 you get a cheap entry
into switching. The DES-604 will simply switch Ethernet and
blink LEDs; don't expect any management, filtering or fast
uplinks.

Using a method similar to 3Com's, the DES-604 minimizes lost
packets by "throttling" the input ports. This helps it achieve
a little better performance than the rest of the field, but
a little more buffer space may help it catch up with 3Com.
At more than 2,000 addresses per port, you'll need a new switch
long before the bridge table is full.

SMC EZ Switch-6x

EZ may describe the decision to bypass purchasing this switch.
Complete lack of configuration options, terminal or otherwise,
is only superseded in shortcomings by its absolutely poor performance.
This switch was not only unable to keep up with the others
in two-stream buffering, it couldn't even hold one wire-speed
stream. This is the Corvair of switches: "unsafe at any speed."

At $200 per port, we can't see any market for a switch with
such poor performance. Just pony up the additional $50 per
port for the SMC TigerSwitch or 3Com LinkSwitch and get real
performance and loads of features to boot.

Robert J. Kohlhepp can be reached at rkohlhepp@nwc.com.




<A HREF ="#HeadList">Back to Headline List</A><BR>
<HR>
<A NAME = "27"></A><IMG SRC="/pubgifs/ec40.gif"><BR><BR>
Copyright
NETWORK COMPUTING via NewsNet
January 15, 1996  Vol. 7 No.1

Putting Ethernet Switches To The Test


We tested throughput using Alantec's PowerBits, which can blast
any packet to any switch port and report where it ends up.
We used it to test the switches for up to six sustained streams,
and we tested the ability to buffer and flow control. The PowerBits
was only able to test Ethernet to Ethernet, so we couldn't
test the faster uplinks some switches offered.

Using the PowerBits, we streamed data from two switch ports
to a single port for 10 seconds.

That means we could have up to 20 Mbps headed for a 10-Mbps
outlet. Most switches kept the output port at wirespeed (10
Mbps), but 3Com and D-Link buffered enough to send more after
the input stream halted. They use an effective buffering scheme
that helps under short-term, heavy-load situations.

The throughput testing was not quite complete, since our PowerBits
equipment was capable of only six streams of data (about 90,000
packets per second at 64 bytes). This was just an exercise
in finding out if the vendors were truthful about supporting
wire speed on multiple ports.

We blasted as many streams as possible, depending on the number
of ports available on each switch (only two on D-Link, for
instance) for a duration of 10 seconds. With the exception
of the SMC EZ Switch-6x, all switches could handle wire-speed
quite handily. Less expensive doesn't mean slow.




<A HREF ="#HeadList">Back to Headline List</A><BR>
<HR>
<A NAME = "28"></A><IMG SRC="/pubgifs/ec40.gif"><BR><BR>
Copyright
NETWORK COMPUTING via NewsNet
January 15, 1996  Vol. 7 No.1

Classic Bridges vs. Switches



Although externally these switches appear to operate as bridges,
they handle ports and bridging tables differently. And they
offer bridging methods other than store-and-forward to reduce
latency. Traditionally, bridges were integral parts of the
network flow. They were expected to know every MAC address,
keep track of paths to those devices and prevent traffic looping
in the network. That isn't quite true anymore.

Workgroups or buildings are attached to the backbone through
the switch, so there's really only one port that talks to rest
of the world-the uplink port. Since the switch knows addresses
of all of the workstations connected to it, address tables
need only be as large as the number of workstations connected
locally. With classic bridges, packets destined for unknown
addresses are flooded on all ports. Switches know all of the
stations

on its local ports and only flood the uplink port.




<A HREF ="#HeadList">Back to Headline List</A><BR>
<HR>
<A NAME = "29"></A><IMG SRC="/pubgifs/ec40.gif"><BR><BR>
Copyright
NETWORK COMPUTING via NewsNet
January 15, 1996  Vol. 7 No.1

Middleware -- Driving Applications On the Network

By:
Bruce Robertson

Middleware is impacting network applications significantly.
Little, however, is understood about specific middleware products
and how they really work, mostly because a deployment and networking
perspective hasn't been used in evaluating middleware. Developers
evaluate middleware from the top down. Often, an application
developer is like the car driver who doesn't really want to
know how the car operates or is maintained. They just want
to drive it. They really only touch the steering wheel and
the pedals. A thorough perspective requires understanding middleware
from the bottom up, from the inside out and over time. Experienced
drivers know that a car has to be maintained, that some cars
work better on some roads than others and that an educated
driver is less likely to have accidents or breakdowns. While
the developer still does the network application driving, consulting
with a qualified network application mechanic will improve
the life of the application.

From the Driver's Seat Programmers essentially define middleware
by its Applications Programming Interface (API).  They want to
know how to use it.  This top-down view yields a characterization
of products by how they are accessed without necessarily
specifying what they actually do diagram "Middleware
Functionality The API Perspective").  Thinking that two cars are
very different because one is manual while the other is automatic
misses the point that underneath they might be mostly the same.
Nevertheless, these top-down categories are commonly used:

 Message-Oriented Middleware (MOM) products

 Remote Procedure Call (RPC)-based products

 Data access products

 Distributed Transaction Processing (DTP) monitor products

 Object Request Broker (ORB) products

Some products, however, provide more than one API, and vendors
are increasingly working together to offer multiple products
layered together as a suite. Each product has chosen a set
of services to offer underneath these APIs, and they are often
remarkably related even if the APIs are different.

MOM systems typically offer a very basic set of commands for
network communication-often as few as SEND and RECEIVE.  We're
not talking about "e-mail" messaging.  MOM products let programs
send data to programs in real time (or slower), while e-mail
infrastructures are still primarily for store-and-forward text
message exchanges between people.

MOM is about as manual, yet as general purpose, as it can get
for the programmer. Application developers create application
specific functions or routines built on these basic messaging
functions. As far as transports go, the interface is even simpler
than programming to a particular network transport API, like
TCP/IP sockets. Many message-oriented products exist, including
Covia Communications Integrator (CI), PeerLogic Pipes, Momentum
XIPC/Message Express, IBM MQSeries, Digital DECmessageQ and
NetWeave, but RDBMS vendors like Oracle and Sybase are getting
into the fray as well with products like Oracle Mobile Agents
and EMS.

RPC-based products are function oriented.  Developers define
their own application-specific functions using an interface
description language (IDL), and then compile that function into
the client and server stub code that actually does the
networking.  The application just makes normal function calls.
Developers essentially create their own APIs.  Many RPC-based
products only generate function stubs for 3GL languages like C,
but some support 4GL products like PowerBuilder.  While many 4GL
tools can call C functions, it is easier if the functions are
tool-specific.  When distributing existing host-based multiuser
applications, the RPC approach is very intuitive:  Each existing
function can be split across the network as needed-just recompile
to distribute a given function.  RPC-based products are fairly
automatic compared with message-oriented solutions.

RPC-based products come from many vendors, which includes Distributed
Computing Environment (DCE) standard vendors such as the major
Unix players (IBM, Digital, Hewlett-Packard and so on, except
Sun), along with Gradient, Open Horizons and others. Sun's
ONC RPC is implemented on many Unix platforms. NobleNet and
NetWise offer third-party products.

Data access products offer data-oriented APIs that reveal data in
tables.  Using a standard API (ODBC), applications get at remote
data using SQL.  However, most RDBMS vendors have proprietary
APIs in addition to ODBC, since those let them directly expose
unique RDBMS functionality.  If the application only needs to
have its data distributed to, and shared from, database servers,
data access works well.  Many tools already support this API, and
since no application specific programming is required on the back
end, it empowers ad-hoc application development.

Here, only the data can be on another node. With all the others,
whole pieces of the overall application can be split across
the network to run on other nodes. While the more recent development
of stored procedures provides some additional program execution
across the network, this is still limited to SQL data manipulation,
and it's RDBMS-specific at that. Besides the big RDBMS vendors
like Oracle (SQL*Net), Sybase (Open Client/Open Server), Microsoft
(DB-Library) and so on, third-party vendors like IBI, TechGnosis
(recently purchased by Intersolv), Cross Access, NetWise (recently
acquired by Microsoft), Neon Systems and many others offer
RDBMS-independent data access products.

DTP monitor products offer a middleware environment oriented
toward handling transactions over a network.  Transaction
monitors add BEGIN and END TRANSACTION semantics to the generic
SEND and RECEIVE.  By using DTP services, the application doesn't
have to include logic to assure transaction integrity.

DTP products are often built on top of message-oriented or RPC
based technology but add significant control and management
functionality. Most DTP vendors are working to implement new
X/Open standard APIs, but today, most have proprietary APIs.
DTP products interact with database resources via the X/A standard
interfaces supported by most RDBMS vendors. Products include
Novell Tuxedo, Transarc Encina, AT&T GIS Top End and IBM CICS.
From the developer's viewpoint, this is a manual transmission
with more gears, but the car has airbags to keep passengers
as safe as possible.

ORB products support network interaction between pieces of an
application with yet another programmer API:  the object Once the
developer starts working in an object-oriented tool (a 3GL, like
C++ or a 4GL), the easiest middleware would directly network the
objects created in the tool.  An ORB just lets the developer of
an application containing many objects easily partition those
objects onto different network nodes.  This is similar to the
RPC-based approach, except for object-oriented tools, not
function-oriented tools.

If the object-oriented developer does not use an ORB, the programming
job will be complicated by having to create the links between
objects and the functions or other APIs offered by the other
middleware solutions. The ORB world has many standards: the
Object Management Group's Common Object Request Broker Architecture
(CORBA), along with de facto standards like Microsoft's OLE
(which is waiting for Cairo before it gets distributed functionality)
and OpenDOC. Much of CORBA currently defines developer APIs,
although later versions are finally defining services to expose
via the APIs and even some interoperability options. ORBs are
available from large system vendors (IBM SOM/DSOM, HP, Digital,
NEC, AT&T GIS) and many third parties, including Iona (Orbix),
Expersoft (PowerBroker), PostModern (ORBeline) and NeXT.

Some development tools contain middleware components, such as
Fort , Dynasty, Seer and Antares Alliance Group (Huron ObjectStar).
Tool products with built-in middleware can usually be adapted
to use external middleware solutions.

Looking Under the Hood

Beyond hiding things from developers (and thus controlling diversity),
middleware takes care of things developers would normally have
to handle in the application. Middleware provides services,
and can therefore reduce the effort, time and cost required
to implement an application.

Application-oriented network mechanics don't focus so much on
what API a middleware product offers, but on the complex issues
surrounding the actual middleware services-specifically how
they impact the network and scale to the enterprise. Dependencies
and deficiencies often become apparent only during deployment,
just like you might only realize your car is guzzling gasoline
after driving it for a while.

Each middleware product can offer, or decline to offer, many
services across the network, but even if programmers notice
and appreciate them, they still may only see the service as
options in the API, instead of something to be deployed and
maintained. Take name services: Most data access middleware
has a name service that lists the RDBMS servers users can access,
and translates that name into an underlying network address
(an IP address or a DNS host name, for example). That's usually
enough for the programmer; the user will see names.

The networker notices that a given RDBMS' implementation for
naming lookup and address resolution is completely static and
based on tables that have to be manually maintained at each
client node. Only lately have RDBMS vendors offered a more
centralized and dynamic approach-one that is infinitely easier
to deploy and manage over time. The networker might insist
on using these more scalable implementations.

Moreover, each product is a mix of APIs and services, each unique
and possibly right for a particular application environment.
Some products even support multiple APIs and services. NetWeave
and ISIS, for example, support both message-oriented and data
access approaches.

Middleware products can also be layered on top of each another.
Many DCE RPC-based products use DCE's other services (like
directory and security). Open Horizons provides data access
over DCE, while Transarc Encina provides DTP services over
DCE. The Orbix ORB runs on top of other middleware (ISIS).
IBM has announced it will create a service provider API for
SOM/DSOM, so other middleware products like PeerLogic Pipes
can fit underneath.

Middleware Transport Services

Middleware is where the application hits the network. Like different
car models, different middleware products offer a variety of
ways to get the basic transportation job done. The largest
differences come in the model for interacting over the network,
including one-to-one interaction using request/reply conversations
or events, and one-to-many interaction using broadcasts and
publish/subscribe. Underneath those basic interaction models
lurk interesting differences between products.

All middleware supports communication in all directions, except
data access middleware where only clients can initiate conversations
and make requests. Data access interaction also typically places
the largest burden on network bandwidth, since large result
sets can be requested ad hoc by end users.

The typical request/reply model used by network transports,
such as TCP/IP, characterizes most RPC and data access middleware
 Ask a question; wait for an answer. This synchronous blocking
behavior suits procedural applications, where things happen
in a logical order, well controlled by a single code base usually
running on the client. The middleware makes sure that the request
and reply are delivered. Often, the middleware does little,
only passing the message directly down to TCP/IP or another
stack for reliable connection-oriented delivery. Conversations
are set up, data is exchanged and the conversation is torn
down. Such mechanics are most efficient if servers aren't overloaded
and if the client only talks to one server at a time.

Applications, however, are increasingly becoming event oriented.
Effective event orientation means ditching the conversational
model, since events require a more dynamic and flexible model
with lower overhead per interaction: non-blocking asynchronous
communication. Applications can't stop working while waiting
for events. Instead, events just happen.

Asynchronous communication is possible with multithreaded RPCs
(DCE using DCE threads, for example) but has proven much easier
with message-oriented middleware. Many message-oriented solutions
implement their asynchronous behavior by using the underlying
operating system's threads support or deliver it internally
for operating systems that aren't inherently multitasking (like
Windows 3.x). Messaging products like NetWeave and Covia CI
offer synchronous and asynchronous behavior.

So, the great "RPC vs. message-oriented middleware" debate comes
down to this: RPCs and messages really aren't completely distinct
options. They are different sets of services and APIs delivered
in typical packaging combinations. Most RPC implementations
are blocking and synchronous, and therefore not perfect for
distributed event-oriented applications.

An event-oriented application working over an asynchronous middleware
mechanism can exploit the parallelism of a network of resources
by sending off multiple requests to multiple servers up front
and accepting responses as they come back, rather than contacting
one server and waiting to hear back from it before continuing
to the next. User response time would improve since more things
get done at once. This is particularly well oriented to integrating
multiple legacy systems into a single end-user application.

Message-oriented solutions (and this would include ORBs, DTP
products and any other systems built on message-oriented infrastructure)
also typically offer connectionless services. Now, a message
doesn't have to require any response at all, even within the
underlying network transport layers. This is lower overhead
than having the middleware's own non-blocking functionality
implemented over connection-oriented transport protocols. It
does require more data per message to include connection-like
information, but since centralized resources no longer have
to keep up connections, they can be more heavily loaded. MOM
products, like PeerLogic Pipes, offer reliable yet connectionless
services over IP.

When message-oriented interaction is combined with queuing,
applications no longer have to be online at the same time (the
same benefit as with e-mail or voice mail). Clients and servers
can queue up messages for each other, and when an actual connection
is made, the messages are exchanged. Some MOM products work
with in-memory queues for performance, while others support
disk-based queues for reliability. Some, like NetWeave, use
a single queue (for best performance with least latency) while
others, like IBM MQSeries, use two (for highest availability
and reliability). This queued messaging lets applications function
well for the mostly disconnected user over low-speed or wireless
links, as well as for very heavily loaded servers (where the
queue enables session concentration and batch-like processing
typical of mainframe systems)

Most request/reply environments use point-to-point connections.
One device talks to another. A growing number of middleware
products, however, are taking advantage of broadcast and multicast
paradigms. In a stock trading application, for example, people
on a trading floor might want to track the cost of a given
stock on their workstations. A server could send that data
out 50 separate times in 50 separate messages to 50 separate
machines, or the 50 workstations could poll the server separately.
The most efficient way, however, is to broadcast it once and
let them all listen. Multicast is a special case of broadcasting
to more than one, but not all, nodes on the network.

These one-to-many approaches can provide interesting networking
implications, specifically because most routed networks don't
support broadcasting between subnetworks. Multicast may only
be supported on particular datalinks and network transports.
Teknekron's Rendezvous Software Bus, for example, offers extensive
broadcast functionality, for example, letting Microsoft Excel
users publish parts of spreadsheets anonymously for other users
to subscribe to independently over the network along with a
broadcast relay component to move messages across subnetwork
boundaries.

Middleware Transparency Services

A key benefit of middleware is how it hides differences between
deployment environments. Transparency must exist not only across,
but between, lots of different platforms. Take network transparency.
It's not just that the middleware API is simpler than the underlying
transport API, it's that the middleware makes differences between
multiple transports transparent to the application, and makes
for more ongoing deployment and maintenance flexibility.

Beyond transparency, middleware should offer network protocol,
operating system, programming language and even API independence.
Network independence, for example, means that two devices communicating
with each other need not have a single network protocol in
common.

Between operating system platforms, middleware normalizes data
format differences. Some use a single platform-independent
data representation on the wire (like XDR in ONC RPC or ASN.1).
Most let the "receiver make right" any differences, which means
the middleware has to know from which machines messages are
coming, but it still results in less overhead per message while
keeping servers pumping out natively formatted data at their
fastest clip.

Moreover, middleware can hide real differences between different
database systems (by supporting multiple vendor products like
IBI EDA/SQL does) or between different legacy application environments
(by providing procedural access to existing business logic
in CICS transactions, for example). All these resources look
similar to the application running on the client, although
some programming may be required to integrate legacy transactions.

Some applications will want full transparency from middleware
itself. In this case, programmers create their own meta-API.
Applications also gain middleware independence by using three
tier application designs that use different middleware between
different tiers of the application. SAP AG's R3, for example,
uses messaging between desktops and application servers, but
uses data access between application servers and database servers.

Middleware Directory Services

Directory services are a large differentiating factor between
products offering similar APIs, transport and transparency
services. Without some level of naming indirection, changing
service locations (server names or IP addresses, for example)
on the fly will be difficult after the application is deployed
to hundreds or thousands of desktops.

Some middleware products offer no help. Others rely on underlying
network transport-specific mechanisms (like DNS, NetWare SAP
or NetBIOS) for name lookup and address translation. These,
however, aren't consistent across all transports, nor are any
of them perfectly suited to the needs of applications trying
to name components dynamically. Still others offer their own
simple name services or full enterprise scalable naming.

Nowhere is this more pronounced than when comparing DCE RPC
with other RPC implementations. While most RPC products punt
on the naming service issues, DCE RPC is often implemented
with DCE's other services including directory services (cell
directory service) and security (Kerberos with DCE-specific
integration). Moreover, DCE's services are not simple naming
services, but complete authentication and data encryption services
delivered in a scalable way.

Contrast this with PeerLogic's Pipes naming service. Since there's
no security service, Pipes provides only name/address translation
services. Authentication is the application developer's problem.
However, its name service is well optimized for robust distributed
dynamic name services. A service can announce itself via the
directory and users can immediately tell that a new resource
is available. Applications can easily search the name space
for services they need. While it's not infinitely extensible,
the Pipes directory service offers some attribute fields that
can be populated as required for application needs, allowing
the application developer to type and otherwise identify services.

Middleware's use of external directory services can be platform
specific. Oracle, for example, now supports NDS in Oracle 7.2,
but only when the server runs directly on the NetWare platform.
If you need to run Oracle on Unix, you can't authenticate against
NDS.

Directory services enhance their basic location transparency
benefits with associated services like transparent load balancing
across services. Some middleware products offer their own directories
along with other mechanisms for load balancing (Tuxedo and
other DTP products). Reliable messaging products like ISIS
support load balancing. High availability and load balancing
go hand in hand with a good dynamic directory service.

Middleware Security Services

Directories are better deployed in conjunction with security
services designed to support a single system image. This is,
for example, when a user logs in once to the network and no
longer individually logs in over and over again to each application
or server.

Security functionality should include authentication and privacy
(encryption), data integrity and identification (perhaps using
digital signature technology) and non-repudiation (return receipt).
Not many middleware products do this internally, although DCE
does provide directory and security services in addition to
the RPC transport to handle most of these needs. Only recently
has Novell announced that the Tuxedo transaction monitor will
use NDS for directory and security needs (a product called
TransactionLink), but only on the NetWare NLM platform version.

While Kerberos (RFC 1510) seems like the external security system
of choice outside the NOS proprietary implementations, it has
only recently (in V5) supported public-key encryption solutions
like RSA. A new standard API called Generic Security Services
API (GSSAPI, defined in RFC 1508) should make it easier for
non-DCE middleware to use security services such as Kerberos
or any other service that supports that API. Other applications
that do not use the DCE RPC mechanism may still directly authenticate
using Kerberos. Suite Software's SuiteDOME ORB uses message
oriented mechanisms across the wire, but supports Kerberos.

Kerberos, however, isn't perfect. It doesn't, by itself, protect
from password-guessing attacks. Nor does it support hardware
based one-time passcode options like Security Dynamics SecurID.
In contrast, Oracle's Secure Network Services now supports
fingerprint recognition in its SQL*Net data access middleware.

Most middleware products offer no guarantee that authentication
will be done securely. Instead, user names and passwords move
between client/server in open text. Anyone with a network analyzer
could compromise your security. Beyond authentication, data
encryption is usually not offered.

Contrast this stark reality with the enhanced functionality
of some middleware products. DCE RPC-compatible applications
can flip a switch to turn on data encryption. Microsoft SQL
Server 6.0's new DCE RPC-based DB-Library supports both per
node and centralized enforcement. While SQL Server 6.0 uses
the NT domain services and not the DCE directory and security
services, authentication is still secure if you choose either
Named Pipes or DCE RPC transport connections.

Creating applications that access resources separately controlled
by different security systems (DCE, RDBMS, NOS, mainframe and
so on) can make things complex. The different systems must
be separately integrated or gatewayed by the middleware or
the application. Otherwise, users must log in to those resources
separately. Oracle's SQL*Net/DCE provides single system logon
using DCE directory and security while running over the DCE
RPC transport. Open Horizon's Connection offers database access
this way, but to more than just Oracle databases. Novell, in
contrast, has not decided if its Net2000 security services
based on NDS will gateway to DCE or NT domain security. So
far, it has only said it'll synchronize directories with those
systems in the future.

Most middleware products still expect the developer to make
security decisions and address those concerns outside the middleware.
Moreover, security control may need to be focused at many levels
 the conversation, socket or transport level (Kerberos, Netscape's
SSL); the document or message level (SHTTP, secure e-mail solutions
like S/MIME and PEM); or even the transaction level (DTP products,
EDI). A lot still has to be decided as networked applications
move from being deployed over relatively controlled private
networks into public networks where full security is a requirement.

Middleware Fault Tolerance Services

Some middleware is focused squarely on offering high availability
and fault tolerant support for networked applications. For
example, Tandem/ISIS offers fault tolerant message-oriented
solutions for reliable messaging, while Teknekron's similar
publish/subscribe mechanism does not.

Beyond specifically nonstop systems lies middleware designed
to assure transaction integrity despite failures, but without
making the application do all the extra work. An update may
not be made, but at least the middleware makes sure that complex
transactions are fully carried out, or completely backed out.
DTP products offload clients from the two-phase commit processing
that data access products require, and let an event-oriented
application get on with other tasks while complex transaction
processing is taking place elsewhere. DTP products are used
more often than data access middleware when high performance
and complete reliability are paramount. By depending on more
middleware services, applications can be smaller.

Distributed fault tolerant solutions require a naming or full
directory service to provide location transparency for service
providers. Client applications can't have hard-coded service
names or network addresses. They will break when a service
is moved to another machine. Some middleware solutions, such
as PeerLogic Pipes and ISIS, automatically start additional
services on new machines if loads get too high on one server,
or for fail-over if a server becomes unavailable.

For multiple instances of a duplicated service, most middleware
systems will provide some load balancing across them. However,
mechanisms vary from nothing (in PeerLogic Pipes) to a DTP
product's careful analysis of resource platform load.

While some of these differences can seem subtle, if you're driving
the applications on your network, it will pay to understand
how to exploit, deploy and maintain these middleware services.
Not only will you gain a better understanding of how applications
impact your network, you'll be able to steer your application
developers down the right service roads.

Bruce Robertson can be reached at brucer@metagroup.com.




<A HREF ="#HeadList">Back to Headline List</A><BR>
<HR>
<A NAME = "30"></A><IMG SRC="/pubgifs/ec40.gif"><BR><BR>
Copyright
NETWORK COMPUTING via NewsNet
January 15, 1996  Vol. 7 No.1

The Redeeming Qualities Of Val-Pak's Network

By:
Maureen Zapryluk

Everyone has received an envelope of coupons in the mail, offering
everything from free pizzas to dry cleaner discounts. Behind
these money-saving offers is a high-volume, high-speed network
that pumps out billions of coupons every year. "Twenty-four
hours a day, seven days a week, huge images are floated onto
the network," says Alan Robson, Val-Pak director of IS. Val
Pak Direct Marketing Systems, a cooperative, direct mail advertising
company based in Largo, Fla., has franchises across the U.S.
and Canada. Churning out countless coupons requires two facilities
in Largo-one for administration and one for production-which
are linked by redundant T1s to a graphics facility in Las Vegas.

Val-Pak's network connects 750 system components on an AppleTalk
and IP network. The underlying infrastructure is switched Ethernet,
FDDI and 10BASE-T. Remote users can dial in via Shiva's LanRover
from home or on the road, into 3Com ONcore concentrators and
Cisco routers. An Internet Web site, (http://www.valpak.com),
provides information about Val-Pak and posts test coupons for
market research. Val-Pak uses CE Software's QuickMail for internal
communications. A project to install Windows NT servers and
Windows95 on desktops is being evaluated, which also would
provide Netscape on Macs or PCs for e-mail and internal services.

"Our network goal is achieved when our customers are able to
take access to information for granted," says Robson. To create
a coupon, a sales rep takes an order from a vendor, then decides
what materials are required, some of which may be images stored
on graphics servers or need to be scanned into the system.
The artist designs the ad using Quark on a Mac. The dummy ad
rests on SPARC or Apple Workgroup servers while a four-color
proof is printed. Then it is sent to the sales rep via modem
or FedEx. The ad is then taken off the Rasterizing Image Processing
Server (RIPS) and is transferred to a film plate for printing.

Supporting this process are Sun SPARC 1000E servers that handle
120 GB of graphics applications and photo research databases.
Artists use a graphics application built around a Sybase database
running on a Sun SPARC. Apple Workgroup Servers contain multiple
80 GB of files of various graphics and data: order numbers,
reference files, past artwork, clip art, logos. Others included
are reference applications and Primac's Unidata business database
application. Val-Pak plans to set up individual Web sites for
database access.

"Managing fluctuations in network data traffic is a challenge,"
says Robson. "There are rush periods and low-volume periods.
Production is cyclical. There are short windows to gauge usage
or plan growth when traffic peaks at different times." Val
Pak is ready to integrate two RISC 6000 servers to store 30
GB of business applications and data, which will supplement
a Tandem system.




<A HREF ="#HeadList">Back to Headline List</A><BR>
<HR>
<A NAME = "31"></A><IMG SRC="/pubgifs/ec40.gif"><BR><BR>
Copyright
NETWORK COMPUTING via NewsNet
January 15, 1996  Vol. 7 No.1

Serving Up HTML Documents On The Web

By:
Scott S. Campbell, Robert J. Kohlhepp and Josh Linder

The World Wide Web is all the rage. It seems that around every
turn on the Internet, a company or organization has a Web presence.
If now is your time to create a presence on the Web, the first
thing you'll have to do is find yourself a server. Whatever
your environment may be, we're sure that one of these servers
will suit you.

To give you a snapshot of what's available, we've taken an in
depth look at 14 Web servers that span the most popular operating
systems and server platforms. We examined Macintosh OS, NetWare,
Unix, Windows 3.1 and Windows NT solutions.

Of the products we tested, the Macintosh-based ones are arguably
the quickest to install and configure, particularly with WebStar
as delivered on Apple's Internet Server Solution. However,
the well-designed installation routines of O'Reilly & Associates'
WebSite are intuitive with clearly thought-out defaults, which
make its installation almost as simple.

In general, Web servers running on one of the Unix operating
systems will be extremely stable and flexible, but these solutions
typically have higher costs associated with their products,
platform and maintenance. Because of the maturity of Unix,
many support tools used with Web servers are readily available
all over the Internet, including scripting, compilers for custom
applications, and precompiled binaries.

All products have a security angle. For example, one advantage
Mac solutions have over most Unix-based products is they don't
automatically index their directories. Hence, users don't have
the opportunity to surf the server's underlying files and directory
structures. Of course, tradeoffs exist. Not indexing directories
automatically means a slight performance hit, which could be
a problem for larger high-volume sites.

If cost cutting is a priority, many Web server options are available.
Solutions, such as NCSA's and W3C's HTTPd, are not only free,
but also can run on an inexpensive server platform, such as
on an Intel machine with Caldera's Linux. Although this solution
is far from maintenance free and will require an administrator
who is familiar with Unix maintenance and configuration, it
does benefit from the features that make Unix a strong, scalable
solution. On the other hand, if you're looking for a turnkey
solution, Apple's Internet Server Solution with WebStar is
one of the easiest solutions around.

Striking a balance among setup, features, maintenance and other
items was a challenge, but, in the end, Purveyor from Process
Software emerged on top. Macintosh-based WebStar from StarNine
fared well, as did WebSite from O'Reilly & Associates.

Process Software Purveyor 1.1a

Process Software's Purveyor 1.1a is the leading contender in
the Windows NT Web server market. Combining easy installation,
good documentation and strong utilities, Process Software's
flagship Internet product is leaps and bounds better than its
1.0 release.

Purveyor runs as a service under Windows NT, and like a number
of the other products in this review, it creates a program
folder with the configuration utilities to ease server management.
Purveyor adds value to Windows NT with features that show its
tight integration with Program Manager. The installation creates
a Purveyor menu and adds seven buttons to the toolbar for complete
control of the server and a Control Panel with service manipulation
options.

Purveyor's well-done documentation consists of three books:
Guide to Server Security, Programmer's Guide and a beefy 209
page User's Guide. The irony is that Process Software's utility
software offering is so easy to use that the extensive documentation
is superfluous. The software is compatible with the standard
CERN HTTPd, so migration of HTML documents, image maps and
CGI scripts is a breeze for Webmasters looking for a change
of server platform. Server configuration and management can
be done locally or remotely using Purveyor's Remote System
Management (RSM) atop a Web browser.

For users who wish to run a Web server under Windows95, Process
offers Process Personal Web Server for Windows95. Like the
NT version, this port is highly integrated with Microsoft's
newest mainstream OS. Look for NetWare and VMS products as
well as a secure version in an upcoming release.

StarNine WebStar 1.2.1

We were very pleased with the simplicity of setting up WebStar,
as StarNine and Apple have chosen to supply this product pre
installed on Apple's Internet Server Solution. As soon as we
took the PowerMac 6150/50 workgroup server out of the box and
powered it up, we could start configuring WebStar and adding
pages for our tests. It was that simple.

Configuration is accomplished via an administration utility,
which provides a simple interface for modifying server features.
Although the provided documentation is a little thin, you can
find detailed technical information on configuring your server
on StarNine's corporate Web pages.

Setting up our pages was very easy, and we were impressed that
we didn't need to make any modifications to page links whose
sources reside in the same directory. On some other server
platforms, such as with GLACI-HTTPD, we routinely had to modify
our image links to reflect where the pages were physically
stored in the directory structure. Hence, if you are supporting
multiple small pages, you can keep all of the relative components
in the same directory and let the server default to local naming
very simple, indeed.

Although not as pervasive as with many of the Unix-based solutions,
CGI support for WebStar is extensive in the third-party domain.
A quick look at StarNine's Web pages provides many sources
of CGI that are modules ready to plug into your WebStar server.
Of course, should that list not cover your specific niche,
you can easily use AppleScript to develop your own CGI application.

StarNine intends to develop both MacHTTP and WebStar products.
They consider MacHTTP to be an entry-level product with which
new administrators can get started. Then as their needs for
power and flexibility grow, administrators can move to WebStar,
which performs approximately four times faster.

O'Reilly & Associates WebSite 1.0d

The first software product from the book publishing giant O'Reilly
& Associates is solid, easy to learn and easy to implement
in a wide variety of scenarios. Although designed for Windows
NT, WebSite runs equally well on Windows95, only requiring
12 MB RAM, which is half the requirement of other NT-based
products.

Since O'Reilly & Associates is primarily a publishing house,
it came as no surprise that WebSite's documentation is of the
highest quality in our entire review. The 325-page Building
Your Own Web Site includes information on every aspect of the
software, and it made implementing every feature simple-even
for a person who has never touched CGI or image map tools.

WebSite is as elegant as it is extremely feature-rich. The two
floppy-disk setup was straightforward and similar to GLACI
HTTPD and the Mac servers, and it asked for minimal configuration
information during its installation. Once installed, WebSite
can be conveniently launched as a service, a service with a
desktop icon or as a desktop application. The strong configuration
utility, which can be run by right-clicking on the server icon,
is easy to use and has several security features.

O'Reilly uses an image mapping technique that's different from
the other products. WebSite stores the actual map coordinates
in the registry; most other products either internally run
an image map applet or perform this action through CGI. This
registry-based solution offers a perceptible performance gain;
however, it lacks CERN- or NCSA-standard .MAP files, consequently
forcing the administrator to create and maintain all map files.

Overall, WebSite is solid, but it lacks some of the more advanced
features of other products we reviewed. During our testing,
1.0d was the latest shipping release, however, 1.1 should be
available by the time you read this review. The new release
will include features on par with Purveyor 1.1a and CompuServe's
InternetOffice Web Server.

StarNine MacHTTP 2.2

MacHTTP is Chuck Shotton's shareware Web server for the Mac,
which was turned over to StarNine Technologies when Shotton
came onboard to develop WebStar. Touted by StarNine as the
most widely used desktop Web server on the Internet, this package
is distributed as a "fat binary" to run natively on both the
680x0 and PowerMac environments at a modest price.

The product is delivered via a single "Stuffit" file that can
be FTPed from StarNine's corporate Web server. No system extensions
or additional configuration is required to run, in contrast
to W3C and NCSA's HTTPd for the Unix platform, which require
significantly more tinkering.

Configuration of MacHTTP is accomplished via a simple text file
containing brief descriptions of server parameters along with
their set values. One problem with this server management approach
is that the administrator must take the server offline to modify
the configuration. This is handled well by configuration utilities
provided with several other solutions, including WebStar and
WebSite, which allow configuration changes on the fly.

Examining the contents of the configuration file may be somewhat
confusing to someone who has never managed a Web server before
and, unfortunately, the electronic documentation that comes
with the package doesn't offer much comfort in the way of details.
However, surfing through StarNine's Web pages reveals hefty,
readable discussions about each of the configuration options.
In this respect, StarNine's support is very similar to WebStar's,
which gives MacHTTP a higher level of utility and ease of use
than a thinly supported product, such as GLACI-HTTPD.

Third-party CGI support is available for MacHTTP and a list
of sources is located on StarNine's Web server. Although it
is not as extensive a list as that provided for WebStar, you
can still choose from several options. Of course, should you
not be able to find the routine you want, you can always resort
to AppleScript.

Netscape Communications

Commerce Server 1.12

By marketing a commercial server on the Unix platform in addition
to its popular browser, Netscape has set the technology market
ablaze. Priced quite a bit below Open Market's product, Netscape's
Commerce Server offers a few more features and a bit easier
installation. For the security conscious, the Commerce Server
is one of only two products in this review that support Secure
Sockets Layer (SSL) secure Web service technology.

Installation starts with a simple script, which then hands off
to your choice of form-enabled Web browser. Using the extensive
online documentation (a printed version is available from Netscape),
we were able to have the server up and running in a few minutes.
With the addition of a few configuration variables, we had
our CGIs, such as image maps, doing business on the back end.
Don't be misled: Netscape's bundle does not offer any CGI utilities,
so you're on your own for securing and installing these utilities.

As with many Unix-based servers, the Commerce Server is fully
capable of being a drop-in replacement for an existing NCSA
server. Since we installed the NCSA's server prior to Netscape's,
the Commerce server was very easy to install. With simple use
of the configuration interface, we were able to tell the server
to look in our existing directories for server content and
CGI script. Of course, with most Unix-based solutions, you
must do quite a bit of setup before a Web server can be installed.

CompuServe InternetOffice Web Server 1.0

When the largest online service company purchased Spry, one
of the promises was that CompuServe would have a cross-platform
Web server by the end of 1995. CompuServe is delivering, and
it has servers that run on several platforms: Windows NT 3.51,
SunOS 4.1.3, Solaris 2.3, BSDI, and HP/UX 9.3 and 9.4. This
range of platforms allows for an easy corporatewide implementation
of Internet services.

In its first incarnation, InternetOffice Web Server looks and
feels solid. The installation, based on InstallShield Wizard,
is complete and sets up the majority of static options, rendering
it unnecessary to change them in the future. It also sub-installs
ODBC drivers, as InternetOffice includes support for ODBC to
act as the middleware for CGI and BGI scripts. Additionally,
InternetOffice Web Server includes support for remote administration
through Windows95 clients, the Perl programming language and
support for multiple IP addresses.

CompuServe's offering ships with five utilities: two administration
tools, an uninstaller, a performance monitor and an event viewer.
The standard administration tool has intuitive Start, Pause
and Stop buttons for manipulating the server, and all of the
options can be set by clicking on tabs within the main window.

Quarterdeck Corp. Web Server 1.0

Quarterdeck's entry Web Server for the Windows 3.1 environment
is a pleasantly compact and user-friendly system. Shipping
on just two diskettes, the product installs easily and operates
over Novell's LAN WorkPlace TCP/IP stack-a definite plus for
anyone already comfortably immersed in a NetWare environment.

One of Quarterdeck's strongest features is its server administration
utility, which is significantly more intuitive than either
WebStar's or NetManage's Personal Web Server utility. It uses
a clear and concise graphical interface to configure all components
of the Web server, from server name and IP address to domain,
group and user-specific access rights. Additional configuration
allows for path aliasing of files and directories, in which
one identifier represents an entire path everywhere it is used
as well as utilities for page redirection, in which a single
identifier refers to a data path on another Web server. The
power in this feature is that the server can expand its capacity
by adding machines, and seamlessly maintain one point of contact
to the Internet.

During testing, we were concerned with problems that resulted
from repeated access to documents on the server, as connections
were lost quite regularly. In attempting to resolve this problem,
we were repeatedly confounded by busy signals on Quarterdeck's
dial-up support line at all times during the day. Quarterdeck
is taking measures to examine its phone support usage in terms
of length of call and product in question to increase availability.

Open Market Systems Secure Web Server 1.1

Open Market, like Netscape, eases installation by using a standard
Web browser to install and configure the server. Open Market
is the only product reviewed here that supports both SSL and
Secure HTTP (S-HHTP) for encryption. Using a multithreaded
process, instead of forking, Secure WebServer uses a little
less memory than most Unix-based forked servers. Like Netscape,
if you stick to a supported platform, Open Market offers service
to the basic product.

Also like Netscape, installation starts with a simple script
that sets up the administrative server, then using a Web browser,
other aspects, such as server name, content directory and server
listening port, are configured. Through more advanced configurations,
directories for CGIs and path aliasing may be added using simple
HTML forms. Unlike the other Unix Web servers, image maps are
handled internally to the server and must be defined in the
CERN format, which is a bit different from the de facto NCSA
format.

Open Market offers security for transferring sensitive data
over the Internet. Improving on Netscape, Open Market offers
both SHTTP and SSL. Although using both is somewhat redundant,
it does offer flexibility in choosing a browser that supports
either protocol.

Unlike most other Unix server implementations, Secure Web Server
is not a plug-in replacement for NCSA's HTTPd. If you are already
running an NCSA server, you may want to consider Netscape for
a secure server replacement. Beware: Open Market's script claims
to assist in conversion, but we encountered less-than-successful
results when attempting to convert our files.

Ameritech Library Services NetPublisher 1.0

Ameritech's NetPublisher provides a number of features aimed
at a narrow variety of organizations and libraries. Providing
custom installations, it's positioned with features geared
toward electronic publishing of library documents as well as
communicating electronically with other libraries. Although
average in most categories, Ameritech's price is far above
any other product we tested.

Ameritech's view of the Web as a vehicle for networking libraries
accounts for this product's focus on allowing easy implementation
of a library system. Since this system is positioned as a "publishing"
solution, rather than a "presentation" system, the server is
complemented by NetPublisher Editor. Unlike anything else in
our review, the Editor views Web sites as containing structured
documents, similar in nature to how hard copy works. Additionally,
NetPublisher is a Z39.50 server that simplifies Internet-based
SQL searches. Although Z39.50 isn't a feature requested by
the majority of Web users, it is one that is the framework
for linking libraries.

NetManage Personal Web Server 4.6

Personal Web Server by NetManage is delivered as one of approximately
38 IP-based components in its product Chameleon for Windows
3.1. Personal Web Server is well named. It's not well suited
to publish any significant number of home pages, but it does
provide the basic services that users might want on platforms
they already have at their disposal.

Once we had the Personal Web Server out of the box, we were
amazed at its serious lack of documentation. There was a fair
amount of documentation supporting the other IP applications
in the package, but very little regarding Web services. Every
source we turned to for technical information proceeded to
point us at simplistic examples in a self-explanatory air.
For the novice Web administrator, it is clearly insufficient.

Although Personal Web Server does function adequately, we couldn't
shake the feeling that it was somewhat of an afterthought in
respect to the entire Chameleon product. Fortunately, it did
include several of the basic configuration features that we
have seen in other products that made it function on a par
with similar systems.

GLACI GLACI-HTTPD 2.01

NetWare is arguably one of the better platforms for file sharing,
and it only makes sense that a file sharing operation such
as Web services would be implemented there. Although we anticipated
a solid server with features and tools comparable to several
of the Windows NT and Unix-based products, GLACI-HTTPD from
the Great Lakes Area Commercial Internet (GLACI) is a Web server
that's still developing as a commercial product. Fortunately,
the development team at GLACI is running full steam ahead.

We came away from the tests of this product feeling that GLACI
is putting a solid effort into providing a Web server solution
for the NetWare platform. However, it still needs some work
to develop into a mature product. For the NetWare-phile who
enjoys system tinkering and writing NLMs, or someone just interested
in very simple home page publishing, this product will fit
the bill adequately. If you need a little more guidance, stay
tuned to GLACI's Web site as more features and utilities are
on the horizon.

Although we did not test it, GLACI does have a secure version
of its Web server which combines the basic GLACI-HTTPD with
the SSL flavor of secure Web service. A fully functional demonstration
copy of the product is easily available from GLACI's Web server
via FTP and is delivered in a single .ZIP file.

NCSA HTTPd 1.5, W3C HTTPd 3.0

Free and stable servers are available on the Internet if you
are not concerned about data encryption or easy installation.
W3C and NCSA offer Web servers precompiled for various platforms.
Source code is available for almost any other platform.

Installation and configuration are not for the Unix newbie,
although the addition of CGI and other trinkets are just as
easy as any of the commercial versions. "Un-tar-ing" the NCSA
distribution reveals a precompiled HTTPd executable as well
as other helpers, such as image map and other utilities. W3C
does not include anything beyond the HTTPd. Normally, installation
is actually quite simple. But, since our /usr/local/bin directory
is NFS mounted, we needed to install elsewhere.

Configuration of these freeware servers mirrors their commercial
counterparts, but graphical administration is not an offering.
Instead, a few configuration files will need to be edited.
For the average installation, these files are quite simple
to work with and include many examples and defaults that indicate
the proper formats.

Both products lack a secure mode, such as SSL or SHTTP. Therefore,
they are quite inappropriate for online sales or other sensitive
data. They do make an excellent choice for internal information
servers or other general data distribution servers.

Folio Corp. Infobase Web Server for Windows NT 1.0

Folio's Infobase Web Server is an easy-to-configure Windows
NT product that has an advantage over competing products: It
allows Internet access to "Infobases," which are Folio VIEWS
3.1 databases. However, this difference has made Folio's Infobase
a niche product lacking in a number of key areas.

The two-disk installation of Infobase Web Server was straightforward
and very similar to other Windows NT and Windows 3.1 products
tested. Resource requirements are hefty for Windows NT 3.5,
requiring a Pentium processor with 24 MB of RAM. Once the product
was installed, the configuration utilities were of little help
and paled in comparison to the bundled goodies and configuration
utilities from Process, Netscape and StarNine. Although Infobase
Web Server runs as a Windows NT Service, it conveniently places
an icon on the desktop for ease of stopping, or killing the
server.

Folio is aimed strictly at organizations with users who require
access to existing FolioVIEWS Infobases. If an NT-based, Web
to-database access is a requirement, although expensive, this
may be your product.

Scott S. Campbell is a network systems analyst at Syracuse University.
He can be reached at sscampbe@syr.edu. Josh Linder is a network
consultant at Syracuse University. He can be reached at jslinder@
syr.edu. Robert J. Kohlhepp can be reached at rkohlhepp@nwc.com.




<A HREF ="#HeadList">Back to Headline List</A><BR>
<HR>
<A NAME = "32"></A><IMG SRC="/pubgifs/ec40.gif"><BR><BR>
Copyright
NETWORK COMPUTING via NewsNet
January 15, 1996  Vol. 7 No.1

Web Servers: A Status Update



If there is one thing we noticed during our review, it is that
those companies that provided detailed explanations about configuration
and usage, either in hard or soft copy, undoubtedly ended up
with products that were easier to install and administer. Having
documentation to back you up is essential in taking on any
new project, such as installing and maintaining a Web server.
Those products that ignore this element, we feel, are destined
to no more than runner-up status.

In the battle of complexity versus ease of use, there is no
clear winner. It depends largely upon a potential administrator's
background and how they like to work. Many people might prefer
a system that is exceptionally easy to set up and maintain
in exchange for a smaller third-party support structure and
resides on a familiar computing platform. On the other hand,
we imagine that there are equally as many people who like the
challenge of diving into a new project that will require develop
ing a whole new set of technical skills on a system that is
highly customized. Either way, there are good solutions out
there.

Overall, we found most of the Web servers we evaluated to all
be in a developmental phase. In one or more areas we found
weaknesses that were either addressed by other products or
were perhaps available now as add-ons, but they should be part
of the product in the long run. One example is in server access
logging and reporting. Without exception, every product created
standardized log files sequentially detailing every transaction
performed by the server. Unfortunately, none of the products
included any utilities for scrutinizing that log file to extract
useful information. Three examples we can envision are: Reporting
on the number of hits to the server from each client domain;
reporting the number of hits on a given page hosted on the
server; and some kind of selected cross-reference of the previous
two. Clearly, these are statistics that any commercial service
could find immediately useful, let alone the curiosity of the
casual site administrator.

The good news is that most of the vendors we spoke with are
actively developing their product lines as this goes to print.
Several of the companies have new servers on the horizon, and
we have every reason to believe that Web servers will soon
come of age as truly mature and robust products.




<A HREF ="#HeadList">Back to Headline List</A><BR>
<HR>
<A NAME = "33"></A><IMG SRC="/pubgifs/ec40.gif"><BR><BR>
Copyright
NETWORK COMPUTING via NewsNet
January 15, 1996  Vol. 7 No.1

How We Tested: Hardware And Operating Systems



Our testing of the listed products obviously required a diverse
set of hardware platforms to support the various operating
systems. Although we made no cross-platform performance tests,
we thought we'd share the configurations we used for our evaluations.

Macintosh OS System 7.5.1: We actually ran two different Macs
for the testing here. WebStar was delivered on an Apple Workgroup
Server 6150/66 with 24 MB of RAM. This machine was more than
adequate for the task and could probably be scaled back for
a small Web site. For those who want to take WebStar to its
limits, Apple will be introducing a PCI-bus version of the
Internet Server Solution this spring.

For testing MacHTTP, we chose what would be considered a "middle
of the road" solution by today's standards-a Quadra 950 with
24 MB of RAM. Although faster than many of the Macintoshes
still in use in the population, and compared with the Power
Macintoshes today, it seemed like a fair compromise. It also
gave us a chance to test the "Fat Binary" claim and confirm
that MacHTTP will run on both platforms.

NetWare 3.12/4.1: GLACI-HTTPD was tested on two separate platforms
to confirm compatibility with both NetWare 3.12 and 4.1. As
a departure from our other platforms, we selected a generic
486/33 MHz with 16 MB of RAM. We felt that there were probably
many sites with comparable equipment collected over the past
few years, and that this was representative of most of those
systems.

For our tests under NetWare 4.1, we used a Dell Dimension XPS
P90c with 32 MB of RAM. With its 90-MHz Pentium processor,
there were plenty of CPU cycles to go around, between running
NetWare 4.1 and

GLACI-HTTPD. Of course, we did not have additional modules running,
such as NetWare NFS, ATPS or something else. Had that been
the case, we probably would have needed to review the memory
requirements a little more closely.

Unix Environment: Our four Unix-based products were tested in
Solaris. Netscape's Commerce Server, NCSA HTTPd, W3C HTTPd,
and Open Market Systems Secure Web server, were tested under
Solaris 2.4 on a SPARCstation IPC. Although this machine is
not fast, it had 48 MB of RAM and should prove to be an adequate
platform for smaller sites with few CGIs. For busier or more
complex configurations, a Sun SPARC 20 class machine may be
in order.

Windows 3.1x: For our Windows 3.1-based products, we used a
Dell 450/ME with 16 MB of RAM. This machine was set up for
booting multiple discrete windows environments, and it was
easy to configure in several different circumstances. Although
many people are probably running both faster, more powerful
machines, we felt that this was a decent example of what is
likely a solid "middle-of-the-road" platform by today's standards.

Windows NT Server 3.51: For the Windows NT environment, we utilized
a Dell PowerEdge XE590-2, a dual-Pentium processor machine,
with 64 MB of RAM. This was a solid machine that gave us zippy
performance, not only while serving up Web pages but also when
switching configurations between products. We don't actually
consider this as a recommended first server, but it may provide
the kind of performance that a heavily used commercial site
might require.




<A HREF ="#HeadList">Back to Headline List</A><BR>
<HR>
<A NAME = "34"></A><IMG SRC="/pubgifs/ec40.gif"><BR><BR>
Copyright
NETWORK COMPUTING via NewsNet
January 15, 1996  Vol. 7 No.1

Network Analyzers Race To Capture Fast Ethernet But Need To
Rev Their Engines

By:
Bruce Boardman

Fast Ethernet is pulling away in the 100-Mbps network race,
with infrastructure vendors turning out hubs, switches and
NICs with abandon. While there's nothing in a Fast Ethernet
packet to decode that is any different from a 10 Mbps Ethernet
packet, those Fast Ethernet packets sure do go by awfully fast.
We tested five analyzers for Fast Ethernet in our Syracuse
University lab. The software products included Novell's LANalyzer,
Intel's LANDesk Traffic Analyst, The AG Group's EtherPeek and
Cinco's NetXRay. At the top end, we pulled the wrappers off
Network General's hardware-based analyzer and ever-popular
Sniffer, now cruising at 100 Mbps.

A direct comparison among these products, however, wouldn't
exactly be fair. Their approaches, the market segments they
target and even the platforms they run on differ greatly. Our
goal was to get a sense of how today's offerings perform. While
we awarded Network General's Sniffer our Editor's Choice because
it clearly stood above this crowd, this market is still emerging.

Network General Fast Ethernet Sniffer

Nothing could compare to the Sniffer. HP is focusing on VG AnyLAN
(surprise, surprise). Azure and Comtest don't have products
yet. Wandel and Goltermann submitted a DA-30, but felt that
it was more of a development and benchmark tool. We agree.
And the software-based products just can't keep up.

However, in many respects, this new Sniffer is a little brother
to the 10-Mbps version. The interface, capture, decode and
performance are similar. And setting up offset filters and
decoding them is no different. But this new version has some
growing to do.

It's missing the ability to run the expert system while capturing
packets, for instance. When we tried to run the expert system,
the Sniffer displayed a message saying that it was dropping
back to classic, or normal mode. Network General says this
version of the expert system, ported from the 10-Mbps product,
can't run reliably online.

All is not lost, however. You can run a captured trace offline
through the expert system, and the Sniffer replays the trace
at a speed that won't overrun the processor. That's great,
in theory, but in practice this meant taking lots of little
capture snap shots, since the capture buffer without filters
fills rather quickly. Filtering cut down on the amount of traffic,
but then we weren't always sure we would know enough about
a reported problem to start off troubleshooting with a filter.
This whole process limits one of the Sniffer's most powerful
tools.

Also, historical statistical sampling is not part of this version,
but Network General plans to add it, along with online expert
processing to the v5.0 release due out around May of this year.
This release is scheduled to be a major upgrade in performance
and feature set. Upgrades to release 5.0 will be free.

The Sniffer's performance really stood out, only dropping a
couple of packets when we blasted 64,524 packets per second
(pps) on the wire-a high rate, despite only 45 percent utilization.
In a less stressful test, measuring statistics only, we stepped
utilization up to 100 percent just to see how accurately the
Sniffer measurements were, and at what point the user interface
locked up. In this scenario the Sniffer accurately measured
utilization to about 98 percent of the 100 Mbps maximum without
locking up.

Cinco NetXRay

Cinco's NetXRay is the '90's software analyzer. To be more precise,
it's the Windows95 analyzer. And it's the only one running
on Windows95 as of this writing. We've made much of how well
NetXRay compares to Novell's LANalyzer (see "NetXRay Exploits
Win95 for Protocol Analysis," October 15, 1995, page 50), but
while Novell hasn't changed much in the LANalyzer recently,
Cinco has had two releases of NetXRay since November 1995.
Cinco is trying harder and winning!

Cinco certainly improved its IPX stack decode, adding NCP since
our last review, but it left out burst mode capability, the
protocol more IPX networks are running.

Windows95-enhanced NetXRay's usability by lending consistency
to the interface. It borrows from Novell LANalyzer's dashboard
theme, displaying packets per second, utilization and errors
with speedometer-like gauges. It also extends this approach
to the capture display. These displays also offered the typical
Windows95 tab that showed us traditional tables of details.

Windows95's multitasking also enhanced NetXRay's responsiveness
under load. When we looked only at statistics and ran up network
utilization, the interface continued to respond up to about
35 percent-much higher than the other software analyzers. We
could tell there was a load on the wire, but we didn't have
to reduce the load to see the results, move the cursor or open
other processes.

In our capture performance test, NetXRay turned in the best
results of the Intel platform-based products. What is not shown
in the performance graph on page 114 is that even though NetXRay
did not capture all packets at 25 percent utilization (36,232
pps), at 20 percent utilization (29,068 pps) NetXRay captured
95 percent of the controlled packets.

AG Group EtherPeek

EtherPeek is the best product that you shouldn't buyyet. Yes,
it's a superior low-end analyzer. It's fast, well designed,
decodes the heck out of Fast Ethernet and has the lowest price
in town. But hold onto your green if you're looking for a 100
Mbps Fast Ethernet analyzer, because EtherPeek isn't as good
as it will be.

EtherPeek's modular design is one of its best features. You
can add functions to the analyzer engine by adding parameter
files. For instance, the version we tested let us decode 100
Mbps Ethernet by adding a hardware interface file for a Farallon
Fast EtherTX-10/100 card. But the modularity can also get in
the way of EtherPeek being a full-featured analyzer. For instance,
EtherPeek has added the ability to capture traffic from a 100
Mbps Fast Ethernet segment, but hasn't updated the analyzer's
ability to calculate statistics. The statistical elements reported
and the format of the display was good up to 10 Mbps, but utilization
rates beyond that produced garbage. AG Group is aware of the
discrepancy and will correct it in its next major release,
due out some time in the first quarter of this year.

The decodes and filters are deep, flexible and customizable.
EtherPeek comes with more than 450 pre-set offset filters that
you can modify. The decodes use the usual color and indentation
to distinguish stack and protocol information, and they were
surprisingly compact and easy to read.

You can add decodes, like the filters and hardware interface
files by simply adding the appropriate file. The AG Group has
a unique policy of making upgrades and new decodes available
for no charge on its Web site. Users can also create their
own decodes, using examples in EtherPeek's Decoder Mechanisms
Document, which explains how to set up unique decodes.

EtherPeek's intelligent threading, which is an expert-like ability
to trace upper-layer session traffic of AppleTalk, TFTP, SMTP,
NFS, SNMP and NCP sessions, is a unique feature among the software
analyzers. The AG Group will add duplicate IP address monitoring
to the next release.

The failure to calculate statistical percentages didn't keep
EtherPeek from performing well. We ran it on a 6150 Power Macintosh
with the Farallon TX 10/100 Ethernet NuBus card. We were surprised
how responsive the interface remained into the upper 20 percent
utilization rates. Given that the 6150 and the NuBus don't
have quite the horsepower or throughput the 90-MHz Pentium
and Intel 100-Mbps PCI card have, EtherPeek will likely provide
better performance on a faster platform.

EtherPeek's capture performed well compared to others. It captured
100 percent of the control packets transferred up to 9,434
pps, representing six percent utilization. Eighty-eight percent
of the packets were captured at 10,416 pps, representing seven
percent utilization. Occasionally it captured all the packets
at this utilization rate. This put EtherPeek solidly behind
NetXRay but well ahead of LANalyzer.

Novell LANalyzer for Windows

This veteran software analyzer is a proven troubleshooting tool
designed to make network analysis easy. LANalyzer is the Chevy
of analyzers. It is the most intuitive analyzer on the market.
Its dashboard-like gauges and simple flat menus are easy to
figure out. Novell helps the neophyte get started with context
sensitive troubleshooting help, a tutorial and a book on baselining
and troubleshooting (see our review "Software Analyzers Bring
Right Features, Right Price," July 1, 1995, page 116)

As a true software analyzer relying completely on the ODI stack,
did LAN-alyzer have the guts to perform at 100-Mbps speeds?
In last year's 10-Mbps tests, we were surprised how well it
did, but the 100-Mbps world was another story. When we ran
utilization above the maximum in the 10-Mbps world, LANalyzer
set off alarms. We found there was no way to increase the alarm
thresholds beyond the 10-Mbps limits, and no way to turn alarming
off. This was a little frustrating since utilization measurements
of 100-Mbps traffic were correct.

Capture performance was poor considering that LANalyzer had
an unfair advantage with its limited filtering capability;
since the LANalyzer cannot set pre-capture offset filters,
it couldn't capture IP sessions as part of the performance
load, which is one important way we tested the other analyzers.
But still, it only captured 100 percent of the packets at up
to 4 percent utilization (6,408 pps), which was only better
than Intel's Traffic Analyst.

The performance results displaying utilization were mixed. On
the positive side, LANalyzer correctly reported utilization
to about 70 percent. But the user interface began to become
sluggish at about 5 percent utilization and completely unusable
at 10 percent. We had to remove the network load before mouse
and screen updates returned. In our test environment, it really
wasn't a big deal, since we can just turn the load off. But
in a real environment it means having to yank the wire.

Intel LANDesk Traffic Analyst

The Traffic Analyst has all of the components to be a good tool.
The user interface is easy to understand and use. Statistics
and capture features are solid. Filtering is sophisticated
and flexible; in fact, the entire product is. But we can only
recommend it for very lightly used Fast Ethernet segments:
Its performance at high speeds is practically unusable.

The initial user interface displays statistical station utilization
tables and graphics that gave us a quick idea of how the network
was doing. There are three unique thermometer-like gauges that
slide up and down with variations of packet rate, utilization
and errors. They also feature little pointers that indicate
traffic high water marks. The approach was good, but like EtherPeek,
it did not scale to 100 Mbps.

Traffic Analyst's statistical design is for traffic loads up
to 10 Mbps. For example, Traffic Analyst topped out at 8,000
pps-a significant rate in the 10-Mbps world, but completely
undersized for the 100-Mbps rates that can easily reach 20,000
or 30,000 pps, and max out at over 114,000 pps.

Utilization rates were much better, accurately displaying into
the 60 percent range before producing incorrect results. The
user interface began to slow down during statistical measurement
at 21 percent utilization.

The user interface didn't do so well under the load of our offset
capture filter, becoming sluggish as utilization approached
10 percent. We scaled back traffic to a load range of 6,408
pps-that of a heavily utilized 10-Mbps Ethernet network-but
the Traffic Analyst could not keep up. We had to run utilization
down to 3 percent utilization (5,522 pps) to capture 100 percent
of the packets. If you capture a packet with the Traffic Analyst,
be thankful!

Bruce Boardman can be reached at bboardman@nwc.com




<A HREF ="#HeadList">Back to Headline List</A><BR>
<HR>
<A NAME = "35"></A><IMG SRC="/pubgifs/ec40.gif"><BR><BR>
Copyright
NETWORK COMPUTING via NewsNet
January 15, 1996  Vol. 7 No.1

How We Tested Fast Ethernet Analyzers



We tested Fast Ethernet analyzers using a stressful offset capture
filter and increasing network traffic to find out just how
well each was able to deal with the loads imposed by 100-Mbps
traffic. We used Netcom Systems' X-1000 Fast Ethernet Tester
a development/test device that can deliver consistent utilization
loads well over the 148,810-pps Fast Ethernet maximum, with
inter-packet delays as small as 40 nanoseconds. It is also
capable of extremely detailed test scenarios for all kinds
of Fast Ethernet devices. As a control, we used a Wandel and
Goltermann DA-30 equipped to test and analyze 100-Mbps Fast
Ethernet. It verified utilization rates and generated background
traffic.

The platforms the analyzers ran on varied. All of the Windows
based analyzers ran on 90-MHz Pentium processors with 32 MB
of memory and Intel Fast Ethernet PCI cards. EtherPeek, our
only Mac-based analyzer, could not run with PCI due to some
unresolved performance issues with Apple's Open Transport PCI
API, which is holding up The AG Group's release of a PCI interface
file (Apple is addressing these performance issues and by press
time will likely have them resolved). Network General delivers
the Sniffer on a 100-MHz Pentium processor with 32 MB of memory
and a specially designed Cogent PCI Fast Ethernet card, to
which Network General specifically tailors its code.

For the most part, all the analyzers were able to correctly
measure utilization to about the 85 percent range, but their
user interfaces either froze or became unusable at much lower
utilization rates. The exception was the Sniffer, which remained
consistently responsive in the 90 percent range and accurately
displayed utilization above 95 percent.

An analyzer's primary job, however, is to capture packets. So
we set each analyzer with a pre-capture offset filter to capture
packets between two IP addresses. To force each analyzer to
examine each packet, we loaded the wire with 64-byte broadcast
packets. We transmitted the broadcast packets at a steady rate
and then transferred a controlled number of packets that met
the offset pre-capture filter.




<A HREF ="#HeadList">Back to Headline List</A><BR>
<HR>
<A NAME = "36"></A><IMG SRC="/pubgifs/ec40.gif"><BR><BR>
Copyright
NETWORK COMPUTING via NewsNet
January 15, 1996  Vol. 7 No.1

20 V.34 PC (Politically Correct) Card Modems Can't Change The
Weather

By:
Bruce Boardman

Looks and features don't account for much with these little
rectangle flat pieces of metal, but the performance packed
inside sure does. Living just a bit longer than we care to
admit with 20 PCMCIA modems, or to be more politically correct,
PC Card modems, we found a large spread in the way that they
performed.

Modems can't do much about the weather (I'm sure a napkin somewhere
holds the beginnings of a "Barometer Jack"), but they had better
be able to deal with it. Phone lines, like the weather, change
from one connection to the next, and the better modems are
able to deal with these changes. In setting up the environment
to test V.34 PC Card modems, we wanted to make the test as
real world as possible. Since we couldn't recreate sun spots
or hurricanes, we settled on most of the possible combinations
of phone connections in the good ole "U.S. of A."

It is important to test modems over varying conditions, so testing
over a single connection type, (that is, over a PBX), while
consistent, does not fully expose the smarts or lack thereof
within each modem. Of equally little value is testing that
begins with a clean phone, to which gradual increments of noise
are added. While some modems may be able to withstand higher
error rates, error correction alone does not a modem make.

Modem manufactures have to balance speed, equalization, error
correction and compression variables, to mention a few, to
blend the perfect modem (make that an "Olive Jack," shaken
not stirred). The best test of this blending is of course how
well it works from the places that you call from, over the
phone lines that you use. And so, that is what we have done.

We ran a battery of tests that accurately measured how often
and how fast each modem performed over varying phone line conditions,
which represented more than 90 percent of the possible connections
that could be made in the continental U.S. The fastest modems,
with the most robust features typically cost more, but high
price didn't always mean a modem would do well. In fact, not
all of the expensive modems were fast or full featured.

Our winner, Practical Peripherals' ProClass 288 V.34, turned
in a close second in performance and edged out the speed kings,
Motorola's Marquis and 3Com's Etherlink III 3C562TP, with a
price that was hard to beat. Another worthy opponent, TravelCard's
288P from Microcom, turned in almost the same performance as
the ProClass 288 V.34, but it lost out because of a higher
price point.

At the bottom of the heap, the Toshiba NoteWorthy NW288CR, failed
to complete successful connections during one test-unfortunately,
this particular test totaled nearly a quarter of the possible
connections (24 percent) in the continental U.S. While not
at all failure prone, the Xircom PS-CEM28 consistently transferred
at the slowest rates in both test suites.

Bells and Whistles

Overall, the modems we tested share most of the same features,
with a few good ones that merit special mention. Cellular capabilities,
offered by the Apex Data V.34 Cellular Data Fax modem, EXP
Computer ThinFAX 288C, Toshiba NoteWorthy NW288CR and Simple
Technology 28.8 Communicator, all boast compatibility with
leading cellular phone manufacturers, the likes of which include
NEC, Mitsubishi, GE and AT&T.

Although not specifically covered in this review, four vendors
shipped us products that were actually combination V.34 modem
and 10-Mbps Ethernet cards. These included ActionTec's ComNet
28.8, Motorola's Marquis, 3Com's EtherLink III 3C562TP and
Xircom's PS-CEM28. Combo cards are a good way to save a slot
and often cost less than purchasing these products separately.

Finally, a couple of unique jacks were displayed by our participants.
MultiTech Systems' MT2834LT has a detachable, very compact,
hard plastic "Cool Jack" dongle. Besides being small, its second
RJ-11 jack provides the ability to plug in a telephone-a nice
feature when you're staying somewhere with only one phone line.
The spring loaded retractable XJack on the Megahertz XJ2288
is really cool and means you don't need a dongle. The XJ288
also protects the modem from burning out with its digital line
guard, should the modem accidentally be plugged into a digital
phone jack.

Less Is More

Most PC Cards, because they run on laptops with limited batteries,
claim to improve battery life by decreasing their power draw.
The NewMedia V.34 Net Surfer, EXP Computer's ThinFAX 288C and
Angia's SafeJack 288 displayed this trait and had the lowest
operating draw measured at 160 mA. When not in use, these cards
can also hibernate in an idle mode. Again, Angia's SafeJack
288 gobbles up a mere 2 mA in idle mode. One last power mode,
sleep mode, takes effect when your laptop is suspended. While
a number of products could survive with power draws of 2 or
3 mA, Xircom and NewMedia required no power at all.

The power-hungry cards were lead by Zoom's V.34C, sucking up
790 mA in operation mode-even more of a power drain than any
of the combo cards. The Xircom PS-CEM28 more than made up for
its zero power draw in sleep mode by gobbling up a significant
410 mA in idle mode-also more than any combo card or otherwise.

Performance Results

Motorola's Marquis and 3Com's EtherLink III LAN/Modem Card were
the best performers, hands down. Not only were they fast, but
they made 49 percent of all possible connections for the random
file test, transferring at or more than 9,000 characters per
second (cps). This was coupled with a low 1 percent ratio of
failure for both performance tests. This pair proved that they
could push the envelope to the edge.

On the other side, the Xircom PS-CEM288 was consistently very
slow. For both the TSB-38 and the random file performance testing,
all of the possible connections, which equal 90.5 percent of
the possible connections in the continental U.S., transferred
below 6,000 cps. Some of the transfers were even as slow as
2,000 cps. Although a pokey performer, the PS-CEM288 is reliable,
not dropping a single connection throughout our testing.

There were a few leaders in the dubious category of failed connections.
Apex Data's Mobile V.34 Plus Cellular Data/FAX had 14 percent
of the possible connections failing when transferring the TSB
38 file. MultiTech System's MT2834LT failed on 12 percent of
the possible connections when transferring the random file.
While these numbers are bad, the worst of all was Toshiba NoteWorthy's
NW288CR, which failed 19 percent of the possible connections
when transferring the TSB-38 file and 24 percent of the possible
connections when transferring the random file.

It is interesting to note that some of the modems support a
DTE rate higher than 115.2 Kbps. While it might seem that higher
rates, like the 300 Kbps boasted by both Microcom modems would
provide an advantage, it wasn't included in our testing. Primarily,
this is because the V.42bis compression implemented by all
of these V.34 modems will only compress to a maximum of 4:1.
This means that a very compressible file will only see a four
times 28.8 Kbps, or 115.2 Kbps transfer rate. Although it is
possible to compress files with a better than 4:1 ratio using
proprietary compression algorithms, to squeeze a compressible
file better than V.42bis sacrifices interoperability-it is
usually only a option in environments where modem interoperability
is not required.

TIA

And I don't mean Maria. The Telecommunications Industry Association
(TIA) issues Telecommunications Systems Bulletins (TSBs) that
give engineers something to do on their day off. Two of these
TSBs, numbers 37 and 38, are of particular relevance to the
issue of testing modems in the real-world way that we described
above.

TSB-37A is a painstaking account of each synapse in the Public
Switched Telephone Network (PSTN). It describes all of the
possible switched connections in the continental U.S. Boiled
down, this consists of seven different local loops (phone company
lingo for the phone line that goes from your place to their
local equipment), and 24 different network connections (phone
company connections between phone company offices). This builds
a matrix of 168 different possible connections.

So, if you think that each modem needed to be tested 168 times,
you'd be wrong! (Sorry, couldn't resist.) Actually the likelihood
of getting one of the 168 connections is not random when looking
at the U.S. as a whole. The TSB-37A describes how often each
of the 168 possible connection is likely to occur.

TSB-38, the next document in the TIA series, explains how (in
great detail) these test should be performed. We turned to
the leaders in this field in order to accomplish this precision
 Telecom Analysis Systems (TAS) of Eatontown, N.J.

With the TAS system we were able to selectively test and accurately
record very specific tests. We configured the equipment to
test all connections that had a likelihood of occurrence greater
than .38 percent (yes that's less then one half of one percent).
In this way, we were able to test 36 different connections
representing just under 91 percent of all possible connections
in the continental U.S.

What is important is that each of the possible connections tested
indicates not only how a modem is going to perform, but how
often it does.

The Tests

We transferred files in a single direction over each connection
while the modems were installed in a pair of Compaq Contura
410CXs. We used two different files, one defined in the TSB
38 and one randomly selected, both with text-like and similar
compression characteristics as measured with PKZIP. We set
the modems with error correction, compression and autobauding
to measure the performance of all of the balancing elements.

Originally we had added the second file in response to a vendor's
concern that modems with larger V.42bis dictionaries were given
an unfair advantage when testing using the TSB-38 files. The
vendor's contention was that a quirk of the TSB-38 file gave
unfairly boosted compression to those with larger dictionaries,
and thereby boosted overall modem performance. He claimed that
some modem vendors were adding to the dictionaries in order
to win magazine reviews. Imagine that! Our tests not only did
not substantiate this, but in every case, modems with big dictionaries
performed better on the random file, indicating that there
was no quirk in the TSB-38 file. Furthermore, our tests revealed
that as expected, modems with larger dictionaries were generally
faster than those with smaller dictionaries.

It is important to note that as a general rule, modems with
larger V.42bis dictionaries will tend to be faster on a compressible
file and slower on uncompressible files when compared to modems
with smaller dictionaries. Also of note: TSB-38 dictates that
if a transfer fails to connect, or is unable to transfer data
because of errors, it is considered to have failed. Tough crowd.

Understanding the Graphs

Each of the file transfer performance graphs represent how fast
or how often a modem is likely to connect. Each graph represents
the entire range of characters per second transferred by all
the modems tested. That overall range has been grouped into
four charts.

For example, the TSB-38 file transfer performance graph has
the group 8,000 cps and above. All of the modems that transferred
data at the rate of 8,000 cps or faster are listed in that
graph. The bars next to the listed modems indicate the percentage
of connections in which they averaged 8,000 cps or more.

Price

The combo cards tested were at a disadvantage due to their higher
cost. We did, however, award points in the features comparison
for their LAN ability. Interestingly enough, the Microcom TravelCard
Fast tied with the Motorola Marquis at $599 for the highest
price, but it only offered modem functionality. Stop back for
a visit in a few issues where we'll compare combo V.34/Ethernet
cards for modem and LAN functionality in a head-to-head review.

Bruce Boardman can be reached at bboardman@nwc.com.




<A HREF ="#HeadList">Back to Headline List</A><BR>
<HR>
<A NAME = "37"></A><IMG SRC="/pubgifs/ec40.gif"><BR><BR>
Copyright
NETWORK COMPUTING via NewsNet
January 15, 1996  Vol. 7 No.1

Whatever Happened To IBM and US Robotics?



There were two modems didn't make it into our final 20:  The US
Robotics Courier Dual Standard V.34 PCMCIA and IBM's WaveRunner.
Both were supplied in plenty of time but would not function with
our TAS test equipment.  In USR's case we tried everything even
swapping cards with USR so that they successfully tested on near
identical test equipment-all to no avail.  Sorry USR fans.  We
hope to test it in the future.  The WaveRunner was a slightly
different story.  Rather than a firmware-based chip set like
AT&T's or Rockwell's, IBM's other card uses a completely software
driven DSP design that required Windows or OS/2.  Our test
software was DOS-based, and while IBM said that a DOS window
should have worked, it was not too happy when it did not.  By
press time a Windows version of the test software will be
available, which should allow us to test the WaveRunner next time
around.




<A HREF ="#HeadList">Back to Headline List</A><BR>
<HR>
<A NAME = "38"></A><IMG SRC="/pubgifs/ec40.gif"><BR><BR>
Copyright
NETWORK COMPUTING via NewsNet
January 15, 1996  Vol. 7 No.1

Online Services And The Internet: The Network Manager's Friend
Or Foe?

By:
Andy Covell

Your users and your management have heard the hype or have been
bitten by the cyber bug, and they now know that they can't
live without Internet access whether they're in the office
or on the road-and they need it now!

So, how do you get your users on the Net without having to devote
half your staff to set-up, troubleshooting and hand-holding?
Online services may be just the answer, since they now offer
reliable, well supported dial-up Internet access.

We evaluated the Windows and Macintosh Internet access offerings
of the "big three" online services-America Online, CompuServe
and Prodigy, as well as the Microsoft Network (for Windows95).
If your organization already supports a corporate LAN or WAN
with Internet connectivity, you may want to use an online service
for mobile or remote personnel. If you lack Internet connectivity
altogether, you might consider an online service for in-house
staff. In either case, you can depend on online services for
easy Internet access that impose a minimal burden on your technical
support staff.

A focus for many corporate users is nuts and bolts network access
to corporate information and computing resources, as well as
easy interaction with corporate network users, so we focused
attention not only on flashy Web browsers, but also on some
of the more mundane but very important features of Internet
access, such as telnet access, e-mail attachments and un-anonymous
FTP. We also gathered information to gauge geographic dial
up coverage and availability of faster V.34 and ISDN lines.

None of the services we reviewed comes close to our idea of
an ideal service with regard to corporate Internet access:
One which is easy to use; offers a full suite of bullet-proof
access tools for both Windows and Macintosh clients; offers
ubiquitous high-speed access; enables seamless third-party
software options; and allows the corporation to easily pick
up what are very reasonable usage fees. Yet, each has its own
highlights, and given a specific situation, one or the other
may be just the ticket.

CompuServe, our Editor's Choice, offers the best overall mix
of features, coverage, flexibility and pricing, and did respectably
grade. The other services contain flaws that relegate them
to a lower rung in our ranking. America Online, the only service
to offer a Web browser for both Windows and Macintosh users,
is a friendly service with a few problems-the largest of which
is its inflexible payment options. The Microsoft Network rates
high on some features, but its newness translates into some
shortcomings when it comes to things like accessibility and
billing options, and its inability to run on anything but Windows95
machines keeps it from competing at the same level as CompuServe
when it comes to widespread deployment. Prodigy has some significant
technical limitations, but the tools Prodigy offers are good
quality, and the service offers good pricing/payment options.

CompuServe

Among the strengths of the CompuServe Information Manager interface
for Windows (WinCIM) and Macintosh (MacCIM) are their easy
to-use FTP, telnet and newsreader tools. The FTP is simple
and quite functional, as is the telnet application, which includes
built-in print and file capture. The newsreader worked well,
offers an auto-decode capability and includes a nifty feature
that lets users search the postings of any selected newsgroup
by keyword. Like the AOL and Prodigy newsreaders, the CompuServe
newsreader offers a utility that lets you search for newsgroups
using keywords.

The WinCIM and MacCIM e-mail program is workable but somewhat
lacking, since Internet addressing is not standard (requiring
that you insert Internet: prior to the address in the To: field)
and attachments to and from the Internet are not supported.

The new WinCIM 2.01 interface provides seamless access to proprietary
CompuServe services and the Internet. Just click the Web icon
on the WinCIM toolbar to launch the Web browser, which is Spry
Mosaic. Then, from the Web browser, click on the WinCIM button
to toggle back to the proprietary interface. Unfortunately,
as it stands now, your users will be confused if they try to
navigate to the Web browser from the WinCIM "Explore Services"
menu by clicking on "Internet" and then "World Wide Web." They'll
end up reviewing instructions for loading the old Netlauncher
software (the WinCIM 2.01 software was not downloadable during
our review). Hopefully, this will be fixed by press time.

The new WinCIM installation also solves the CompuServe/Windows95
PPP conflict found in the previous PPP/Winsock setup by copying
the CompuServe Winsock into CompuServe directories, leaving
the Microsoft Winsock in the WINDOWS directory.

The lack of a Web browser for the Macintosh is clearly a drawback,
but CompuServe gives Macintosh users instructions and a new
configuration utility that makes it fairly easy for users to
set up MacPPP and MacTCP and run with a third-party browser.
For Windows users, the CompuServe PPP and Winsock support also
provides great flexibility so you can consider a wide range
of Windows-based, third-party software solutions.

CompuServe has a well-supported invoice billing arrangement
a clear plus for decision-makers considering an online service.
The corporate billing option, coupled with the provision of
some reasonable pricing alternatives, puts CompuServe at the
top of the heap when it comes to this aspect of the online
connection.

America Online

The America Online service is smooth and slick, and easy to
use. In fact, it's the easiest to use of the packages we tried.
It's no surprise that AOL is the first choice for millions
of consumers. While an easy-to-use, pretty package certainly
may have its place, the clear consumer orientation won't make
it for many corporate applications.

The basic Internet tools that AOL provides are well done. The
FTP client is functional and includes text, GIF and JPEG viewers.
The newsreader includes a function for searching for newsgroups,
and it offers a File Grabber, which automatically downloads
and decodes threaded binary-encoded postings. The lack of a
telnet interface is one drawback, but for corporations that
don't have any terminal applications, it's not a problem.

The AOL Web browser, while a little clunkier looking than the
more conventional browsers of its competitors, still gives
you the basics. And it's fully integrated with the AOL interface
on both the Windows and Macintosh platforms. Additionally,
AOL offers Winsock for Windows users, making it possible to
implement other third-party TCP/IP clients.

While most of this is good news for potential corporate customers,
AOL pricing and payment options are not. AOL offers only one
fee schedule and does not offer any sort of corporate account
with invoice billing. The solution we were offered from the
sales personnel, when we asked how to go about setting up accounts
for corporate users, was to put each account on an American
Express card. It is unlikely that this will be a workable solution
for organizations thinking of using an online service in any
serious way.

Microsoft Network

Comparing Prodigy, CompuServe and America Online with the Microsoft
Network is really comparing various apples to an orange. Sure
they all provide dial-up access to content, services and the
Internet at comparable price points. But the MSN Explorer only
runs on Windows95 machines, and the MSN interface, developed
from the ground up as part of Windows95, is more tightly integrated
with the operating system. Also, MSN full Internet access does
not run on much of the installed base of Intel desktops (a
386 or 486 must have 6 MB minimum, with 8 MB as a more realistic
minimum); a Macintosh version of the interface is still being
developed.

The Microsoft Network interface can be loaded from the Windows95
installation disk(s), but the Internet access portions of the
basic package are limited. Once you log into MSN, you can view
newsgroups or use Internet e-mail (using MS Exchange configured
for MSN). For full Internet access, you must either download
software from the MSN, or purchase the Plus! package and load
from the installation CD. You also need to change the phone
number you dial in your MSN Settings, and load some programs
and files (for example, telnet.exe and ftp.exe) from the Windows95
installation media. Of course all of this is automated, making
the task a little time consuming, but not difficult.

Access to the Microsoft Network can be carried out explicitly,
by double-clicking on the MSN icon. Starting up any program
that requires Internet access will also automatically initiate
MSN login. You can even create Internet shortcuts on your desktop
that will automatically connect to the Microsoft Network and
navigate to the resource specified. This is an experience that
drives home the tight integration of the Windows95 operating
system with full-featured Internet access via the Microsoft
Network. The PPP support of MSN's full Internet access dial
ups, coupled with the Windows95 Winsock, give you plenty of
third-party software options.

The use of the Microsoft Exchange package for MSN e-mail provides
a solid e-mail package. Binary attachments to and from the
Internet are supported, and the e-mail program can be configured
to send and receive mail and other electronic correspondence
from a variety of sources in addition to the Internet.

The FTP and telnet programs, which are part of Windows95 and
reside in the Windows directory, are not announced as part
of the "Internet Center" within the Microsoft Network. To run
them you use the Run... command and just enter telnet or ftp,
or create a shortcut, so you have an icon to click. The FTP
program is a command-driven interface with no advanced features.
The telnet is good, allowing for significant tailoring when
it comes to fonts, colors and the like, and it enables terminal
logging as well as cut and paste.

The new version of Microsoft's Web browser (Internet Explorer
2.0) is feature-rich when compared to the browsers found on
the other services. For example, it includes support for several
new HTML extensions, Secure Sockets Layer and built-in e-mail.
A VRML plug-in should be available as you read this.

Prodigy

The Prodigy service offers an easy-to-use, proprietary package
with some good pricing options. While not up to the standards
set by CompuServe on many fronts, the service does have some
features that make it rather unique, and it will likely improve
greatly over the short term. Much of the Prodigy interface
is being ported to the HTML environment, and a Mac version
of the Web browser is due out within 45 days of this writing.
Look for good things to come with regard to Prodigy and the
Internet.

Prodigy lacks standalone FTP and telnet tools entirely. Telnet
is just not possible, while FTP must be done via the Web browser.
Although FTP via the Web works pretty well for anonymous FTP,
it gets quite cumbersome with un-anonymous FTP, and it introduces
a security hole in the process, since users must enter username
and password as part of a pseudo-URL to gain access to a remote
host for FTP. If a user isn't careful, that information is
retained in the URL history, and anyone with subsequent access
to browser can easily retrieve this sensitive information from
the global history list.

Prodigy's e-mail program is adequate, but not great. It offers
standard Internet addressing, provided an address does not
exceed 40 characters (in which case a nickname must be created
for the long Internet address). Attachments to and from Internet
users are not yet supported.

The Prodigy newsreader works well, and it was the newsreader
we ended up turning to most frequently. It has a very good
FAQ SEARCH feature, which enables you to easily retrieve and
display the FAQ from any newsgroup you read.

The Prodigy Web browser has all the basic functions. Its noteworthy
features include the ability to send Internet e-mail from within
the browser. Another good feature of the Prodigy Web interface
is a Prodigy home page search link, which takes you to another
Prodigy Web page that links to a large number of Internet search
tools including: Yahoo, WAIS, Archie, Infoseek, Image Finder,
People Search and the W3 search engines. This useful page exemplifies
the move to HTML for all Prodigy information and help files.

Andy Covell is manager of faculty computing services at the
Syracuse University School of Management. He can be reached
at abcovell@som.syr.edu.




<A HREF ="#HeadList">Back to Headline List</A><BR>
<HR>
<A NAME = "39"></A><IMG SRC="/pubgifs/ec40.gif"><BR><BR>
Copyright
NETWORK COMPUTING via NewsNet
January 15, 1996  Vol. 7 No.1

Online Services Pricing And Payment Options


For a network manager considering an online service for end
user Internet access, price and payment options are an important
consideration. Paying exorbitant fees for employees who rack
up the usage, or hassling with individual credit card payments
or reimbursements is clearly something to avoid. CompuServe,
with two pricing schemes and an established invoice billing
option, sets the standard. Prodigy, with a brand new invoice
billing option, is now a challenger in this category, while
AOL and MSN clearly have some catching up to do.




<A HREF ="#HeadList">Back to Headline List</A><BR>
<HR>
<A NAME = "40"></A><IMG SRC="/pubgifs/ec40.gif"><BR><BR>
Copyright
NETWORK COMPUTING via NewsNet
January 15, 1996  Vol. 7 No.1

Other Internet Access Options Are In The Wings



Each of the online services we reviewed has its limitations,
and none may be up to the task for you. But never fear-there
are numerous alternatives. One is to sign on with a national
Internet service provider like Performance Systems International
(PSI) or Netcom. These outfits offer aggressive pricing and
they are out in front when it comes to support for technologies
like V.34 and ISDN. Telecommunications companies are also an
option, with MCI and its internetMCI Dial Access service leading
the way. Even the Regional Bell Operating Companies will be
getting into the act with Pacific Bell and Ameritech both announcing
that they will offer Internet dial-up access in 1996. Local
and regional Internet access providers are another possibility
if your needs are contained within a smaller geographic area.
These providers sometimes offer very attractive prices and
services. For those who like the support of an online service
but don't like the way Internet access works in the online
service framework, both AOL and CompuServe are offering Internet
only packages that may cater to your needs quite nicely: The
GNN (AOL) and Spryte (CompuServe) services should be in full
swing by press time. We'll do our part to try to help you understand
your dial-up Internet access options with thorough reviews
of some of the services we've mentioned above.




<A HREF ="#HeadList">Back to Headline List</A><BR>
<HR>
<A NAME = "41"></A><IMG SRC="/pubgifs/ec40.gif"><BR><BR>
Copyright
NETWORK COMPUTING via NewsNet
January 15, 1996  Vol. 7 No.1

On The Edge: ATM: Ready Or Not? Here It Comes!

By:
Art Wittmann

I love new stuff. I would rather spend a month trying to get
a new technology to work (even if it means putting the user
community through a little turmoil on the road to newer and
better services) than buy an old proven technology and get
it to work in a day. I like the challenge, and I almost universally
like the result.

I started writing for Network Computing three months after its
premier issue hit my mailbox. The first couple of years, I
freelanced and kept my full-time job at the University of Wisconsin's
Computer Aided Engineering Center (CAE), where I've worked
for more than a decade. The past few years, I've been with
Network Computing full time, while still maintaining a part
time appointment at CAE. In that time, I've pushed, both upon
you and upon the staff at CAE, my lust for new technology (and
toys for that matter)

I bought CD players when a basic one cost more than $800, and
made CAE move to NetWare 3.0 long before we ever should have.
On the other hand, jumping on the latest technology bandwagon
for the better part of a decade has given me some perspective
as well as a number of battle scars. And that is the perspective
of this column, On the Edge: Looking at new technologies and
trends and trying to decide whether to jump in with both feet
or sit on the side lines and watch for a while longer. Occasionally
we'll talk about industry players and their implementation
of technology, and sometimes we'll just talk about the technology
itself. What could make a better first topic than ATM?

On to ATM

Not so long ago, some colleagues at the University of Wisconsin
and I were trying to decide if converting our network to ATM
in the next six months was the right move. We talked at some
length about the pros and cons of making the leap, but the
guy who holds the purse strings kept coming back the central
question: "Is it ready?"

We began the discussion as though this "problem" with ATM was
something new. "Gee, the standards are evolving so slowly,
what if we have to buy something and then throw it away in
a year or so?"

Yeah, right-like we've never faced that problem before! Maybe
you don't remember the battles over UTP Ethernet and all the
nonstandard ports of that technology that shipped. If you haven't
been in the local area networking game that long, then maybe
you'll be a little more familiar with questions about FDDI.
If you waited until FDDI's Station Management standard was
set in stone you wouldn't have bought any FDDI equipment until,
ummm I guess they still aren't done with that standard, are
they?

At some point, the standards bodies give themselves a life by
enhancing technology that is more than mature enough to deploy.
In the case of FDDI, more than a decade has been spent enhancing
the FDDI standard. Purgatory must be filled with people who
have the character to serve on such committees for their entire
run-and when you're sent there, you're forced to make conversation
with them. Ah, they could regale you with such stories-the
arguments over the proper use of the DR bit in the SDR byte
could amuse you for years. (For those of you who have just
reached for your FDDI standards manuals to look up the SRD
byte-stop. I made it up-and you should be ashamed of yourselves.)

Is ATM like this? Of course it is. The ATM Forum will create
some less-than-useful features within ATM and we'd be foolish
to believe otherwise. However, important and revolutionary
work is still to be done by these committees, so the question
of ATM's suitability for production networks is quite valid.

Look Who's Talking Now

Two companies in particular claim that ATM is ready for prime
time. Cisco Systems and Bay Networks have both recently released
products with the claim that these are the right ones upon
which to base your ATM infrastructure. In Cisco's case, we're
talking about the LightStream 1010, which was announced late
last November and will be delivered early this year. In Bay's
case, the product is the System 5000AH and its associated modules.

Both products came with the promise of being the stuff from
which production networks are made. I talked to both companies
at some length about the products and each company went out
of its way to tell me how these boxes were field upgradeable
both in terms of firmware and microcode. No small wonder, two
of the most critical ATM standards are just getting to the
point of being deployable. These standards are Private Network
to Network Interface (PNNI) and Available Bit Rate (ABR). Supporting
PNNI basically allows you to build extremely large ATM-based
networks (like Internet-sized networks)

Cisco calls PNNI the most complex and ambitious routing protocol
ever devised. Cisco also claims that the LS1010 is the first
box in the world to support it. It's always good to be the
first one to support a standard, because then your customers
can't ask you about compatibility with other customers. PNNI
is indeed a beefy spec, requiring Cisco to use processors 15
to 20 times more powerful than those found in the 7000 series
to support it.

ABR is supported on Cisco's box and will be supported through
a microcode upgrade on Bay's box. ABR is basically a mechanism
to allow bursty LAN-style traffic to run well on busy (congested)
ATM networks. It's a tricky technology and depends upon rapidly
responding to requests to slow down traffic flows. If an ATM
device can't respond quickly, it'll probably miss the time
window where a response would have done any good.

This means it needs to be done in hardware near where ATM cells
are sent and received. This also means that if ABR isn't supported
on the switch you already own, you've got a new boat anchor
when you decide to build a network that supports ABR. That
generally includes any switch produced prior to this year.
Now you can see why Bay and Cisco are pushing hard on the fact
that the newer technology is very upgradeable-right down to
the microcode level.

Let's Say You Buy Now

If you decide you want ATM now, and there are plenty of reasons
why you might want it, just how much of your early 1996 investment
can you expect to retain over the next two or three years?
This is a really good question and one that you should ask
any ATM vendor you might be considering. You'll have to consider
a couple of purchases as short-term buys.

Since ABR has to take place in hardware, any NIC or ATM edge
device (such as an Ethernet to ATM switch) will either need
a significant upgrade or need to be replaced. This is because
ABR would be done by the same hardware that does segmentation
and reassembly of packets (so-called SAR chips). Chip vendors
are now working on developing new SAR chips that include ABR,
but don't expect to see NICs and edge devices that use the
new chips until the end of the year.

ABR is a particularly important specification, as it allows
ATM networks to handle the bursty traffic that is commonly
seen in packet-based networks. Without the ability to slow
data sources at the ATM level, an ATM-based network may not
perform well at all. Let's say your ATM switch starts getting
busy and has to discard 3 percent of cells received. Most of
us have observed packet retry rates up to 3 percent, right?
If you further imagine that your average packet is 512 bytes
and gets broken down to 120 or so cells, the chance of one
cell in every packet being discarded is fairly high. The useful
throughput of your ATM network becomes small and goes down
as the average size of your data packet goes up-not quite the
sort of behavior to which we're accustomed.

TCP/IP has its own throttling mechanisms, so as packets get
trashed, TCP/IP will back off. However, that may result in
getting less than optimal bandwidth use as well. Few other
protocols have throttling mechanisms, and so ABR's is a highly
valuable function. This is not to say that you can't build
a functional network without it. You'll just build a better
one with it.

Some vendors have come up with proprietary ways to handle the
cell discard problem. Cisco, for example, has a technique called
early packet discard. The technique essentially attempts to
drop all cells from a single packet, thus considerably upping
the chances for getting other packets through the switch intact.

So Is Now the Time to Jump in and Buy?

I think so. I'm urging the folks that I work with to replace
our routers with Ethernet and ATM switches this summer. The
benefits of virtual LANs, greatly expanded bandwidth and manageability
seem just too enticing. Replacing old routers with new routers
is expensive and doesn't deliver the flexibility that we'll
demand from our network in the next three to four years.

A couple of years ago, I wrote an article saying that 1997 would
be about the right time frame to consider buying ATM. I was
wrong by about six months. Mid-1996 is a good time to start
buying and now is a good time to start planning. You're probably
still looking at a single-vendor solution, but that's OK. Most
of us chose a single-vendor solution anyway. Ask the hard questions
and make sure that vendor is doing the work to protect your
investment. Accept that you'll replace some of the stuff you
buy, but don't be afraid that you won't get a functional network
in the end. n

Art Wittmann is a senior editor of Network Computing and associate
director of the Computer Aided Engineering Center of the University
of Wisconsin-Madison. He can be reached via the Internet at
wittmann@engr.wisc.edu.




<A HREF ="#HeadList">Back to Headline List</A><BR>
<HR>
<A NAME = "42"></A><IMG SRC="/pubgifs/ec40.gif"><BR><BR>
Copyright
NETWORK COMPUTING via NewsNet
January 15, 1996  Vol. 7 No.1

Middleware & Cross-Platform Development: To DB2 From The Desktop: Too Many Ways

By:
Bruce Robertson

There are many ways to get to DB2-too many ways. Since
the mainframe is still by far the largest data repository in
most corporations, and DB2 is IBM's widely used RDBMS for the
mainframe, getting to DB2 has become the number one target
for third-party middleware vendors. Not surprisingly, since
those DB2 solutions usually involve purchasing additional software
for the mainframe, the price tag is often $10,000 or more,
and sometimes more than $100,000. It will pay to choose wisely.

Unfortunately, corporations usually approach the problem of
accessing DB2 data by assuming they need DB2 data to look like
tables in some ODBC-enabled desktop tool like Excel or Access,
or even an in-house developed PowerBuilder application. Such
data may indeed be in a row or a table in DB2 and may end up
being in a table in Excel or Access, or on a screen in the
PowerBuilder application. But getting between the two doesn't
mean that the DB2 table must be visible to any of these desktop
tools.

Traditional data access middleware does make DB2 tables directly
visible to the desktop query tools. Other middleware (message
oriented, RPC-based, TP monitors and even some day Object Request
Brokers) can also bring data down to the desktop, but without
letting the user see the tables in DB2 directly. 3270 screen
scraping products, along with other GUI distribution options
like Web forms to RDBMS links, can also get DB2 data down to
the desktop. Finally, replication solutions can copy the data
down to the desktop for local access.

There are a lot of options. Which one is right for you depends
on strengths and weaknesses of each access option and particular
product when evaluated in the context of the real need of the
organization. Let's take a look at each DB2 data-to-desktop
approach.

IBM DRDA for Direct Data Access to DB2

There are many products that offer direct data access to DB2
data. Some are specific to DB2; others also support access
to other nonrelational data sources like IMS or ADABAS on the
mainframe. For still others, DB2 is just another remote database
on another platform their middleware supports. There are at
least some basic distinguishing technology solutions, however,
that provide a road map for the many products.

First, look to IBM. After all, it's IBM's database. And IBM's
mainframe.

Distributed Relational Data Architecture (DRDA) is IBM's middleware
technology for connecting its DB2 databases. DRDA products
go beyond mainframe DB2 connectivity, accessing DB2/400, DB2
6000, SQL/DS and any other DRDA-enabled database. The mainframe
DRDA server component is bundled with DB2 now, so there's no
additional mainframe software cost to stomach. IBM has even
submitted DRDA to X/Open for consideration as a database networking
standard.

But, while IBM gives OS/2 desktops a native DRDA requester,
Windows clients must connect via an intermediate OS/2 gateway
machine running IBM's DDCS/2 software, depicted to the left.
This intermediate gateway approach does not require APPC and
LU 6.2 on each client, but the gateway itself can become a
bottleneck under significant transaction load or large result
downloads. Moreover, OS/2 might not be a strategic OS choice
for your organization.

The new option is to avoid the full gateway approach. Third
parties like Wall Data and Rocket Software have taken advantage
of DRDA's openness to create native Windows DRDA requesters
sporting the ODBC interface. StarWave's StarSQL connects over
DRDA's APPC network, but it can handle non-LU 6.2 clients via
Microsoft SNA Server on NT or other SNA gateway products, as
shown in the first figure below. Wall Data's RUMBA for Database
Access supports APPC connections over TCP/IP to the mainframe
using IBM's new AnyNet technology.

Proprietary Direct Data Access to DB2

Outside of DRDA options, many vendors have proprietary DB2 connectivity
products. Many, but not all, offer more functionality than
DB2 alone, particularly the stored procedures (including returned
data) that are not yet supported in DB2. To do this, these
products include an expensive separate host component. With
the host component handling the processing, predefined queries
(static SQL or DB2 plans) can be executed on the host for better
performance along with lower host and network utilization.

Third-party database gateways are typically more functional
and flexible than IBM's DDCS/2. The venerable Micro DecisionWare
Database Gateway, for example, is being slowly reincarnated
by new owner Sybase. The Database Gateway's APPC/LU6.2 connection
to the mainframe consolidates connections from Sybase/Microsoft
DB-Library and ODBC clients, depicted in the middle figure
below. Those clients (Windows, Mac, DOS, OS/2 and Unix) can
connect over IPX/SPX, TCP/IP, AppleTalk, VINES IP and NetBEUI
NetBIOS to the Database Gateway server (running on OS/2, Windows
NT or AIX). Not all client/protocol/gateway combinations support
all these options, but TCP/IP works on all. Since Sybase's
own dialect of SQL is used, DB2 ends up looking like a Sybase
database for direct data access. By writing separate CICS programs
to query DB2 (and other) data or to execute existing CICS transactions,
remote stored procedure calls from clients can be handled.

Similar proprietary gateway connections are available from the
usual middleware data access suspects: IBI EDA/SQL, TechGnosis
SequeLink, IBM's new Data Joiner, Cross Access, Digital AccessWorks,
Software AG Entire Access, NetWeave, Independence Technology
DAL, XDB XDB-Link, Oracle and many others. Cross Access and
EDA/SQL support host-based query governor options to keep DB2
from being overcome by ad-hoc transactions rather than OLTP
interaction.

There is a more direct approach, however. Neon Software's Shadow
Direct supports direct TCP/IP access from desktop to mainframe
DB2 data. The direct proprietary approach eliminates gateway
and APPC complications. Like Thomson Software's ODB/Server,
but unlike most proprietary gateway solutions, Shadow Direct's
host component does not run in CICS and provides greater performance
since it is multithreaded, unlike CICS itself. Enhanced manageability
is available while more control is offered through limits on
CPU resources (by session and query), rows/query throttles,
and timeouts for inactivity or long queries. Shadow Direct
even generates System Management Facility data for monitoring
and accounting use. The Neon solution can be seen in the bottom
figure to the left.

Though Neon does not handle as many data sources as EDA/SQL
and SequeLink, it does supply a unique automatic way to use
DB2 plans from existing applications for higher performance
on OLTP activities along with optimized ODBC catalogs. Moreover,
Neon provides programmatic links to stored procedures processed
by its server, or to code running in other subsystems like
CICS or IMS.

DB2 Procedural Access to DB2

For many applications, letting users see DB2 directly opens
up a big can of worms. Users can bog the mainframe down with
poorly designed ad-hoc queries. Shielding DB2 data behind new
or old/reused functions or stored procedures is a better approach,
since users can only execute queries predefined by IS personnel.
Moreover, this can offload the mainframe better than many other
middleware options. Future applications are more likely to
use a three-tier architecture, where clients talk to an intermediate
application server, and that server makes requests from DB2
and other resources as necessary. This approach can be seen
in the first figure on page 156. DB2 won't need to be seen
as a table to applications anymore. In fact, applications won't
need to see DB2 at all-they'll just see other applications.

As noted, data access middleware products increasingly support
SQL-specific stored procedures or can call other CICS-coded
transactions along with their direct data access. More generic
middleware products, in contrast, offer full programmatic access
to the mainframe with a wider variety of networking, API and
programming tool options but offer no direct data access to
DB2. Existing business rules and code can be reused by distributed
applications instead of the original terminal applications
for which they were written. Most message-oriented, RPC-based
and distributed transaction monitor products also allow the
mainframe to be a direct and equal partner in the distributed
application development environment.

DB2 Via 3270 Applications

A variety of products offer the ability to retrofit existing
character-based terminal applications (accessing DB2 data or
anything else) by mapping 3270 structured fields into GUI screens
or even fields in database tables. When an ODBC or other directly
enabled application asks for a record, the client-based subsystem
actually calls up appropriate 3270 screens and captures data
off them. No work or extra software license money is spent
on the host side (where software gets expensive quickly), and
desktops don't need LAN access to the mainframe. Changes to
the original 3270 applications, however, break screen-scraper
applications. For most situations, screen scraping is only
an interim solution to get a GUI face on an old application.

DB2 Via the Web

A major new category of database access uses Web forms. Using
products such as IBM DB2 WWW, IBM/Lotus InterNotes Web Publisher,
Allaire Cold Fusion, Aspect Software dbWeb, ExperTelligence
WebBase and others, a Web form can trigger a DB2 query, although
work has to be done to map DB2 fields to fields on the form
presented to the user by the Web server, as depicted in the
middle figure below. The user, however, requires no new software
for DB2 access beyond the Web browser.

This Web-specific, three-tier approach offers wide leverage
via Internet access and Web ubiquity, but current products
sacrifice security or performance. The no client software approach
is a return to the terminal emulation world in a sense, with
3270 replaced by the new Web terminal over TCP/IP and the Internet.
The quick deployment opportunity (no software to be deployed
to all the clients) and leveraging of the Internet makes Web
access to DB2 data an attractive alternative to consider. You
can collect data from desktops you can't control. But the user
can't easily use that data in client-based applications. Web
forms also don't offer all the control and functionality that
a carefully constructed client application written in PowerBuilder
can. Forms may not be able to offer local data validation,
and Web browsers like Netscape still don't let a user tab from
field to field while simultaneously scrolling the screen.

E-mail accessible rules processors like BeyondMail can offer
another style of ubiquitous access. As with Web forms, however,
developers will be unhappy with the user interface options
and control. But it's a good alternative when wide access is
more important than performance.

DB2 Access by Replication

A final option for accessing DB2 data is to have a system that
moves or copies that data out to the desktop or other LAN-based
RDBMS servers. Instead of accessing DB2 data where it is, replication
products bring the data to you, as shown in the last figure
below. Local access can be much speedier and completely offloads
the mainframe, which can be great for decision support or heavy
data analysis applications. Data can also be simplified and
clarified as it's copied. Sybase's recently shipped Replication
Server for DB2 supports real-time data synchronization at the
transaction level. Oracle, IBM DataPropogator Relational, Trinsic
InfoPump, Praxis OmniReplicator and others offer replication,
and many support Lotus Notes.

Unfortunately, data synchronization is a difficult task, and
most corporations aren't prepared to handle data updates back
to the mainframe via replication. So, replication is often
stuck into a read-only copy function.

Bruce Robertson can be reached at brucer@metagroup.com.




<A HREF ="#HeadList">Back to Headline List</A><BR>
<HR>
<A NAME = "43"></A><IMG SRC="/pubgifs/ec40.gif"><BR><BR>
Copyright
NETWORK COMPUTING via NewsNet
January 15, 1996  Vol. 7 No.1

Net Results: The Sweet Music Of Network Management

By:
Dave Molta

Once every year my son's school holds a career day. You know,
this is where mothers and fathers who are doctors or police
officers or pilots come and tell the kids what it's like in
the real world. My basketball buddy Jim is an air traffic controller
and he brings in some airplane models that draw plenty of oohs
and ahs from the kids. I considered bringing in a 10BASE-T
hub, but decided that it just wouldn't have the same effect.
Ah, the life of a network manager: plenty of guts, but not
much glory.

Welcome to my first Network Computing column. Some of you may
recognize my name. After all, I've been writing reviews, features
and workshops for several years now. Twenty years ago, I went
to college to study electrical engineering and somehow ended
up with a degree in music. After I discovered that I was more
likely to end up a high school marching band director than
the lead trombonist on the Tonight Show, I caught the wave
of personal computers and networking and never looked back.
Today, I manage the network at Syracuse University, one of
the magazine's distributed lab sites, though I often feel like
it's managing me. "So what exactly do you do at your job?"
my son asks. "Well, it's kind of complicated, Danny. Now go
practice your trumpet." He doesn't press the issue. He's happy,
especially when daddy comes home with Orangemen season tickets.

Here at Syracuse, we're in the third year of a fairly ambitious
migration from mainframe to client/server-based systems. The
ride's been fast and rough. Deploying new technology, managing
a fast-changing systems environment and developing a culture
that views change as positive: These are the key challenges
we face, and I like to think that we're coming to grips with
all three.

On the technology front, we're building and rebuilding network
and systems infrastructure, doubling the number of Ethernet
connections each of the past three years to around 5,000 today,
deploying wide-area links, ATM, wireless, database servers,
Web technologies, you name it. We've phased out old systems,
including a fairly large VAX/VMS complex and an IBM 3090 mainframe
and replaced much of their functionality with Unix, NetWare
and NT servers.

Here's a shocker: Managing these changes hasn't been easy, neither
for my technical staff nor for our internal customers. We've
had to invent new ways of doing business, pushing expertise
and support out closer to where the real work of the organization
gets done, and retraining many of our staff in new technologies.
This has all taken place during an era of significant budget
cuts, forcing tough choices on priorities. In our efforts to
adapt, we've consolidated units, flattened our central computing
organization, embraced total quality management-and lived to
tell about it.

I know what some of you are thinking: It's a university. You
can afford to take chances on new technologies. People are
accustomed to frequent system changes and unexpected downtime.
Don't we wish! We may be willing to bleed at the edge a bit
more than some, but make no mistake about it, we have a business
to run, or so I'm told on a daily basis.

Call Me When It Breaks

I once knew a technical engineer whom I'll call Bill. Bill was
a technical genius, a guru if you will. He was proud of the
network he had designed and built and he could fix any problem
that might arise. As I got to know Bill a little better, I
learned that Bill's motto was, "Tell me when it's broke and
I'll gladly fix it." You gotta love a guy with an attitude
like that, right?

Wrong. I love people who design systems that don't develop problems
that need fixing. I'm talking about systems that are well planned
and implemented, fault tolerant, rock solid. Is this too much
to ask? Probably. Is it a worthy goal to aim for? I think so.
The problem is that we're not as good as we need to be at this
and I bet you're not either. Why? I'll offer the following
triad: people, technology and resources.

Like it or not, there's a shortage of qualified technical professionals
with knowledge of networking and client/server technologies
in today's market. A network administrator who had been with
me less than two years left recently for a job that paid nearly
$20,000 a year more than we did. One of our best client/server
project leaders recently received a competitive offer to increase
her salary by $30,000. Can any of you relate to this? Finding
good people and holding on to them is a monumental task for
most organizations trying to migrate to client/server technologies.

How do you deal with this situation? One good strategy comes
from Tom Landry, the former Dallas Cowboys coach. While other
teams were drafting players to fill holes in their team, Landry
would focus on selecting the best athlete available at the
draft. The point was to find a quality person you could train
to fill your needs. If you limit your search to people with
specific jobs skills, you'll either have to break the budget
or settle for mediocrity. The key is to find quality people
who can develop skills you'll need. In the long run, you build
a better organization. The same principle holds true when you
phase out old systems. I'll bet that you'll be better off in
the long run if you retrain that VM systems programmer to do
Unix or NetWare or NT rather than look for someone new with
the specific skills you need.

The Technology Tap Dance

I'm tired of all those seminars stressing the importance of
aligning technology and business goals and meeting the customer's
needs. I don't take issue with the fundamental assertions,
but they're too simplistic. Sure, we have to remember the bottom
line, but we also have to be realistic about what's possible
with technology and what's not. We have to be selective in
what we deploy, because the bottom line for most organizations
is making it work-every day. Try to be all things to all people
and you're likely to find yourself in the midst of two angry
mobs: your staff and your internal customers.

Being a successful network manager is about making good technology
decisions. Over time, most managers realize they can't do it
alone. It requires teamwork, delegation of authority, communication
and organization of peoples' efforts. It also requires some
good instincts in selecting which vendors are blowing smoke
and which ones are in it for the long haul. Most of us became
technical managers on the basis of our technical skills rather
than our managerial acumen. No wonder some of the brightest
minds in my organization want no part of management.

Hardly a day goes by when I don't think about all the great
things my organization could do with a few more people, more
capital resources or a slightly larger operating budget. Don't
delude yourself into believing that your budget is more sacred
than anyone else's. Yes, network computing is strategic to
the enterprise, but so are a lot of other things. Focus your
efforts first on getting the most out of what you've already
got. Once you've done that and there is still a need, you've
got a more compelling case to make to management.

On the other side of the fence are those "lucky" enough to be
in high-growth organizations. I've been there, too, and it
isn't all it's cracked up to be. Too often, the pace of growth
is simply too fast to manage effectively. Lots of organizations
make an all-too-quick transition from "on fire" to "burnt out."
It comes down to making the right decisions about people and
technology, and managing your resources effectively.

My buddy Jim moved recently, so I don't know what they'll do
on career day at Danny's school. I could tell the kids how
exciting it is to decide between switching or routing for our
next backbone infrastructure. Or maybe I'll just dig out my
old trombone.

Dave Molta is director of network systems at Syracuse University.
He can be reached at dmolta@nwc.com.





<A HREF ="#HeadList">Back to Headline List</A><BR>
<HR>
<A NAME = "44"></A><IMG SRC="/pubgifs/ec40.gif"><BR><BR>
Copyright
NETWORK COMPUTING via NewsNet
January 15, 1996  Vol. 7 No.1

E-Mail & Messaging: Adding Interactive Services To Your Web Server

By:
Eric Hall

Look around the World Wide Web, and most of what you'll find
today are online equivalents of printed propaganda. There may
also be some downloadable files or imagesas well as the seemingly
requisite overabundance of hypertext links to every imaginable
corner of the Internet. But, for the most part, Web servers
are about as interactive as their paper-based counterparts.

While getting to this level of highly-functional-yet-somewhat
interoperable online publishing can seem miraculous to many,
it is far from where these systems need to be if they are ever
to be something more than an outlet for marketing's creativity.
In order for the Web to become the next-generation platform
for distributed computing, it must go beyond read-only into
the world of interactive. Users should be able to add, edit
and delete data in real time, as security and application locking
allow.

We'll outline some of the concepts and technologies to help
you get there today. We'll also look at some basic examples,
and provide pointers to other materials where appropriate and
available. By no means can we cover everything. We will attempt
to give you enough information so that you can get started
in the right direction.

"What kinds of applications can be called from the server, and
where can they be found?" These are the two most common questions
asked at this point. Before we can answer these questions,
we must first look at how the server and applications communicate.

The Common Gateway Interface (CGI)

Most, but not all, Web servers have two basic methods of handling
back-end data. They either read a file or communicate with
other programs through the Common Gateway Interface (CGI).
It's up to the application to figure out what to do with that
data, and to return HTML to the server. This is an extremely
simplistic overview of the process, and there are many possible
exceptions and extensions.

Examples of CGI can be seen on many servers today. Clickable
image maps such as those found on Network Computing's home
page http://techweb.cmp.com/nc/current use CGI. As you can
see in the figure "Clickable Image Maps," the client sends
an HTTP request containing the URL and XY coordinates to the
server, which passes the coordinates to a local application.
The application executes an event that has been associated
with those coordinates and returns HTML to the server for delivery
to the client. In the case of image maps, most of the time
this HTML data consists of an HTTP redirect command that instructs
the client to request another document from the servers.

Another common use of CGI is for searching through files or
databases (Yahoo and WebCrawler are good examples). A user
constructs a query that is sent to the server, which then spawns
an application that conducts the actual search and returns
the "hits" in HTML form.

So, to answer the question about the kinds of applications that
can be called by the server: They include any application that
supports CGI on the front end and can generate HTML responses.
In fact, you can call almost any program you want, as long
as it supports command-line operation.

However, the unfortunate truth is that no two Web servers are
alike. They have all been developed with specific audiences
in mind and "enhanced" to suit those markets. This means that
just because something works with server brand X does not mean
that it will work with server brand Y. For example, there are
considerable differences between the Netscape server's CGI
interface and the O'Reilly and Associates WebSite server's
CGI interface, even though they both use Windows NT's CMD.EXE
command interpreter. The differences between the various Unix
shells, Digital's DCL, DOS' COMMAND.COM, and all other platforms,
can also impact what you want to do.

CGI Detailed: GET Vs. POST

One of the most common methods of implementing CGI is through
the use of GETs and POSTs. In the "Clickable Image Map" figure,
the client submitted a URL with a question mark and XY coordinates
appended to the end. This method of CGI call is known as a
GET. Not only are GETs the oldest, but they are also the default
and easiest to implement. Another form of call, known as POST
has become more popular, and is generally considered to be
much more powerful. The differences between them are subtle,
but significant.

The GET method is named for what the client does. It asks the
server to GET the HTML document at HTTP://URL?Param. The server
sees that the URL points to an executable, calls it and passes
it the parameters.

In contrast, the POST method is used when the client submits
a block of data to the server. The client is not necessarily
assuming that anything will be returned to it, although the
connection is still open. In most cases, the server's Webmaster
has configured things so that the client is given a thank you
message, or response data is sent to the client.

CGI Examples

Let's walk through an example using O'Reilly and Associates
WebSite server, since its use of Windows NT's CMD.EXE makes
it easy to write simplistic demonstration scripts. WebSite
can also run on Windows 95, so it's easy to use on a personal
system for learning purposes. Besides, you can evaluate it
free for 60 days. It's available at http://website.ora.com.

Once the WebSite Manager is downloaded, use the administrative
tool to locate the directory for DOS executables (C:\WEBSITE\CGI
DOS is the default). Using NotePad or some other text editor,
create a file called DIRSEARCH.CMD in that directory and carefully
copy the contents of the "Sample File: DIRSEARCH.CMD" figure
and save the file. Using a Web client, connect to your server
and point to http://
/DOS-CGI/DIRSEARCH.CMD. Assuming that your server is configured
to allow your client to access DOS-based CGI scripts, you should
get an HTML page stating No Files Specified. Don't worry. This
is what we want! Connect again and this time add ?command.com
to the end. You should see a standard directory listing of
all the files named command.com. DIRSEARCH.CMD is a simple
NT batch file that scans your server's C: drive for any occurrence
of a specified file and returns the directory list to the user.
If a file is not specified, then an error message is displayed.
We'll extend this program later, but first we need to examine
it in detail.

Always Test the Data First

The first line in the box below is a parameter test to make
sure that a user has supplied a filename. If "%1" is blank
(that is, does not exist), then the batch file jumps to the
:errlabel label, which returns an error message to the user.
Any seasoned programmer will tell you that it's important to
test your program inputs before acting on them, and it is even
more crucial here for several reasons.

Most important is an open issue regarding system security. There
are known weaknesses with many operating systems that allow
shell metacharacters to break your script or a called application,
thereby exposing your system. It sounds far-fetched, but it
happens all the time. By adding keystroke traps to your script,
however, you can go a long way toward preventing this from
happening. Refer to the CERT archives for more information
on this subject. This weakness is not prevalent on all systems,
but it is on many of them.

Another good reason to test the parameters is for program control.
As we'll show below, you can use this function to minimize
your development. Finally, there's a programming reason to
test the input: to make sure that the data is valid. If instead
of conducting a directory search, you queried a database, you
would want to make sure that all of the necessary information
had been provided. Testing the data in a script is more efficient
than letting the called application fail. You would still have
to test for errors, so you might as well do it up front.

Define Your MIME Type

The use of MIME types to coordinate data exchange is now part
of the HTTP specification, as of version 1.0. As you can see,
both the :goodlabel and :errlabel sections begin with MIME
type declarations echoed to STDOUT.

For our purposes, we are simply declaring that the following
data is HTML. The Web client interprets this and knows to read
and apply whatever HTML formatting tags it sees in the data.
If the MIME type had not been declared, then your browser may
or may not interpret the HTML tags.

As you may have guessed, you can also send binaries or other
nontext data back to the client. For example, if you wanted
to return a GIF image, you would define the MIME content-type
as Image/GIF. The client would then receive and display the
image appropriately. Likewise, you could send application binaries
by using the Application/Octet-stream MIME type, and so on.
Note the echo statement that follows the MIME declaration.
You must send a blank line after you send the content-type
 statement.

The Rest of It

The remaining code is fairly self explanatory.  There are,
however, some things worth noting.  the < and > characters are
proceeded by ^.  This is to prevent CMD.EXE from interpreting
them as redirection commands.  Also notice that the DIR C:\"%1"/s
is the actual command that we are calling.  The remaining lines
of code are used to prepare the environment and command line, as
well as generate HTML on its behalf.

Image map servers and dynamic directory builders are two of
the more common CGI-aware applications that you'll find. Additionally,
there are a variety of third-party tools, such as database
requesters and indexed searching agents, available on the Internet.
If you wish to write your own, you can apply the same concepts
we've shown you to a compiled language.

Adding Support for Forms

In its current form, DIRSEARCH.CMD must be given a parameter
on the URL's command line. This is far from intuitive, and
if you made your users work like this, you'd have very little
traffic on your site. Most users prefer to enter information
on a form, and so do administrators, since the level of control
increases tremendously.

The easiest way to build a form is through the use of the <ISINDEX> 
HTML tag. This simple tag has the delightful affect of putting
an edit box widget on your page. When a user enters text into
the edit box and presses the enter key, the text is appended
to the URL (with the question mark), and the page is reloaded.
At this point, a valid search string exists, and the script
will process accordingly.

For our example, change the line in the figure " EACH.CMD" that
reads ^<H1^>No Files Specified^</H1^> to Enter a filename to
search for:  ^<ISINDEX^>.  Now point your Web browser to
/BIN/CGI-DOS/DIR SEARCH, and you will be greeted with an edit
box.  Type in COMMAND.COM and press enter, and the same script
will now branch through the :goodlabel section, and return a
directory listing.

Don't be confused by the name ISINDEX, it's an unfortunate
moniker for such a flexible gizmo.  This tag does not
automatically make your documents searchable; some other tool
(such as DIR, in our case) must do the searching if that's what
you want.  The <ISINDEX> simply provides an edit box whose contents
are appended to the current URL.

Just as the <ISINDEX> tag is poorly named, so is it poorly
treated by most browsers.  Many of them prepend the edit box with
an inane comment about the document being searchable, which is
just plain wrong.  In order for us to have an attractive form
that doesn't confuse people, we'll have to use "real" forms.

Creating Real HTML Forms

Form creation with HTML is a fairly simple task, and as we've
shown, passing data with CGI is also straightforward. When
you combine these two components' simplicity and strength,
tremendous results can develop.

In order to illustrate this, we need to modify our sample code.
Edit DIRSEARCH.CMD once again, and replace the <ISINDEX> 
tag with the code shown in the figure "ISINDEX Replacement".

Save these changes, and reload our now familiar URL.  Now,
instead of seeing an <ISINDEX> input field, you should see a real
HTML form.  When you enter a file name into the edit box and
click on the "submit" button, the script will process the
directory search and return the list of matching files.

Let's examine this syntax in detail.  The line that contains the
<Form> HTML tag does what you might guess:  It notifies the
client that the following text contains form elements, until a
closing </Form> tag is encountered.

There are additional keywords that can be used with the <Form> 
tag, which provide greater amounts of control. One of these
is the Action= statement, which tells the browser where to
send the form contents. By default, the destination is a URL
but it could be any CGI application available to the user.
For companies that have several distributed Web servers, but
only one or two CGI servers, this is a great way to split the
load. Also of importance is the Method= keyword, which allows
you to specify that the query data is to be submitted using
the GET or POST methods as described earlier.

Widget Lingo

Another keyword, the <Input> tag, has many options, but it
basically applies to almost every type of form widget available.
This includes edit boxes, radio buttons, check boxes and command
buttons.  The only types of widgets not controlled by <Input> are
list boxes (either drop-down or scrollable) and large, multi line
text boxes, both of which have their own specific tags.

In our example, the first occurrence reads <Input
Name="FILENAME"> .  This syntax will create an edit box with the
resource name of Filename.  The Name= command associates a name
with the control data.  After successfully running a query, look
at the client's URL command line.  Notice that where it used to
have ?command.com, it now has ?filename=command.com.  One final
thing to remember about the Name= option is that it does not
apply to command buttons (you'll see why in a moment)

Notice that we did not explicitly declare the edit box as such.
That's because the default of <Input> widget is an edit box.  If
we chose to do so, we would declare it using the Type=Text
attribute.  Other valid Type= declarations include Password
fields (like edit boxes, only keystrokes are echoed as
asterisks), CheckBox, Radio, Submit and Reset.

Additionally, we could preload a string into the edit box by
using the Value= statement. This can also be used with password
fields, as well as checkboxes, radio buttons and command buttons,
although its usage varies widely among them.

The default width (in theory, at least) of an edit box is 20
characters, and the height is one. You can define different
sizes using the Size= directive. When setting the width, use
Size=W. When setting the height, use Size=W,H. Note that setting
the width does not restrict input to that amount of characters,
but instead defines the width of the box itself.

One problem with the GET method is that text is appended to
the URL as entered. If you let the user enter as much as they
want, you run the risk of script or called application failure
due to overflown buffers. The Maxlength= attribute is useful
for limiting the maximum number of characters that an edit
box will accept, thereby limiting your system's exposure.

Altogether, our listic <Input Name="FILENAME"> example could (and
probably should) read as <Input Type="TEXT" Name="FILENAME"
Width=20 Maxlength=20>.  Note the use of quote marks around the
textual values; this is an HTML requirement that needs to be
followed for maximum compatibility.

Command Buttons

Command buttons are not nearly as complex as the edit boxes;
there are only two possible types.  Our example uses the
Type="Submit" variety, which builds the parameter list and
appends it to the URL specified by the <Form Action=...>  tag.
The other type of command button is Type="Reset", which clears
the form and resets all of the widgets to their original state.

The only optional attribute that can be set for command buttons
is Value=.  In this case, the Value= attribute defines the text
that appears on the button's face.  It does not change the
button's behavior.  For our example, we could change <Input
Type="Submit"> to <Input Type="Submit" Value="Search"> , and add
another button that cleared the editbox by using <Input
Type="Reset" Value="Clear">.

And Then Some

You should experiment with the different types of widgets and
their optional parameters. Together they provide a fairly comprehensive
set of tools for getting user input into your CGI scripts and
applications. A good starting place is http://www.ncsa.uiuc.edu
 SDG/Software/Mosaic/Docs/fill-out-forms/ overview.html, and
by searching Yahoo's (http://www.yahoo.com) database for "HTML
and Forms."

Anyone familiar with other types of graphical forms-based tools
will tell you that these gizmos, while handy, are hardly sufficient.
New extensions have been added to the HTML drafts that address
some concerns, but not all. Vendors are stepping up with products
that address these concerns, however, so you should be able
to find something that suits your need.

On the back end, Lotus Development and Netscape have both promised
to ship HTML filters for their respective groupware products,
allowing you to leverage in-form value testing, mathematical
functions and the like. How these extensions will be handled
by generic clients is anybody's guess. But it is a step in
the right direction.

As for the clients, Netscape Navigator 2.0 and Oracle's PowerBrowser
clients both promise to offer event-driven scripting language
extensions that will allow Webmasters to embed extended controls
into the forms directly. Although these clients will perform
the functions as directed, it means more work for the administrators,
since you may have to support multiple proprietary extensions.
Again, it's a step that will drive further standards-based
development.

GET vs. POST Revisited

We have been using the GET method to pass data down to our
script.  The POST method, which offers more capabilities,
deserves some attention as well.  To see a quick demonstration,
the <Form...>  line in DIRSEARCH.CMD to read <Form Method="POST"
Action= "BIN/CGI-DOS/DIRSEARCH"> , and reload the form again.
This won't the behavior of your script, but you may receive a
security notice from your client.

Remember, we've been using O'Reilly's WebSite. It's CGI interface
to CMD.EXE is relatively consistent between POST and GET. However,
there is one significant difference. WebSite manager generates
a unique temporary file that contains the data provided during
the post operation. You can sniff through this output file
on a programmatic basis if needed, allowing your script to
branch according to content. This is extremely handy if you
have to allow users to submit large strings of text or binary
objects.

Beyond Simple CGI

GET and POST both offer value, depending on your objectives.
However, in this implementation they are also both limited
in that they rely on NT's CMD.EXE, a character-based command
interpreter, as well as require that scripts and applications
support the use of STDIN and STDOUT. This precludes the use
of Windows-based applications as CGI-callable services.

A handful of vendors are working together to develop WIN-CGI
to circumvent this problem. Implemented as a CGI-aware library,
you can extend any application for which you have the source
code, recompiling it to read and write CGI instead of the more
traditional I/O interfaces. Such efforts aren't new. Many sources
are available for CGI libraries for various servers and operating
systems. Contact your Web server vendor for leads.

Now that you understand some of the concepts behind CGI programming,
you should be able to develop applications that communicate
between the Web and any data system. Remember to follow the
specifications, and do a lot of testing, and you'll come out
ahead.

Eric Hall is an independent networking consultant, currently
working in Europe. He can be reached at ehall@nwc.com.




<A HREF ="#HeadList">Back to Headline List</A><BR>
<HR>
<A NAME = "45"></A><IMG SRC="/pubgifs/ec40.gif"><BR><BR>
Copyright
NETWORK COMPUTING via NewsNet
January 15, 1996  Vol. 7 No.1

E-Mail & Messaging: Sample File: DIRSEARCH.CMD


IF "%1=="" goto errlabel

:goodlabel

echo Content-type: text/html

echo.

echo ^ <html^>^<head>

echo ^ <title^>All occurances of %1^</title^>^</head^>

echo ^ <body^>

echo ^ <h1^>All occurances of %1^</h1^>

echo ^ <pre^>

dir c:\"%1" /s

echo ^ </pre^>

echo ^ </body^>

echo ^ </html^>

goto exitlabel

:errlabel

echo Content-type: text/html

echo.

echo ^ <html^>^<head>
 
echo ^ <title^>No files specified!^^</title^>^</head^>

echo ^ <body^>

echo ^ <h1^>No files specified^</h1^>

echo ^ </body^>

echo ^ </html^>

goto exitlabel

:exitlabel




<A HREF ="#HeadList">Back to Headline List</A><BR>
<HR>
<A NAME = "46"></A><IMG SRC="/pubgifs/ec40.gif"><BR><BR>
Copyright
NETWORK COMPUTING via NewsNet
January 15, 1996  Vol. 7 No.1

E-Mail & Messaging: ISINDEX Replacement



echo ^ <Form^>
 
echo Enter A File Name to search for:

echo ^ <Input Name=FILENAME"^>
 
echo ^ <Input Type=SUBMIT"^>

echo ^ </Form^>
 




<A HREF ="#HeadList">Back to Headline List</A><BR>
<HR>
<A NAME = "47"></A><IMG SRC="/pubgifs/ec40.gif"><BR><BR>
Copyright
NETWORK COMPUTING via NewsNet
January 15, 1996  Vol. 7 No.1

Fast Networking: Appearing In The NIC Of Time

By:
Art Wittmann

It's amazing that it took this long, but it's finally here,
and at a price almost anyone can afford. It's fast networking
100 Mbps or faster to be exact, and it's about time. Ethernet
and Token-Ring have been around roughly as long as the IBM
PC and its derivatives, and they just don't cut it as a backbone
technology anymore.

Ten years ago, if anyone told me that processors would increase
in power by 500 times and we'd still use the same networking,
I would have laughed. Yet, that's exactly what happened. We
went from the .2 MIPS of the 8088 to the 100 MIPS of the Pentium,
and Ethernet and Token-Ring have been with us all the way.

It's time for a change. Every desktop computer sporting a 486
or better can saturate a 4-Mbps or 10-Mbps pipe, and most newer
servers and power workstations can do a fairly good job on
a 100-Mbps pipe. More important, we now have applications that
can actually take advantage of that speed. Multimedia, databases,
graphically intensive you-name-it all can suck up lots of bandwidth.
End stations need either fast connections or exclusive access
to their own Ethernet or Token-Ring segments. Servers need
high-speed connections, as their clients can and do pull so
much data from them quickly.

The products are new, and with anything new in our industry,
caution is the key word. The package may say 100 Mbps, but
your actual performance could be only slightly better than
it was with your old network. Let's look at some of the important
considerations.

Even considering fast networking for ISA-based computers makes
almost no sense. In practice, most ISA systems, whether servers
or workstations, are cooking along pretty well if they fill
a Token-Ring or Ethernet pipe. Their chances of taking advantage
of more bandwidth are slim. On the other hand, if the technology
you choose is a shared medium, like Fast Ethernet or FDDI,
then upgrading an ISA system may make some sense. Fast Ethernet
in such a system makes more sense than FDDI, since an FDDI
card will probably cost as much as or more than your ISA system
is worth. And, please, don't even talk about ATM on ISA, at
least not yet.

EISA, PCI and most of the RISC workstation buses can take significant
advantage of fast networking, and here is where it will shine.
We've found EISA systems that can move more than 70 Mbps through
an FDDI card, as well as PCI and SBus systems that can move
more than 90 Mbps. The resultant workload on your processor
is likely to be significantly different, however, depending
on the bus you pick. Due to much higher throughput rates and
low latency, we found that bus mastering PCI cards are likely
to impact the system bus 70 percent to 90 percent less than
EISA cards.

Your mileage may vary depending on the quality of the drivers
involved. As a general rule, buses like PCI are more suited
to processors like the Pentium and P6, and we should mention
the Alpha, PowerPC and MIPS 4000 processors as well. Video,
fast networking and SCSI cards all will leave more of the CPU's
time for your application on PCI vs. EISA, ISA or MCA.

An interesting rule of thumb says that any given processor can
move about the same number of megabits of data as its MIPS
rating. So, if you want a processor that can move at 100 Mbps
or faster, you'd better have a 100-MIPS processor. In practice,
this turns out to be a fairly good rule of thumb. While generally
you'll get better performance running a performance benchmark,
real-life numbers usually will follow the rule. So, don't expect
your old 25-MHz 486 to take full advantage of these technologies.
It takes a pretty speedy chip to really shine through.

Even with a fast processor, it takes a good OS/driver and protocol
implementation to get the most out of a high-speed NIC. TCP
IP is a good example of a protocol that will turn in good numbers.
Novell's IPX, with its burst mode, will do well. However, its
older SPX-I protocol is too slow to be helped much by a fast
NIC. SPX-II is much better, but it isn't used much in practice.
AppleTalk isn't too bad, since several packets can be sent
before an acknowledgment is required, but it is limited to
512-byte packets, which slows it down significantly. What slows
AppleTalk down even more is the OS to which it's tied.

Which Type of Fast Networking?

Fast networking is a good catchall phrase, but we shouldn't
indiscriminately combine all of the technologies included in
this Buyer's Guide. Here's a quick look at the pros and cons
of the five major players.

FDDI is the granddaddy of the fast networking topologies. FDDI
is an acronym for Fiber Distributed Data Interconnect, and
it was designed as a high-availability shared media system.
With its token-ring architecture, small rings can deliver up
to 98 percent of the 100 Mbps for data, and even the largest
rings will deliver upward of 90 percent of the available 100
Mbps. Furthermore, developing a fault-tolerant backbone is
fairly easy because of FDDI's dual-ring architecture.

FDDI has also been standardized for Category 5 twisted-pair
cabling. Building fiber-based backbones with servers and routers
connected to concentrators via copper wiring is a method for
building larger networks today. The down side to FDDI is its
price. It is expensive, and it isn't likely to get much cheaper
in the near future. It is, however, well tested and well accepted.

If FDDI is the old man of the pack, ATM is the young whiz kid
who thinks he has all the answers. Unlike FDDI, ATM is purely
a switched medium, allowing only an ATM switch and end station
to share the bandwidth of any one physical port. In our guide,
that means every station will get unfettered access to full
duplex 100 Mbps or 155 Mbps. As ATM matures, it also promises
to deliver mechanisms for negotiating point-to-point connections
with an assured quality of service. So, if you've got an application
that needs to continuously transfer 40 Mbps to some station
across the network, your application can make a request for
such a connection and the bandwidth will be reserved for you.

The technology is not mature, and buying ATM now is living on
the bleeding edge. ATM is still expensive, but the prices are
dropping quickly. ATM will no doubt dominate backbone technologies
in a few years, but whether or not it's worth the investment
to bring it to the desktop is a tough question.

Fibre Channel is the speed demon's dream. Its present top speed
is 800 Mbps. Only a few applications really need this kind
of speed. Distributed parallel processing is one. Medical imaging
may be another. The animation folks in Hollywood have looked
at it, too. If your applications really need speed, and you've
invested in the very best hardware, Fibre Channel may be a
good choice. However, don't think about it for your servers
on a backbone, at least not yet. No major vendors currently
are shipping Fibre Channel boards for routers, so getting from
Fibre Channel to anything else can be difficult.

100VG-AnyLAN is a specification that Hewlett-Packard championed
as a rival to Fast Ethernet. IBM is supposed to be in HP's
camp, too-as VG-AnyLAN will support either Token-Ring or Ethernet
style frames-but IBM's interests seem to be elsewhere. In many
respects, 100VG-AnyLAN is technically superior to Fast Ethernet.
Mechanisms are built into the specification to allow the traffic
to be prioritized. That's important if you want to support
voice or video data. The problem is that other than HP, there
are very few proponents of 100VG-AnyLAN.

AnyLAN's claim to fame is its ability to run over four pairs
of Category 3 twisted-pair wiring (plain ol' phone cable).
Thus, it requires no new wiring if you have four pairs of wire.
Unfortunately, the Fast Ethernet crowd figured out how to do
that, too, making AnyLAN a little less attractive.

With the likes of Bay Networks, Intel and 3Com behind Fast Ethernet,
you'll find quality cards and healthy competition for your
business. One good thing about Fast Ethernet is that the connector
and pin-out are the same as for Ethernet. Many manufacturers
make 10/100 cards that will automatically sense the type of
port to which they are connected. Once installed, you only
need to install the back-end hardware. The card and driver
will automatically take advantage of the higher speed when
it senses the presence of a Fast Ethernet hub.

If you've got Category 5 wiring (the more expensive data-grade
stuff) in place, 100BASE-TX is the type of Fast Ethernet you
want. If you've got lots of Category 3 cable pulled, the emerging
100BASE-TP4 standard will take advantage of it. How well TP4
will work in practice has caused some concern. From everything
we've seen, it should work fine up to 100 meters.

Art Wittmann is a senior editor at Network Computing and associate
director of the Computer Aided Engineering Center of the University
of Wisconsin-Madison. He can be reached via the Internet at
awittmann@engr.wisc.edu.




<A HREF ="#HeadList">Back to Headline List</A><BR>
<HR>
<A NAME = "48"></A><IMG SRC="/pubgifs/ec40.gif"><BR><BR>
Copyright
NETWORK COMPUTING via NewsNet
January 15, 1996  Vol. 7 No.1

Check List: Making Networking Pay Off



- Proper Cabling-(Fiber, Category 5 UTP, STP)

- Workstations with high-speed buses (EISA or PCI)

- Workstations with high-performance CPUs (486DX-66 or better)

- Operating Systems capable of handling fast networking (Windows95,
NT, NetWare, Unix)

- Applications that demand the speed




<A HREF ="#HeadList">Back to Headline List</A><BR>
<HR>
<A NAME = "49"></A><IMG SRC="/pubgifs/ec40.gif"><BR><BR>
Copyright
NETWORK COMPUTING via NewsNet
January 15, 1996  Vol. 7 No.1

And The Winner Will Be



While all of the technologies included here have technical merit,
not all will take the marketplace by storm. And, while it shouldn't
be the sole criterion, reasonable market presence is important
for any technology you plan to deploy widely.

The only real lightweight in this group is HP's 100VG-AnyLAN.
It's been positioned against Fast Ethernet, which has built
up what seems to be an insurmountable momentum.

Fibre Channel will be successful, but only in certain market
niches such as distributed parallel processing and high-end
scientific computing. Its other home will be in the online
storage market, where Fibre Channel's arbitrated loop architecture
will be popular.

FDDI has a few good years left, but it will lose favor as ATM
becomes standardized, stable and cheap. Within three years,
new backbones installed on anything other than ATM will be
unlikely.

Fast Ethernet is a great desktop technology. It's only slightly
more expensive than regular Ethernet, and 10/100 cards make
it easy to deploy. While waiting for ATM to become a viable
standard, choose Fast Ethernet for desktop and for in-building
runs from wiring closet to router. It's a great value.




<A HREF ="#HeadList">Back to Headline List</A><BR>
<HR>
<A NAME = "50"></A><IMG SRC="/pubgifs/ec40.gif"><BR><BR>
Copyright
NETWORK COMPUTING via NewsNet
January 15, 1996  Vol. 7 No.1

Product Update

By:
Pam Fruth

INTERNET

Performance Tech Instant Internet 3.0

Improved. Just add new features and stir. Performance Technology
has enhanced its combination hardware/software Instant Internet
3.0 to provide Internet access for up to 50 simultaneous users
on a NetWare or Performance Technology POWERLan network. Versions
1.0 and 2.0 supported digital, T1 and leased-line connections
but required external routers. The new version adds a built
in 64-KB ISDN connection but still requires external routers
if you need a dedicated T1. This standalone gateway runs on
a Performance Technology multitasking kernel whose sole function
is to handle Internet access requests, while client software
includes a 32-bit WinSock DLL. A new set of administrative
tools includes comprehensive access control and logging features
so you can surf specific Web sites and download files via FTP
on a configurable per user/group basis. You can also manage
Instant Internet 3.0 with SNMP tools such as HP OpenView.

Available: Now

$4,195 (single Ethernet unit, ISDN version)

Performance Technology

(210) 979-2000

fax (210) 979-2002

http://www.perftech.com


NETWORK MANAGEMENT

Fast Track Open Agent For NT

New. For the road to better NT management via SNMP, consider
Fast Track. This vendor's new product, Open Agent for NT, serves
as an SNMP agent for Windows NT servers and workstations, and
offers features that augment the SNMP agent from Microsoft's
NT package. Besides providing access to raw data statistics,
Open Agent for NT lets you retrieve information on hardware,
like CPU type and physical RAM, and on the network operating
system such as the NT version, revision and any installed service
packs. To evaluate IP routing and addressing dilemmas, you
can set up a remote ping so that an SNMP management station
can notify an NT server to ping an IP device. If you want to
know when users log in with the wrong password or when they
log out, Open Agent can install traps that point out these
instances, as well as filter which types of traps will be forwarded
to the management system. This product works with several SNMP
management consoles, such as HP OpenView and IBM NetView.

Available: Now

$495 per server

Fast Track

(301) 990-1500; fax (301) 977-5491

http://www.ftinc.com


REMOTE ACCESS

AirSoft Powerburst

New. AirSoft's Powerburst can revitalize the way you network
when you're out of the office. This remote node accelerator
software for NetWare can dramatically reduce network access
time without making any changes to current configurations,
according to AirSoft. Powerburst is a combination of Windows
based client software running on a remote workstation, and
agent software on a dedicated PC that interacts with a NetWare
file server. The client and agent implement AirSoft's caching
algorithms to do away with unnecessary file system calls. The
Powerburst Agent can support up to 64 concurrent users, and
does not sacrifice security (it never writes to the file server).
Powerburst works with remote node servers from several major
vendors.

Available: Now

$995 (agent for up to 64 simultaneous users), $645 (client licenses
in a five-user pack)

AirSoft

(408) 777-7500; fax (408) 777-7527

http://www.airsoft.com


Written by Pam Fruth (pfruth@nwc.com)




<A HREF ="#HeadList">Back to Headline List</A><BR>
<HR>
<A NAME = "51"></A><IMG SRC="/pubgifs/ec40.gif"><BR><BR>
Copyright
NETWORK COMPUTING via NewsNet
January 15, 1996  Vol. 7 No.1

The Last Byte: ISDN Blues: And The Internet Beat Goes On

By:
Timothy Haight

Longtime readers of Network Computing may be familiar

with my guru, a former Silicon Valley executive turned psychic
adviser, who now lives on the San Francisco Peninsula's skyline,
with one eye watching the Valley and the other on the deep,
blue sea. I visit him now and again when it all becomes too
much, as I did last week...

When I got there, he was playing his djembe, an African goatskin
drum. Before I could say more than two words, he handed me
a beaded gourd he called a shekere and began telling me how
to shake it.

"Start with a simple downward stroke on the downbeat pulse,"
he said. "Hold it gently with your upper hand around the neck,
and use the fingers on your lower hand to move it up and down.
Like this"

"But I came here to talk!" I said.

"You need this more," he said. "Now. And-one, and-two, and-three.
Get it?"

I more or less got it, and he went back to playing his drum.
After a few minutes I said, "ISDN is still driving me nuts."

"ISDN drives you nuts every year. Don't break your rhythm. Loosen
up your shoulders. Relax your upper arms. Don't forget to breathe."

"But it isn't ISDN really. It's the Internet. I got this ISDN
line so I could surf the Internet faster, but all the servers
I want to go to are so jammed up that the throughput is no
faster than with my old modem."

"Don't stop playing," he said. "I need that 4/4 pulse, so I
can do the 3/4 and we can get 6/8. Oh, yes. Why don't you go
to other servers?"

"I don't want to go to other servers! The popular sites are
jammed because that's where people want to go!"

"Now look what that did to your muscle tension. Let those shoulders
go! Listen to the beat. I wish we had someone here to play
the bell. Try accessing the servers after midnight."

"I know all the tricks," I said. "I guess what really bugs me
is I feel so helpless. I'm used to this kind of stuff on my
LAN. Sometimes you're CPU-bound. Sometimes it's bandwidth.
Or disk. So you open up the bottleneck. But on the Internet,
who do you call?"

My guru stopped playing. After a while, he said, "I don't know."
He started playing again. "The marketplace will work it out."

"I don't know if it will," I said. "Maybe as long as most people
have modems, the information providers will optimize their
servers for slow access. They can blame the slow speeds on
the modems, and most people won't know the difference."

"Well, maybe those things are what happens when a medium gets
a mass audience."

"Well, maybe it sucks!"

"Will you keep playing! The thing is to keep in sync with it,
not to fight it. The Internet is evolving. Maybe next week
it'll be different."

"Maybe it'll be worse!"

"OK, write a column about it. Maybe that will make you feel
better. In the meantime, could you try this pattern? One-and
two, one-and-two, one-and-two. Push down, lift with the fingers,
down hard, then skip a beat. And don't hold that shekere so
tight, OK?"

Timothy Haight can be reached at thaight@nwc.com.





<A HREF ="#HeadList">Back to Headline List</A><BR>
<HR>
</PRE>
</BODY>
</HTML>
</DOC>